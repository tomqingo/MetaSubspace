Traceback (most recent call last):
  File "train_psgd_new.py", line 99, in <module>
    f = open(os.path.join(args.classresume,'subspace_best.txt'),'rb')
FileNotFoundError: [Errno 2] No such file or directory: './save_final_cifarfs/subspace/5_way_5_shot/classselect.mat/subspace_best.txt'
Traceback (most recent call last):
  File "train_psgd_new.py", line 99, in <module>
    f = open(os.path.join(args.classresume,'subspace_best.txt'),'rb')
FileNotFoundError: [Errno 2] No such file or directory: './save_final_cifarfs/subspace/5_way_5_shot/subspace_best.txt'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
> /home/amax/luoqin/MAML-Pytorch/train_psgd_new.py(213)main()
-> model.load_state_dict(checkpoint['state_dict'])
(Pdb) OrderedDict([('projection_variable', tensor([[-0.0286,  0.1775,  0.0933,  ..., -0.0404,  0.0009, -0.2068],
        [-0.0185,  0.1598,  0.0511,  ..., -0.2048,  0.4974, -0.0598],
        [-0.0603, -0.0691, -0.0157,  ...,  0.3033, -0.0284,  0.3023],
        ...,
        [ 0.0022, -0.1212, -0.0184,  ...,  0.4742,  0.3186, -0.1865],
        [ 0.0348,  0.1880,  0.0480,  ...,  0.3146,  0.1380,  0.0242],
        [-0.0423,  0.1093,  0.0283,  ...,  0.3106,  0.0133, -0.1324]],
       device='cuda:0')), ('net.vars.0', tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0')), ('net.vars.1', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.2', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0')), ('net.vars.3', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.4', tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0')), ('net.vars.5', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.6', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0')), ('net.vars.7', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.8', tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0')), ('net.vars.9', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.10', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0')), ('net.vars.11', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.12', tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0')), ('net.vars.13', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.14', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0')), ('net.vars.15', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars.16', tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0')), ('net.vars.17', tensor([0., 0., 0., 0., 0.], device='cuda:0')), ('net.vars_bn.0', tensor([0.1801, 0.1682, 0.4898, 0.3787, 0.3914, 0.4154, 0.3154, 0.6855, 0.3672,
        0.3147, 0.2477, 0.3589, 0.1946, 0.3025, 0.4299, 0.5167, 0.3105, 0.5443,
        0.4085, 0.5455, 0.4495, 0.6735, 0.4872, 1.1376, 0.2524, 0.3148, 0.2468,
        0.2911, 1.1963, 0.4413, 0.5401, 0.5606], device='cuda:0')), ('net.vars_bn.1', tensor([0.2353, 0.1553, 0.8414, 0.6632, 0.7418, 0.8317, 0.2373, 1.2208, 0.4180,
        0.3040, 0.1845, 0.4111, 0.0988, 0.4535, 0.8972, 0.7430, 0.3864, 1.4025,
        0.6436, 0.9613, 0.9368, 1.8485, 1.4622, 2.1466, 0.3350, 0.3570, 0.3046,
        0.3358, 2.4055, 0.5691, 0.9322, 0.4570], device='cuda:0')), ('net.vars_bn.2', tensor([0.7126, 1.2808, 0.5161, 1.2066, 0.6817, 1.7242, 0.5553, 1.1737, 1.0718,
        0.9499, 0.7257, 0.8385, 0.7777, 1.0710, 0.4213, 0.4601, 1.5099, 0.4959,
        0.9600, 0.6058, 1.4499, 1.3370, 1.3300, 1.1410, 0.9047, 0.8103, 1.6443,
        0.8875, 0.4841, 0.5718, 2.2356, 0.5216], device='cuda:0')), ('net.vars_bn.3', tensor([2.3606, 5.5413, 1.9267, 6.3841, 2.6324, 7.7347, 2.1400, 6.4302, 4.3522,
        3.7614, 2.2720, 4.5066, 2.9487, 6.7648, 1.6870, 1.4208, 6.2574, 1.4302,
        3.2008, 2.6072, 6.1993, 9.4480, 8.7682, 4.4838, 3.3429, 2.3446, 8.6188,
        3.7931, 1.5931, 2.2913, 8.6239, 1.5177], device='cuda:0')), ('net.vars_bn.4', tensor([0.9120, 0.3518, 0.6588, 0.5638, 1.8652, 0.5015, 0.7250, 0.3888, 0.7392,
        0.9755, 0.6204, 2.3614, 0.3621, 1.0407, 0.7286, 0.5540, 0.9983, 0.5778,
        0.6192, 1.0125, 0.6622, 0.4940, 0.7134, 0.7951, 0.5624, 1.1543, 1.3520,
        0.7717, 0.8467, 0.5633, 0.6107, 0.4060], device='cuda:0')), ('net.vars_bn.5', tensor([3.6250, 1.2404, 3.9511, 1.8187, 8.4158, 1.5999, 2.7844, 1.4701, 2.7173,
        3.6140, 2.1788, 8.2195, 1.2745, 4.4004, 2.4488, 1.6411, 3.9977, 2.1303,
        2.1608, 4.3167, 2.6392, 1.7309, 2.4598, 2.9353, 1.9535, 4.9053, 5.5893,
        2.7985, 3.9024, 2.1949, 2.3121, 1.4512], device='cuda:0')), ('net.vars_bn.6', tensor([ 6.4048,  9.5699,  1.6119,  2.3413,  1.2475,  0.6741,  3.6278,  1.7630,
         1.2880,  2.9066,  2.2757,  1.9526,  2.4382,  1.9009,  2.7461, 11.3551,
         2.6912,  0.9598,  1.7092,  0.7823,  3.5755,  3.1517,  1.2877,  9.0780,
         1.9551,  1.7308,  1.6549, 10.9641,  2.5577,  0.4379,  6.5480,  4.5582],
       device='cuda:0')), ('net.vars_bn.7', tensor([27.5001, 60.4004,  6.3471,  8.6244,  4.4449,  3.1709, 23.5314,  7.5801,
         5.1684,  9.6229,  7.9771, 10.7971,  8.7727,  6.3654, 10.9272, 83.8558,
         9.5608,  3.2084,  6.1709,  3.1352,  9.9472, 10.4767,  4.1999, 27.3116,
         7.5721,  7.6752,  6.7067, 60.7187,  9.2242,  1.9981, 35.6357, 19.1505],
       device='cuda:0'))])
(Pdb) Traceback (most recent call last):
  File "train_psgd_new.py", line 600, in <module>
    main()
  File "train_psgd_new.py", line 213, in main
    model.load_state_dict(checkpoint['state_dict'])
  File "train_psgd_new.py", line 213, in main
    model.load_state_dict(checkpoint['state_dict'])
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
Traceback (most recent call last):
  File "train_psgd_new.py", line 600, in <module>
    main()
  File "train_psgd_new.py", line 213, in main
    model.load_state_dict(checkpoint['state_dict'])
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/site-packages/torch/nn/modules/module.py", line 830, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for Learner:
	Missing key(s) in state_dict: "vars.0", "vars.1", "vars.2", "vars.3", "vars.4", "vars.5", "vars.6", "vars.7", "vars.8", "vars.9", "vars.10", "vars.11", "vars.12", "vars.13", "vars.14", "vars.15", "vars.16", "vars.17", "vars_bn.0", "vars_bn.1", "vars_bn.2", "vars_bn.3", "vars_bn.4", "vars_bn.5", "vars_bn.6", "vars_bn.7". 
	Unexpected key(s) in state_dict: "projection_variable", "net.vars.0", "net.vars.1", "net.vars.2", "net.vars.3", "net.vars.4", "net.vars.5", "net.vars.6", "net.vars.7", "net.vars.8", "net.vars.9", "net.vars.10", "net.vars.11", "net.vars.12", "net.vars.13", "net.vars.14", "net.vars.15", "net.vars.16", "net.vars.17", "net.vars_bn.0", "net.vars_bn.1", "net.vars_bn.2", "net.vars_bn.3", "net.vars_bn.4", "net.vars_bn.5", "net.vars_bn.6", "net.vars_bn.7". 
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Traceback (most recent call last):
  File "train_psgd_new.py", line 616, in <module>
    main()
  File "train_psgd_new.py", line 255, in main
    ]), download=True, sample=False, samplenum=args.k_shot)
  File "/home/amax/luoqin/MAML-Pytorch/cifardataset.py", line 91, in __init__
    with open('test.txt', 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'test.txt'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
total time: 0.00026917457580566406
train loss:  []
train acc:  []
test loss:  []
test acc:  []
best prec: 0.0
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
Traceback (most recent call last):
  File "train_psgd_new.py", line 616, in <module>
    main()
  File "train_psgd_new.py", line 300, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 378, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    vec.append(param.grad.detach().reshape(-1))
AttributeError: 'NoneType' object has no attribute 'detach'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
Traceback (most recent call last):
  File "train_psgd_new.py", line 616, in <module>
    main()
  File "train_psgd_new.py", line 300, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 378, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    vec.append(param.grad.detach().reshape(-1))
AttributeError: 'NoneType' object has no attribute 'detach'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
> /home/amax/luoqin/MAML-Pytorch/train_psgd_new.py(168)get_model_grad_vec()
-> for name,param in model.net.named_parameters():
(Pdb) Meta(
  (net): Learner(
    conv2d:(ch_in:3, ch_out:32, k:3x3, stride:1, padding:1)
    relu:(True,)
    bn:(32,)
    max_pool2d:(k:2, stride:2, padding:0)
    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:1)
    relu:(True,)
    bn:(32,)
    max_pool2d:(k:2, stride:2, padding:0)
    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:1)
    relu:(True,)
    bn:(32,)
    max_pool2d:(k:2, stride:2, padding:0)
    conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:1)
    relu:(True,)
    bn:(32,)
    max_pool2d:(k:2, stride:2, padding:0)
    flatten:()
    linear:(in:128, out:5)
    
    (vars): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (8): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]
        (9): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (10): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (11): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (12): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]
        (13): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (14): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (15): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (16): Parameter containing: [torch.cuda.FloatTensor of size 5x128 (GPU 0)]
        (17): Parameter containing: [torch.cuda.FloatTensor of size 5 (GPU 0)]
    )
    (vars_bn): ParameterList(
        (0): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (4): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
        (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
    )
  )
)
(Pdb) Learner(
  conv2d:(ch_in:3, ch_out:32, k:3x3, stride:1, padding:1)
  relu:(True,)
  bn:(32,)
  max_pool2d:(k:2, stride:2, padding:0)
  conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:1)
  relu:(True,)
  bn:(32,)
  max_pool2d:(k:2, stride:2, padding:0)
  conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:1)
  relu:(True,)
  bn:(32,)
  max_pool2d:(k:2, stride:2, padding:0)
  conv2d:(ch_in:32, ch_out:32, k:3x3, stride:1, padding:1)
  relu:(True,)
  bn:(32,)
  max_pool2d:(k:2, stride:2, padding:0)
  flatten:()
  linear:(in:128, out:5)
  
  (vars): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 32x3x3x3 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 32x32x3x3 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 5x128 (GPU 0)]
      (17): Parameter containing: [torch.cuda.FloatTensor of size 5 (GPU 0)]
  )
  (vars_bn): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 32 (GPU 0)]
  )
)
(Pdb) <generator object Module.named_parameters at 0x7f9a7414d780>
(Pdb) *** SyntaxError: unexpected EOF while parsing
(Pdb) [('vars.0', Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True)), ('vars.1', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.2', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)), ('vars.3', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.4', Parameter containing:
tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0',
       requires_grad=True)), ('vars.5', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.6', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)), ('vars.7', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.8', Parameter containing:
tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0', requires_grad=True)), ('vars.9', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.10', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)), ('vars.11', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.12', Parameter containing:
tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0',
       requires_grad=True)), ('vars.13', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.14', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)), ('vars.15', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars.16', Parameter containing:
tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0',
       requires_grad=True)), ('vars.17', Parameter containing:
tensor([0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)), ('vars_bn.0', Parameter containing:
tensor([0.1975, 0.1801, 0.5483, 0.3793, 0.4078, 0.4361, 0.3168, 0.7470, 0.3574,
        0.3150, 0.2538, 0.3444, 0.2084, 0.3094, 0.4488, 0.5044, 0.3072, 0.5414,
        0.4048, 0.5310, 0.4414, 0.6875, 0.4791, 1.1606, 0.2712, 0.3698, 0.2671,
        0.3029, 1.1317, 0.4512, 0.5668, 0.5360], device='cuda:0')), ('vars_bn.1', Parameter containing:
tensor([0.2412, 0.1535, 0.9066, 0.6248, 0.7422, 0.8125, 0.2320, 1.3429, 0.3984,
        0.2898, 0.1934, 0.3824, 0.1228, 0.4459, 0.9042, 0.6975, 0.3684, 1.3506,
        0.6113, 0.9096, 0.8871, 1.7943, 1.3504, 2.1749, 0.3656, 0.4258, 0.3164,
        0.3431, 2.2013, 0.5764, 1.0507, 0.4454], device='cuda:0')), ('vars_bn.2', Parameter containing:
tensor([0.6714, 1.2077, 0.5049, 1.2256, 0.7105, 1.6136, 0.5420, 1.1286, 1.0070,
        0.9001, 0.6806, 0.8095, 0.7606, 1.0064, 0.4094, 0.5151, 1.4625, 0.5214,
        0.9438, 0.6499, 1.3641, 1.2868, 1.2608, 1.1236, 0.8715, 0.7676, 1.5302,
        0.8535, 0.4911, 0.5484, 2.0819, 0.5125], device='cuda:0')), ('vars_bn.3', Parameter containing:
tensor([2.1494, 5.0624, 1.7780, 5.9811, 2.5061, 7.0428, 1.9697, 5.8854, 3.9680,
        3.4296, 2.0719, 4.1342, 2.7349, 6.1329, 1.5494, 1.4201, 5.7780, 1.4084,
        3.0411, 2.5474, 5.6907, 8.6407, 7.9984, 4.2237, 3.0842, 2.1501, 7.8279,
        3.4791, 1.5314, 2.0937, 7.8750, 1.4077], device='cuda:0')), ('vars_bn.4', Parameter containing:
tensor([0.8803, 0.3394, 0.6104, 0.5528, 1.8014, 0.5078, 0.7050, 0.3821, 0.7283,
        0.9776, 0.6904, 2.1985, 0.3509, 0.9826, 0.7067, 0.6127, 0.9793, 0.5884,
        0.5956, 0.9636, 0.6498, 0.5310, 0.6824, 0.7471, 0.5267, 1.1179, 1.2764,
        0.7484, 0.8018, 0.6005, 0.6080, 0.4264], device='cuda:0')), ('vars_bn.5', Parameter containing:
tensor([3.3507, 1.1433, 3.5723, 1.6974, 7.7398, 1.5114, 2.5758, 1.3611, 2.5408,
        3.3974, 2.1648, 7.4923, 1.1767, 4.0180, 2.2529, 1.6663, 3.7281, 2.0272,
        1.9920, 3.9490, 2.4435, 1.6707, 2.2687, 2.6914, 1.7833, 4.5477, 5.1070,
        2.5869, 3.5642, 2.0991, 2.1694, 1.3985], device='cuda:0')), ('vars_bn.6', Parameter containing:
tensor([ 5.7800,  8.6700,  1.5443,  2.1990,  1.2359,  0.6255,  3.2958,  1.6465,
         1.1856,  2.7160,  2.1490,  1.8438,  2.3211,  1.7741,  2.5455, 10.2500,
         2.4785,  0.9321,  1.6050,  0.7153,  3.4160,  2.9318,  1.2101,  8.2786,
         1.8236,  1.6249,  1.5506,  9.8917,  2.4418,  0.4184,  5.9732,  4.1417],
       device='cuda:0')), ('vars_bn.7', Parameter containing:
tensor([24.7691, 54.4334,  5.8584,  7.8755,  4.1521,  2.8781, 21.2210,  6.9179,
         4.6825,  8.8337,  7.3209,  9.8588,  8.0699,  5.8118,  9.9240, 75.5098,
         8.6696,  3.0126,  5.6732,  2.8337,  9.1813,  9.5800,  3.8492, 24.7503,
         6.9277,  6.9944,  6.1088, 54.6790,  8.4983,  1.8252, 32.1686, 17.2867],
       device='cuda:0'))]
(Pdb) Traceback (most recent call last):
  File "train_psgd_new.py", line 617, in <module>
    main()
  File "train_psgd_new.py", line 301, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 379, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    for name,param in model.net.named_parameters():
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    for name,param in model.net.named_parameters():
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
vars.0 Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True)
vars.1 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.2 Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)
vars.3 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.4 Parameter containing:
tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0',
       requires_grad=True)
vars.5 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.6 Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)
vars.7 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.8 Parameter containing:
tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0', requires_grad=True)
vars.9 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.10 Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)
vars.11 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.12 Parameter containing:
tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0',
       requires_grad=True)
vars.13 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.14 Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True)
vars.15 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars.16 Parameter containing:
tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0',
       requires_grad=True)
vars.17 Parameter containing:
tensor([0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
vars_bn.0 Parameter containing:
tensor([0.1975, 0.1801, 0.5483, 0.3793, 0.4078, 0.4361, 0.3168, 0.7470, 0.3574,
        0.3150, 0.2538, 0.3444, 0.2084, 0.3094, 0.4488, 0.5044, 0.3072, 0.5414,
        0.4048, 0.5310, 0.4414, 0.6875, 0.4791, 1.1606, 0.2712, 0.3698, 0.2671,
        0.3029, 1.1317, 0.4512, 0.5668, 0.5360], device='cuda:0')
Traceback (most recent call last):
  File "train_psgd_new.py", line 618, in <module>
    main()
  File "train_psgd_new.py", line 302, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 380, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 170, in get_model_grad_vec
    vec.append(param.grad.detach().reshape(-1))
AttributeError: 'NoneType' object has no attribute 'detach'
Traceback (most recent call last):
  File "train_psgd_new.py", line 114, in <module>
    f = open(os.path.join(args.classresume,'subspace_best.txt'),'rb')
FileNotFoundError: [Errno 2] No such file or directory: './save_final_cifarfs/subspace/5_way_5_shot/subspace_best.txt'
Traceback (most recent call last):
  File "train_psgd_new.py", line 114, in <module>
    f = open(os.path.join(args.classresume,'subspace_best.txt'),'rb')
FileNotFoundError: [Errno 2] No such file or directory: './save_final_cifarfs/subspace/5_way_5_shot/subspace_best.txt'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
0 ('vars.0', Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True))
Traceback (most recent call last):
  File "train_psgd_new.py", line 618, in <module>
    main()
  File "train_psgd_new.py", line 302, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 380, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 170, in get_model_grad_vec
    vec.append(param.grad.detach().reshape(-1))
AttributeError: 'tuple' object has no attribute 'grad'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
0 ('vars.0', Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True))
1 ('vars.1', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
2 ('vars.2', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
3 ('vars.3', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
4 ('vars.4', Parameter containing:
tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0',
       requires_grad=True))
5 ('vars.5', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
6 ('vars.6', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
7 ('vars.7', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
8 ('vars.8', Parameter containing:
tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0', requires_grad=True))
9 ('vars.9', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
10 ('vars.10', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
11 ('vars.11', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
12 ('vars.12', Parameter containing:
tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0',
       requires_grad=True))
13 ('vars.13', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
14 ('vars.14', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
15 ('vars.15', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
16 ('vars.16', Parameter containing:
tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0',
       requires_grad=True))
17 ('vars.17', Parameter containing:
tensor([0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
18 ('vars_bn.0', Parameter containing:
tensor([0.1975, 0.1801, 0.5483, 0.3793, 0.4078, 0.4361, 0.3168, 0.7470, 0.3574,
        0.3150, 0.2538, 0.3444, 0.2084, 0.3094, 0.4488, 0.5044, 0.3072, 0.5414,
        0.4048, 0.5310, 0.4414, 0.6875, 0.4791, 1.1606, 0.2712, 0.3698, 0.2671,
        0.3029, 1.1317, 0.4512, 0.5668, 0.5360], device='cuda:0'))
Traceback (most recent call last):
  File "train_psgd_new.py", line 618, in <module>
    main()
  File "train_psgd_new.py", line 302, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 380, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 170, in get_model_grad_vec
    vec.append(param[1].grad.detach().reshape(-1))
AttributeError: 'NoneType' object has no attribute 'detach'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
0 ('vars.0', Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True))
True
1 ('vars.1', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
2 ('vars.2', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
3 ('vars.3', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
4 ('vars.4', Parameter containing:
tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0',
       requires_grad=True))
True
5 ('vars.5', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
6 ('vars.6', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
7 ('vars.7', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
8 ('vars.8', Parameter containing:
tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0', requires_grad=True))
True
9 ('vars.9', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
10 ('vars.10', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
11 ('vars.11', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
12 ('vars.12', Parameter containing:
tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0',
       requires_grad=True))
True
13 ('vars.13', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
14 ('vars.14', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
15 ('vars.15', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
16 ('vars.16', Parameter containing:
tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0',
       requires_grad=True))
True
17 ('vars.17', Parameter containing:
tensor([0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
18 ('vars_bn.0', Parameter containing:
tensor([0.1975, 0.1801, 0.5483, 0.3793, 0.4078, 0.4361, 0.3168, 0.7470, 0.3574,
        0.3150, 0.2538, 0.3444, 0.2084, 0.3094, 0.4488, 0.5044, 0.3072, 0.5414,
        0.4048, 0.5310, 0.4414, 0.6875, 0.4791, 1.1606, 0.2712, 0.3698, 0.2671,
        0.3029, 1.1317, 0.4512, 0.5668, 0.5360], device='cuda:0'))
False
Traceback (most recent call last):
  File "train_psgd_new.py", line 619, in <module>
    main()
  File "train_psgd_new.py", line 303, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 381, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 171, in get_model_grad_vec
    vec.append(param[1].grad.detach().reshape(-1))
AttributeError: 'NoneType' object has no attribute 'detach'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
Traceback (most recent call last):
  File "train_psgd_new.py", line 619, in <module>
    main()
  File "train_psgd_new.py", line 303, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 373, in train
    output = model.net(input_var)
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/amax/luoqin/MAML-Pytorch/learner.py", line 162, in forward
    x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: the derivative for 'running_mean' is not implemented
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
0 ('vars.0', Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True))
True
1 ('vars.1', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
2 ('vars.2', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
3 ('vars.3', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
4 ('vars.4', Parameter containing:
tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0',
       requires_grad=True))
True
5 ('vars.5', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
6 ('vars.6', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
7 ('vars.7', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
8 ('vars.8', Parameter containing:
tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0', requires_grad=True))
True
9 ('vars.9', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
10 ('vars.10', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
11 ('vars.11', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
12 ('vars.12', Parameter containing:
tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0',
       requires_grad=True))
True
13 ('vars.13', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
14 ('vars.14', Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True))
True
15 ('vars.15', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
16 ('vars.16', Parameter containing:
tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0',
       requires_grad=True))
True
17 ('vars.17', Parameter containing:
tensor([0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True))
True
18 ('vars_bn.0', Parameter containing:
tensor([0.1975, 0.1801, 0.5483, 0.3793, 0.4078, 0.4361, 0.3168, 0.7470, 0.3574,
        0.3150, 0.2538, 0.3444, 0.2084, 0.3094, 0.4488, 0.5044, 0.3072, 0.5414,
        0.4048, 0.5310, 0.4414, 0.6875, 0.4791, 1.1606, 0.2712, 0.3698, 0.2671,
        0.3029, 1.1317, 0.4512, 0.5668, 0.5360], device='cuda:0'))
False
Traceback (most recent call last):
  File "train_psgd_new.py", line 619, in <module>
    main()
  File "train_psgd_new.py", line 303, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 381, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 171, in get_model_grad_vec
    vec.append(param[1].grad.detach().reshape(-1))
AttributeError: 'NoneType' object has no attribute 'detach'
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
> /home/amax/luoqin/MAML-Pytorch/train_psgd_new.py(168)get_model_grad_vec()
-> for name,param in enumerate(model.net.parameters()):
(Pdb) [Parameter containing:
tensor([[[[-3.9761e-01,  2.0490e-01,  2.1423e-01],
          [ 3.4417e-01, -1.7341e-01, -3.1705e-01],
          [-2.2840e-01, -3.1148e-01,  5.6281e-02]],

         [[-1.7132e-01,  2.0080e-01,  7.9612e-02],
          [-6.0785e-01,  2.0547e-02,  1.2189e-01],
          [-3.3491e-01,  1.7664e-01, -2.3538e-03]],

         [[-3.6712e-01,  1.7289e-01,  3.6222e-01],
          [ 4.9999e-01, -9.7438e-02,  5.2046e-02],
          [ 2.9289e-02,  2.3533e-01,  8.3031e-03]]],


        [[[ 3.2255e-01,  3.8683e-02, -2.5828e-02],
          [-3.3373e-01, -1.8897e-02,  4.5500e-01],
          [ 4.5263e-02, -1.2960e-01, -2.9714e-01]],

         [[ 1.5821e-01, -3.3418e-02, -2.0894e-02],
          [-2.1623e-01, -2.1877e-01, -3.6589e-02],
          [-2.4995e-01, -1.0281e-01,  1.2432e-01]],

         [[ 2.0826e-01,  1.3295e-01, -5.8485e-01],
          [-2.3224e-01,  1.5781e-01, -1.7468e-01],
          [ 2.6088e-02,  4.9762e-02,  6.2826e-01]]],


        [[[ 7.3758e-02, -4.0845e-01,  3.1392e-01],
          [ 7.9570e-02, -4.9757e-02,  1.4795e-01],
          [-3.0879e-01, -1.7756e-01, -2.8810e-01]],

         [[ 1.1367e-01, -1.4131e-01,  1.5754e-01],
          [-8.0854e-02, -1.6505e-01, -3.3336e-01],
          [-5.4868e-01,  2.8449e-01,  1.1198e-01]],

         [[ 2.1313e-02, -1.6321e-01, -3.2723e-01],
          [ 2.8747e-02,  2.3421e-01,  2.5890e-01],
          [-1.0070e-01, -4.5799e-01, -4.0488e-01]]],


        [[[ 2.0843e-01,  2.4243e-02, -2.4100e-01],
          [ 8.1703e-02,  4.7911e-02, -2.6537e-01],
          [-4.9587e-01, -2.5465e-01,  2.2817e-01]],

         [[-1.2372e-02, -2.4458e-01,  2.2702e-01],
          [ 2.0732e-02,  1.0146e-01,  3.7762e-01],
          [ 3.4208e-01, -1.6089e-01,  1.0392e-01]],

         [[-1.6099e-01,  1.7033e-02,  2.0574e-01],
          [ 9.2716e-02, -4.3843e-01,  2.7873e-02],
          [-5.6947e-01, -2.0786e-02,  1.6473e-01]]],


        [[[ 1.8788e-01,  3.8739e-01,  3.6021e-01],
          [ 9.3583e-02, -1.2201e-01,  1.2970e-01],
          [ 1.9296e-02,  6.3791e-02,  7.3757e-02]],

         [[ 1.4640e-01, -6.6283e-03,  1.5408e-01],
          [-3.8329e-01, -3.1999e-03,  3.2964e-01],
          [-3.7389e-02,  2.9501e-01, -3.7151e-01]],

         [[-3.1498e-01, -1.3558e-02,  4.2551e-01],
          [-4.7078e-01, -1.5560e-01,  5.0266e-01],
          [ 1.6950e-01, -7.7706e-02, -2.6314e-02]]],


        [[[ 8.8607e-02,  2.3477e-01, -2.7369e-02],
          [-2.5987e-01, -6.8871e-01, -8.7679e-03],
          [ 6.6201e-02,  2.5212e-01,  9.7491e-02]],

         [[-2.5881e-01, -4.4382e-01, -7.4660e-02],
          [ 3.1960e-01,  7.0599e-02, -8.7952e-02],
          [ 2.4506e-01, -2.9990e-02, -3.0752e-02]],

         [[-5.1053e-01, -5.7309e-03,  1.4360e-01],
          [ 2.4748e-03, -1.2881e-01, -4.8889e-01],
          [-6.5054e-03,  2.8800e-01,  2.3915e-01]]],


        [[[ 1.0502e-01,  7.1315e-02, -5.1087e-01],
          [-7.5686e-02,  2.8042e-01, -5.5363e-02],
          [-3.2904e-01,  8.4893e-02, -3.3477e-02]],

         [[-2.6975e-01, -1.0916e-01, -5.0016e-02],
          [ 1.6647e-01,  2.3265e-01, -1.7701e-01],
          [-3.0917e-01,  1.2547e-01,  3.1809e-01]],

         [[ 2.3730e-01, -2.5481e-01,  3.9592e-01],
          [-2.6051e-01,  2.3524e-02, -2.4490e-01],
          [-2.0249e-01, -1.3434e-01,  3.8678e-01]]],


        [[[-1.9855e-01, -5.5705e-01, -3.7033e-01],
          [ 7.5433e-02,  1.9337e-01,  3.5764e-02],
          [-6.3833e-01, -4.3688e-03, -6.2453e-01]],

         [[ 4.9238e-02, -1.4050e-01,  3.4008e-01],
          [-4.4485e-01, -5.3954e-02, -3.7117e-01],
          [-3.2678e-02, -1.0750e-01,  1.3228e-01]],

         [[-4.1199e-01,  2.8446e-02, -2.3516e-01],
          [ 1.6253e-01,  1.8320e-01, -1.1102e-01],
          [ 1.1345e-01,  2.0202e-01,  8.5840e-02]]],


        [[[-3.6624e-02,  2.8355e-02, -2.6571e-01],
          [ 1.6539e-01,  4.5140e-01, -1.6414e-01],
          [ 2.4881e-01,  3.9853e-01, -3.8741e-01]],

         [[ 7.1968e-04,  1.1417e-01,  5.1600e-02],
          [-1.1916e-01,  1.0702e-01, -7.9326e-02],
          [-2.7328e-01,  1.8678e-01,  7.2410e-02]],

         [[ 9.0917e-03,  1.3104e-03, -6.9029e-03],
          [ 1.3629e-01, -1.6399e-01, -3.6650e-01],
          [ 9.0354e-02,  1.3293e-01,  9.3917e-02]]],


        [[[-1.7433e-01,  1.6398e-01,  9.0411e-03],
          [-3.9898e-01,  3.8435e-01, -2.0526e-01],
          [-1.1428e-01, -2.5767e-01, -2.7991e-01]],

         [[ 3.2865e-01,  9.3440e-02, -1.1290e-01],
          [ 2.3864e-01, -5.3887e-02,  1.1198e-01],
          [-6.0245e-02, -2.4944e-02,  9.4051e-02]],

         [[ 9.2741e-02, -2.6899e-01,  1.5197e-01],
          [-7.7847e-02, -3.8005e-02, -4.3431e-01],
          [-6.7387e-02, -5.0738e-02,  3.8504e-01]]],


        [[[-6.8265e-02,  5.1626e-01,  4.5853e-01],
          [ 1.0749e-01, -1.5850e-01, -1.0596e-01],
          [ 3.5908e-01,  1.6253e-01, -4.6499e-01]],

         [[-4.6101e-01,  2.2544e-01, -4.1660e-01],
          [ 4.1225e-01, -1.7826e-01,  3.0352e-01],
          [ 2.4218e-02, -1.3052e-01, -8.3359e-01]],

         [[-4.6096e-01,  2.8239e-01, -1.6801e-01],
          [ 7.4039e-02,  1.9503e-01,  5.9404e-01],
          [-3.1811e-01, -1.3443e-01,  4.8774e-01]]],


        [[[ 2.2122e-01,  3.2074e-01,  1.9098e-02],
          [-3.1117e-01, -1.4103e-01, -2.0399e-01],
          [ 5.8542e-02, -1.2666e-01,  1.0483e-01]],

         [[ 3.4273e-02, -1.7004e-01, -8.5887e-02],
          [ 2.9254e-01,  1.8632e-01, -1.0853e-02],
          [ 2.3356e-01, -3.0016e-01,  2.7654e-01]],

         [[ 6.4999e-02,  1.5016e-01, -7.5603e-02],
          [-8.4342e-02, -5.8294e-01, -2.4498e-01],
          [-3.1310e-02,  2.8113e-01,  3.4309e-02]]],


        [[[ 2.3986e-01,  3.1469e-01, -2.9074e-01],
          [ 9.5559e-02, -1.9686e-01,  5.5649e-01],
          [ 2.2817e-01, -3.8428e-02, -2.5605e-02]],

         [[ 3.3913e-01, -9.7196e-01, -1.3494e-02],
          [-4.0697e-01,  5.9528e-04, -4.5809e-04],
          [ 1.9547e-01,  3.8297e-01,  2.3888e-01]],

         [[-3.1831e-02,  1.0706e-01,  4.2099e-01],
          [ 3.8174e-02, -2.5756e-01,  2.7324e-01],
          [-2.8626e-01, -6.1256e-01, -2.9963e-02]]],


        [[[ 1.5929e-03, -1.7596e-01,  1.4228e-01],
          [-4.5145e-02, -2.3058e-01, -2.8136e-02],
          [ 3.5173e-01,  1.7116e-01, -3.5555e-02]],

         [[ 1.4796e-01, -7.4863e-02,  2.9830e-01],
          [ 9.2578e-03, -3.0150e-01, -3.9469e-01],
          [-1.7163e-02, -4.5246e-01,  2.2017e-01]],

         [[ 4.7073e-01,  2.6771e-01, -4.7284e-02],
          [ 2.5360e-01, -1.5668e-02, -3.6390e-01],
          [ 3.1833e-01,  2.2383e-01,  1.2366e-01]]],


        [[[ 1.2034e-01,  1.3729e-01, -2.7174e-01],
          [ 3.3717e-01,  3.9703e-01, -2.0917e-02],
          [ 2.2163e-01,  2.0586e-01,  1.2710e-01]],

         [[-2.2554e-01,  8.3153e-03,  4.4362e-01],
          [-1.9256e-02,  3.7591e-01, -9.3134e-02],
          [-4.2813e-01,  3.5459e-02, -5.5063e-02]],

         [[ 3.7547e-01,  7.4146e-02,  2.6944e-01],
          [-9.7984e-02,  1.7102e-01,  1.0991e-01],
          [ 5.1873e-02,  1.1573e-01, -5.9406e-01]]],


        [[[-1.4098e-01, -1.3296e-01, -1.4273e-01],
          [-1.1574e-01, -2.5079e-01, -1.5419e-02],
          [-6.7130e-02,  1.1734e-01, -1.7700e-01]],

         [[-4.9512e-01, -3.5682e-02, -7.3085e-01],
          [ 1.0257e-01,  9.3500e-02,  3.5873e-01],
          [ 7.3546e-02,  2.5407e-01, -2.6939e-01]],

         [[ 6.8503e-01,  1.4265e-01,  3.1946e-01],
          [-1.1867e-02,  2.5488e-02, -3.6672e-01],
          [ 5.6643e-02, -1.5787e-01,  4.6732e-01]]],


        [[[-8.7757e-02,  1.2636e-01,  1.9033e-02],
          [-2.7488e-01, -2.3360e-01,  3.2864e-01],
          [-2.0625e-02, -2.1124e-01, -2.0950e-01]],

         [[ 1.7235e-01, -4.6502e-02, -9.3589e-02],
          [ 8.1011e-02, -3.3268e-02, -9.1919e-02],
          [-4.8867e-01, -3.1042e-01,  4.1722e-01]],

         [[ 3.9796e-01, -9.6039e-02, -1.9748e-01],
          [ 7.6038e-01,  7.1195e-02, -1.9887e-01],
          [ 1.2522e-01,  2.3308e-01,  2.2603e-01]]],


        [[[-2.7009e-02, -3.4524e-01, -3.4077e-01],
          [ 3.9700e-01,  2.8149e-01, -2.8912e-01],
          [ 1.4903e-01,  7.2106e-02,  2.6731e-01]],

         [[-1.5522e-01, -5.4333e-01,  1.8937e-01],
          [ 3.4213e-01, -2.2097e-01,  7.1711e-02],
          [ 3.2938e-01,  6.0931e-01,  8.4448e-02]],

         [[ 1.3531e-01, -1.2123e-01, -5.2297e-01],
          [ 1.5098e-02, -6.0093e-01,  2.2651e-01],
          [ 2.7169e-02,  4.9714e-01,  5.3137e-02]]],


        [[[-1.8463e-01, -2.9346e-02,  1.2419e-01],
          [-1.5247e-01,  9.5297e-02,  1.2255e-01],
          [-5.1225e-01,  1.1755e-01, -8.4409e-02]],

         [[-9.0639e-02, -3.7364e-01,  5.6077e-01],
          [ 3.1780e-01, -4.0608e-01,  2.3808e-01],
          [-1.4982e-01,  4.4046e-05,  1.9104e-01]],

         [[-1.4814e-01,  3.9153e-01,  6.0462e-05],
          [ 2.4411e-01, -6.5614e-03, -7.6591e-02],
          [-1.9283e-01, -1.9913e-01, -2.5186e-01]]],


        [[[ 5.1512e-03, -4.1432e-01, -3.1011e-01],
          [-3.3692e-01,  2.2124e-01,  3.5927e-02],
          [ 3.0801e-01, -6.4227e-02, -1.4171e-01]],

         [[-7.1886e-02, -6.1885e-02, -5.4922e-01],
          [-3.1198e-01,  4.9256e-01,  5.9123e-01],
          [-1.1022e-02,  2.9154e-01,  2.5121e-01]],

         [[-6.3621e-01, -1.6523e-01,  5.8461e-01],
          [-1.3734e-01,  1.3672e-01,  1.5865e-01],
          [ 1.8464e-01, -3.9815e-01,  1.9990e-03]]],


        [[[-3.7043e-01, -2.4904e-01,  6.7038e-01],
          [-1.7497e-01,  3.6440e-01,  2.6411e-01],
          [-2.7353e-01, -3.4469e-01,  1.3409e-01]],

         [[-4.3967e-01, -2.8636e-01,  1.8443e-01],
          [ 5.8027e-04,  1.1624e-02, -1.7870e-01],
          [ 1.7663e-01, -2.2182e-01,  3.3582e-01]],

         [[ 5.8397e-02,  2.6805e-01,  9.9667e-02],
          [ 5.2691e-01, -1.2956e-02,  2.6091e-01],
          [-2.2070e-01, -7.3556e-03, -4.4835e-02]]],


        [[[-1.9651e-01,  1.4131e-01,  4.1820e-01],
          [-3.3493e-01,  5.8320e-02, -1.8649e-01],
          [-2.1779e-01, -5.3636e-02, -9.8592e-02]],

         [[ 1.6214e-02, -8.7707e-02,  2.6971e-01],
          [-4.1773e-02, -2.9531e-01, -2.5329e-01],
          [-1.9498e-01, -6.3941e-01, -1.0223e-01]],

         [[-2.0298e-01,  2.9528e-01,  7.2150e-01],
          [-1.1124e-02,  3.3093e-01,  1.5439e-01],
          [-3.9675e-01, -1.0188e-01, -5.3512e-03]]],


        [[[-9.1832e-02,  2.6724e-01, -2.3981e-01],
          [-2.8316e-01,  2.8609e-01, -1.9613e-03],
          [ 1.3380e-01, -4.4384e-02, -2.5451e-01]],

         [[ 5.4903e-01,  2.7686e-01, -4.2289e-01],
          [ 4.0571e-01,  3.7194e-02,  3.1670e-02],
          [ 9.8259e-02, -6.9577e-01,  6.4939e-02]],

         [[-5.1712e-02, -9.9154e-02, -3.6770e-02],
          [ 1.2398e-01, -6.8761e-02, -1.5922e-01],
          [-8.5930e-02, -2.5107e-01,  2.7685e-02]]],


        [[[-9.4660e-02, -4.1252e-01, -3.1118e-01],
          [-1.4279e-01,  2.1766e-01,  5.8790e-01],
          [ 2.9589e-01, -1.9563e-01, -6.8544e-01]],

         [[-4.5013e-01,  2.3558e-01, -7.4321e-02],
          [-3.1731e-01, -4.4763e-02, -2.1065e-01],
          [-1.6249e-01, -2.0169e-01, -3.7793e-01]],

         [[-2.7642e-02, -2.6506e-01, -2.1444e-01],
          [ 4.4168e-01, -3.2764e-01, -6.0578e-03],
          [-3.0459e-01, -1.3287e-01,  4.1294e-01]]],


        [[[-2.0006e-01,  2.3423e-01,  9.3985e-02],
          [-1.1907e-01,  2.6589e-01, -1.4498e-01],
          [ 3.6876e-02,  3.3215e-01, -5.7129e-02]],

         [[-1.4467e-01,  4.5270e-01,  1.0705e-01],
          [ 2.0442e-01, -2.5272e-01, -1.5310e-01],
          [-2.0874e-01, -3.4647e-02,  1.1772e-01]],

         [[ 4.1531e-01, -3.0702e-01,  3.0834e-01],
          [ 1.7632e-01,  2.4987e-01, -2.8922e-01],
          [-4.8571e-01,  1.6729e-01,  7.2343e-01]]],


        [[[-7.1707e-02, -6.2245e-02, -2.2458e-01],
          [-7.1150e-02, -2.4310e-01, -2.1564e-02],
          [ 1.9989e-03, -1.0905e-01, -2.3273e-01]],

         [[ 6.1215e-02, -5.2921e-01, -6.7328e-02],
          [-1.4538e-01,  9.5195e-02,  2.9821e-01],
          [-8.7086e-02,  4.0522e-01, -2.3776e-02]],

         [[ 5.8602e-02, -8.0116e-02, -5.0088e-01],
          [-4.0598e-02, -2.2277e-01,  1.0741e-02],
          [ 9.6422e-02, -2.4907e-01,  2.4428e-01]]],


        [[[-2.7949e-01, -2.2267e-01,  9.2161e-02],
          [ 5.6571e-01,  3.4382e-01, -1.8899e-01],
          [ 1.3805e-01,  1.8568e-01, -4.1085e-01]],

         [[-5.7605e-01, -9.2604e-02, -1.0326e-01],
          [ 1.7225e-01, -4.9150e-02, -2.6086e-01],
          [ 1.3608e-01,  2.6127e-02, -2.5469e-01]],

         [[ 2.4162e-01, -6.9572e-01,  2.5008e-01],
          [-2.1610e-01,  2.3697e-01, -1.6722e-01],
          [ 3.1469e-01,  1.0417e-02,  3.8277e-01]]],


        [[[ 4.4217e-01,  6.0492e-02,  2.2416e-01],
          [-2.3563e-01, -2.8358e-01,  2.9160e-01],
          [ 3.0138e-02, -1.6362e-01,  1.1890e-01]],

         [[ 4.7771e-01,  1.3867e-01, -1.0464e-01],
          [-1.3722e-01,  2.0240e-01, -2.3690e-01],
          [-1.0158e-01,  5.1243e-01, -1.4657e-01]],

         [[ 2.4978e-01,  1.7912e-01, -5.8656e-02],
          [-3.2400e-02,  1.7725e-01, -2.1785e-01],
          [ 5.9903e-02, -2.4285e-01, -5.1334e-01]]],


        [[[-6.5217e-03, -3.2547e-02, -4.1746e-02],
          [ 5.0206e-02,  1.6880e-01, -3.0321e-01],
          [-1.2237e-01,  1.7047e-01,  6.9349e-02]],

         [[ 2.2257e-01,  2.1366e-01,  1.1824e-01],
          [-2.9591e-02, -2.7692e-01,  4.7851e-01],
          [-1.3851e-01, -2.0656e-01,  3.0761e-01]],

         [[-3.1807e-01, -1.0083e-01, -3.7241e-02],
          [-2.5047e-01, -1.8405e-01, -5.3102e-01],
          [ 2.5851e-01, -4.8723e-01,  2.2134e-01]]],


        [[[-7.3265e-02,  1.0403e-01, -1.5208e-02],
          [-1.8050e-01, -3.0361e-01,  3.2042e-01],
          [-3.8173e-01,  4.6027e-01,  1.6187e-02]],

         [[ 1.4651e-01, -6.8752e-02, -1.5437e-01],
          [-2.6220e-02, -2.3387e-01, -3.1183e-02],
          [ 2.6325e-01, -5.9492e-02, -2.8059e-01]],

         [[ 7.4968e-01, -1.6660e-01, -3.2368e-01],
          [ 2.9387e-01, -6.4401e-01, -3.1447e-01],
          [ 7.1228e-01,  1.2651e-01, -4.9538e-01]]],


        [[[-1.5070e-01, -1.7135e-01,  1.4042e-01],
          [-1.0301e-01,  1.8149e-01,  2.9626e-01],
          [-2.1753e-02,  2.2103e-01, -6.9506e-02]],

         [[ 8.3017e-02, -1.3229e-01,  2.3749e-01],
          [ 1.6754e-01,  8.5116e-01,  1.5858e-01],
          [ 1.9380e-01, -3.1306e-01, -3.9732e-01]],

         [[ 2.1923e-01, -2.5158e-01,  5.2500e-01],
          [ 5.3409e-02, -2.2372e-01,  4.7056e-01],
          [ 3.2104e-01,  2.3760e-02,  4.4662e-01]]],


        [[[ 2.2095e-01, -1.3622e-01, -5.2251e-01],
          [ 3.8655e-01, -7.7426e-02, -5.1673e-02],
          [ 3.6014e-01, -5.2998e-02, -1.5029e-01]],

         [[-7.7292e-02,  2.1482e-02, -2.6477e-01],
          [ 5.3847e-01, -5.4976e-02, -2.2046e-01],
          [ 3.7885e-03,  3.5949e-01,  1.6678e-01]],

         [[ 4.3188e-01,  1.3890e-01,  1.6508e-01],
          [-5.1495e-01, -2.6710e-01,  8.6005e-02],
          [-2.5255e-01, -1.0476e-01, -2.4332e-01]]]], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[[[-2.3232e-02,  4.8038e-02,  8.3283e-03],
          [-6.1919e-02, -1.5601e-01, -1.4299e-02],
          [-1.4727e-01, -6.8560e-03,  7.1608e-02]],

         [[-6.2632e-02, -2.3052e-02, -8.8899e-02],
          [-1.6718e-01,  3.4871e-03,  6.1430e-02],
          [-7.3113e-02,  7.0390e-02,  7.5247e-02]],

         [[ 7.5366e-03, -3.0542e-02,  1.1724e-01],
          [-5.1929e-02,  3.1015e-02,  1.9446e-02],
          [-4.5160e-02, -1.6719e-02, -5.0545e-02]],

         ...,

         [[-6.7204e-02,  3.7384e-02,  6.7865e-03],
          [ 9.2998e-02,  4.6883e-02,  1.0079e-01],
          [ 4.9575e-03, -1.2696e-02, -3.2825e-02]],

         [[ 1.4647e-02,  4.2716e-02,  8.3279e-02],
          [ 5.2501e-02, -1.0216e-01,  8.8646e-02],
          [-1.4404e-01,  4.4134e-02, -4.0498e-02]],

         [[ 1.3560e-02,  4.9114e-02,  2.8624e-02],
          [-1.0635e-02,  3.7955e-02, -2.6188e-02],
          [-1.5772e-02, -3.2045e-02,  1.2601e-01]]],


        [[[ 9.5607e-02, -1.6386e-02,  1.4755e-04],
          [ 3.4794e-02, -1.5471e-01, -1.1953e-01],
          [-3.4478e-02, -1.5934e-01,  2.5655e-02]],

         [[ 7.9366e-02,  1.9286e-01, -9.4477e-02],
          [-8.8019e-02, -7.8353e-02,  1.6809e-01],
          [-6.8426e-02, -7.2353e-02, -6.2201e-02]],

         [[ 2.5375e-02, -1.8404e-02,  9.3871e-02],
          [ 7.9006e-02,  2.4849e-02,  1.4406e-01],
          [-2.2514e-02, -7.9572e-02,  7.7309e-02]],

         ...,

         [[-1.1416e-01, -1.0646e-01,  1.0965e-02],
          [-1.9913e-03, -1.4895e-01, -1.5140e-01],
          [-6.4509e-04,  1.0350e-01, -1.3203e-01]],

         [[-5.0733e-03, -4.4672e-03, -2.4701e-02],
          [ 2.3589e-02, -4.1083e-02, -1.0321e-01],
          [ 3.5034e-02, -2.7292e-02,  1.0647e-03]],

         [[ 5.5128e-02,  2.8078e-02, -6.5220e-03],
          [-4.5101e-02, -1.9109e-01,  6.4274e-02],
          [ 1.0410e-01, -1.2496e-01, -1.5187e-01]]],


        [[[-3.0054e-02,  1.0745e-01,  6.3065e-02],
          [ 5.0180e-02, -2.8099e-02,  7.3497e-02],
          [ 8.4114e-02, -1.0489e-01, -1.0518e-01]],

         [[-1.0829e-01, -6.9358e-02,  5.2214e-02],
          [-1.8938e-01,  3.6309e-02, -7.5897e-02],
          [ 8.2844e-03,  7.4807e-02,  1.3989e-01]],

         [[-4.9388e-02, -8.5427e-02,  2.8516e-02],
          [-1.1025e-01, -9.3965e-02, -5.5377e-02],
          [-4.1219e-02,  1.7773e-01, -7.8369e-02]],

         ...,

         [[ 5.8926e-03, -3.7243e-02, -9.8872e-03],
          [-2.5552e-02, -7.1485e-04, -3.4281e-02],
          [-4.6716e-02, -1.6749e-01, -4.1657e-02]],

         [[-2.2463e-02,  7.6260e-02, -8.3762e-02],
          [-1.1146e-01,  5.9717e-02, -4.1294e-02],
          [ 1.1862e-01, -2.3377e-02, -6.8440e-02]],

         [[-8.9821e-02, -8.1255e-02, -2.4482e-02],
          [ 4.1233e-02, -1.3419e-01,  9.4343e-02],
          [ 1.0619e-02,  1.2985e-01,  5.1072e-02]]],


        ...,


        [[[ 4.2724e-02, -1.1995e-01,  5.2220e-02],
          [ 1.1120e-01,  1.1626e-01, -1.8914e-02],
          [-2.4831e-02,  9.8973e-02,  7.5409e-02]],

         [[-2.9239e-02,  5.8368e-02,  1.9542e-02],
          [-4.7373e-02, -1.2038e-02,  5.7677e-02],
          [-3.6761e-03, -6.2564e-03, -9.5596e-02]],

         [[ 3.0999e-02, -5.0326e-02,  7.0624e-03],
          [ 4.4799e-02,  8.7026e-02, -3.7994e-02],
          [-4.6737e-02,  7.5886e-03, -2.8569e-02]],

         ...,

         [[ 9.1214e-02, -5.1878e-02, -1.3197e-01],
          [ 3.6528e-03, -8.3824e-02, -1.6965e-02],
          [-2.2598e-02,  5.4068e-02, -3.3850e-02]],

         [[-1.1008e-01, -1.0402e-02, -1.1372e-01],
          [ 6.0094e-02, -1.5571e-01,  1.3344e-01],
          [ 8.5870e-02,  3.2249e-02,  1.8769e-02]],

         [[-2.3038e-01,  1.5228e-01, -8.3197e-02],
          [-6.0252e-02, -2.4380e-02,  2.0931e-02],
          [-2.9688e-02,  5.2450e-02,  3.5757e-02]]],


        [[[-1.5440e-02,  6.1959e-02,  8.4417e-02],
          [-8.5919e-03, -9.1537e-02, -2.2438e-01],
          [ 6.8780e-02, -8.5308e-02,  1.9298e-01]],

         [[-5.0260e-02, -4.7156e-03,  5.6284e-02],
          [-5.6865e-02,  6.5023e-02, -1.7820e-01],
          [ 2.0380e-01, -4.5045e-02, -1.0848e-01]],

         [[ 1.3816e-01, -8.4734e-02,  1.2719e-01],
          [ 9.0831e-02, -1.7852e-02, -9.1102e-03],
          [-6.0151e-02,  2.9797e-02, -1.5038e-02]],

         ...,

         [[ 8.9391e-02, -1.6447e-01,  7.8785e-02],
          [-3.3208e-02,  3.2635e-02,  9.9927e-02],
          [-8.1577e-02,  4.7305e-02, -1.1458e-01]],

         [[-2.0376e-02,  2.3608e-02, -1.4262e-01],
          [-5.1964e-02, -1.1989e-01,  9.1159e-02],
          [-1.5723e-02,  3.9239e-02,  2.7948e-02]],

         [[ 7.9402e-02,  1.0022e-01,  7.4102e-02],
          [-5.8667e-02,  1.3565e-01, -1.1800e-01],
          [ 9.3947e-02,  2.4006e-02,  2.5013e-02]]],


        [[[-4.7888e-02, -1.1272e-03,  5.3468e-02],
          [-7.5735e-02, -1.2729e-01, -6.4921e-03],
          [ 2.9094e-05,  7.3103e-02, -4.1714e-02]],

         [[-7.8398e-03, -8.0627e-02,  5.2184e-02],
          [-3.6547e-02,  7.5515e-02,  6.1762e-02],
          [ 3.1347e-02, -6.5047e-02, -2.9395e-02]],

         [[-5.5472e-02,  7.3591e-02, -1.7650e-01],
          [ 6.9328e-02, -3.9148e-02, -3.2997e-03],
          [-1.0718e-01, -9.3627e-02,  6.3284e-02]],

         ...,

         [[-8.1812e-02,  3.1469e-03,  6.0356e-02],
          [-1.1225e-02,  1.0466e-01, -1.0174e-01],
          [-5.0786e-02, -5.5501e-02, -3.3674e-02]],

         [[-2.6461e-02,  2.0776e-02, -5.5764e-02],
          [ 4.7110e-02, -1.0964e-01,  3.3206e-02],
          [ 5.4569e-02,  3.6337e-03, -3.8852e-02]],

         [[-1.6791e-02, -7.8159e-02,  6.6135e-02],
          [-1.2345e-01, -3.8171e-02,  3.2097e-02],
          [ 9.5068e-02, -2.4440e-02, -5.1745e-02]]]], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[[[-0.0446,  0.0179, -0.0501],
          [-0.2010,  0.1924,  0.1136],
          [ 0.0299,  0.1584,  0.2078]],

         [[-0.0380, -0.0327,  0.0396],
          [ 0.0441,  0.0237, -0.1716],
          [ 0.0821,  0.0458,  0.1006]],

         [[-0.1364, -0.0243, -0.1313],
          [ 0.0232, -0.2783,  0.0200],
          [ 0.0324,  0.1571,  0.0352]],

         ...,

         [[-0.1562, -0.0332, -0.1615],
          [ 0.0202, -0.0150,  0.1644],
          [ 0.0727, -0.0394,  0.1734]],

         [[-0.0014, -0.0596,  0.0654],
          [ 0.0088,  0.0012,  0.0099],
          [-0.1377,  0.0060, -0.0032]],

         [[ 0.0360,  0.0209, -0.0766],
          [-0.1563, -0.1522, -0.0496],
          [-0.0637, -0.0679,  0.0924]]],


        [[[ 0.0035, -0.0864, -0.0443],
          [ 0.0339, -0.0089, -0.0154],
          [-0.0107,  0.0104,  0.0278]],

         [[ 0.0853, -0.0583, -0.0060],
          [-0.0383, -0.0065,  0.0768],
          [ 0.0622,  0.0439, -0.0538]],

         [[ 0.0699,  0.0762, -0.0722],
          [-0.1797,  0.1667, -0.0960],
          [ 0.0839, -0.0995, -0.0166]],

         ...,

         [[ 0.0489,  0.0811,  0.0007],
          [-0.1603,  0.0831,  0.0418],
          [-0.0077, -0.1705, -0.0134]],

         [[ 0.0542, -0.0305,  0.0185],
          [-0.0107,  0.0530,  0.0049],
          [-0.0244, -0.0710,  0.0539]],

         [[ 0.0685, -0.0203, -0.0114],
          [ 0.0085,  0.0724,  0.0945],
          [-0.0993, -0.2224,  0.0235]]],


        [[[ 0.1620, -0.0197, -0.0765],
          [-0.0509, -0.0517,  0.0300],
          [-0.1224,  0.0488,  0.1386]],

         [[ 0.0603, -0.1452, -0.1393],
          [-0.0328, -0.0876,  0.0322],
          [-0.0237, -0.0421, -0.0859]],

         [[-0.0685,  0.0903, -0.0328],
          [-0.0325, -0.0345,  0.0724],
          [-0.1717, -0.0832,  0.0052]],

         ...,

         [[ 0.0066,  0.0579, -0.0147],
          [-0.0089, -0.0762,  0.1103],
          [-0.0143,  0.1384, -0.1129]],

         [[ 0.0308, -0.0868, -0.0914],
          [-0.0770, -0.0071, -0.1008],
          [-0.0099,  0.0039, -0.1411]],

         [[-0.0916, -0.1959, -0.0282],
          [-0.1078,  0.0702, -0.0694],
          [-0.0188, -0.0445,  0.0590]]],


        ...,


        [[[ 0.0825, -0.1076,  0.0888],
          [ 0.0684,  0.1310, -0.1529],
          [ 0.0330,  0.0608,  0.0761]],

         [[ 0.0645,  0.0749, -0.0703],
          [ 0.0498,  0.0143, -0.0467],
          [ 0.1668,  0.1006,  0.0376]],

         [[ 0.0396,  0.0063,  0.0277],
          [ 0.1047, -0.0527,  0.0412],
          [-0.0170, -0.0383, -0.0257]],

         ...,

         [[-0.0308,  0.0258, -0.1001],
          [ 0.0678, -0.0521, -0.0245],
          [-0.0203,  0.0891,  0.0260]],

         [[-0.0164, -0.1361, -0.0858],
          [ 0.0071,  0.0149,  0.0016],
          [ 0.1193, -0.0884, -0.0723]],

         [[ 0.0644,  0.0243,  0.0624],
          [-0.0539,  0.1063,  0.1459],
          [ 0.0105, -0.0801,  0.1060]]],


        [[[-0.1327, -0.0172,  0.0235],
          [-0.0522, -0.1073, -0.0479],
          [-0.0621,  0.0939,  0.0508]],

         [[-0.1154, -0.0853, -0.0234],
          [-0.0306,  0.1418, -0.0808],
          [-0.1163,  0.0993,  0.1229]],

         [[ 0.0446,  0.0346, -0.0584],
          [-0.0343,  0.1128,  0.0262],
          [ 0.0147,  0.0442, -0.0422]],

         ...,

         [[-0.0441, -0.0676,  0.0318],
          [ 0.0658, -0.0614,  0.0994],
          [-0.0737,  0.1366,  0.1101]],

         [[-0.0929, -0.1213,  0.0976],
          [-0.0549, -0.0346,  0.2013],
          [ 0.0265, -0.0284,  0.0259]],

         [[-0.0211, -0.0304,  0.0142],
          [ 0.0101,  0.0679,  0.0172],
          [-0.0048,  0.0401, -0.1101]]],


        [[[-0.0643, -0.1191, -0.1634],
          [-0.0315,  0.0372,  0.0296],
          [ 0.0581, -0.0067,  0.0555]],

         [[-0.0755,  0.1259,  0.0716],
          [-0.1413, -0.0859,  0.0294],
          [ 0.1018,  0.0248,  0.0374]],

         [[-0.1583,  0.0224, -0.0815],
          [-0.0931,  0.0793, -0.0595],
          [ 0.0320,  0.0020,  0.0340]],

         ...,

         [[-0.0945, -0.0596, -0.0362],
          [-0.1906, -0.0656,  0.1630],
          [-0.0306,  0.0115,  0.0226]],

         [[-0.1109,  0.0007,  0.1505],
          [-0.0852,  0.0012, -0.0420],
          [-0.0076, -0.0217, -0.0986]],

         [[ 0.0466,  0.1306,  0.0300],
          [ 0.0737, -0.1594, -0.1423],
          [-0.0164, -0.1052,  0.0251]]]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[[[-9.3723e-03,  8.8503e-02, -1.3942e-01],
          [-1.9106e-01, -1.7566e-01,  5.3479e-02],
          [-5.4070e-02,  2.7315e-02,  8.4507e-02]],

         [[ 5.1615e-03, -6.4667e-02, -1.2353e-01],
          [-8.1612e-02,  4.0175e-02,  1.2795e-01],
          [-6.0239e-02,  6.9083e-03, -7.8364e-02]],

         [[-4.5817e-02,  3.6597e-02, -7.0675e-02],
          [-2.6277e-02,  1.4023e-02, -1.1777e-01],
          [ 1.0241e-01, -3.5692e-02, -1.6361e-01]],

         ...,

         [[-1.8003e-02, -7.9602e-02, -1.0971e-01],
          [-3.9657e-03,  9.2719e-02,  1.9064e-02],
          [-3.5036e-02,  2.2565e-02,  3.8324e-02]],

         [[-5.3749e-02,  1.1677e-03, -8.6887e-02],
          [ 8.1016e-03, -1.4984e-01, -2.5804e-02],
          [ 4.7494e-02,  3.1660e-02, -1.4414e-02]],

         [[ 1.3359e-02,  4.3634e-02, -6.4009e-02],
          [ 5.9640e-02, -1.5412e-02, -1.2275e-02],
          [-9.4093e-02,  7.0500e-02,  1.6120e-01]]],


        [[[ 7.9965e-02,  7.5508e-02, -2.9533e-02],
          [ 2.1492e-02, -5.8090e-02, -8.3662e-02],
          [-3.3484e-02,  1.0252e-01, -2.2235e-02]],

         [[-2.5525e-02, -8.4122e-02, -1.5829e-02],
          [ 9.9174e-02, -2.1633e-01, -1.2493e-01],
          [ 3.7101e-02,  5.7740e-02,  6.3547e-02]],

         [[-8.2252e-02,  1.3882e-01,  4.8095e-02],
          [-1.0751e-01,  6.7604e-02,  5.2779e-02],
          [-1.9475e-01,  1.2759e-02,  1.5458e-02]],

         ...,

         [[ 5.6265e-02, -9.2414e-03,  3.4176e-03],
          [ 5.0622e-02,  1.2395e-01, -7.4635e-02],
          [ 6.1438e-03, -6.6590e-02, -7.5796e-03]],

         [[-4.1029e-02, -9.4164e-02,  4.1226e-02],
          [-6.5169e-02,  2.1438e-02,  7.3540e-02],
          [-5.6854e-02,  3.5727e-02,  1.3365e-02]],

         [[-1.7394e-02,  2.3524e-02,  1.5555e-01],
          [-9.3803e-02, -1.3327e-01,  2.6814e-02],
          [ 1.2667e-01,  9.4852e-02,  7.8532e-02]]],


        [[[ 2.6987e-01, -1.0480e-01,  8.0710e-03],
          [-2.4981e-01, -7.2806e-02,  6.2215e-03],
          [-3.0784e-02,  1.4542e-03, -9.8570e-02]],

         [[ 1.7972e-01,  7.0035e-02,  9.2605e-02],
          [-9.1130e-02,  6.9005e-02,  1.4495e-02],
          [ 3.3115e-02,  7.4316e-04,  9.1506e-02]],

         [[ 6.9340e-02,  5.7690e-03,  1.7495e-01],
          [ 1.2404e-01, -4.3682e-02,  1.4432e-01],
          [-1.5388e-01, -9.9462e-02, -7.3262e-02]],

         ...,

         [[-9.4967e-02,  6.5006e-02, -4.4037e-03],
          [ 3.0454e-02, -7.4577e-02, -1.6702e-02],
          [ 1.5971e-01,  5.8408e-02, -5.2003e-03]],

         [[-1.3071e-02,  2.8663e-02, -5.7926e-02],
          [-1.6372e-01, -7.9469e-02, -1.0886e-01],
          [-1.1609e-02,  9.2361e-03, -1.4226e-02]],

         [[-7.5463e-02, -9.5355e-02,  2.3839e-02],
          [-1.1919e-01, -1.9255e-01,  2.1003e-02],
          [-1.9515e-03, -9.3511e-03, -1.3876e-01]]],


        ...,


        [[[ 1.2826e-02,  9.8554e-02,  1.0366e-01],
          [-1.1196e-01,  8.0350e-02,  2.2268e-02],
          [ 6.7263e-02, -5.9147e-02,  6.8370e-02]],

         [[-8.9849e-03, -6.5853e-02,  4.8278e-02],
          [ 1.4713e-01, -6.0172e-02,  9.3937e-02],
          [-5.9029e-02, -3.1882e-02,  4.1954e-02]],

         [[ 4.4621e-02, -7.0867e-02,  1.2509e-01],
          [ 5.3029e-02, -1.3489e-02,  5.9714e-02],
          [ 1.2540e-01, -5.2697e-04,  3.3463e-02]],

         ...,

         [[-1.1409e-01,  1.7906e-01, -1.2201e-02],
          [ 2.8885e-02,  1.4867e-01, -2.4937e-02],
          [-7.8688e-03, -2.6530e-02, -1.2739e-01]],

         [[ 1.2116e-02,  1.6458e-01, -1.2254e-01],
          [-1.4348e-02, -3.8360e-02, -9.2297e-02],
          [ 1.4539e-02,  1.4699e-02, -2.2779e-02]],

         [[-4.8218e-02, -7.4399e-03,  2.1115e-02],
          [-6.2967e-02, -9.2519e-03, -4.2308e-02],
          [-1.1799e-01, -3.5251e-02,  1.2575e-01]]],


        [[[ 8.3872e-02, -3.5535e-02, -2.8776e-02],
          [ 6.8894e-02,  1.5811e-01,  2.0448e-02],
          [-3.5080e-02,  8.1515e-02,  9.0694e-02]],

         [[ 2.7758e-02,  2.6160e-02, -8.6261e-02],
          [ 9.2314e-02,  2.9613e-02,  2.1451e-01],
          [ 7.6781e-02, -8.8102e-02,  1.4670e-01]],

         [[ 1.0780e-01, -3.6096e-02,  1.3159e-01],
          [ 3.5512e-02,  1.8209e-02, -1.1470e-01],
          [ 2.2782e-02,  7.3565e-02, -4.3926e-02]],

         ...,

         [[ 3.6422e-02,  3.6428e-02, -7.6654e-02],
          [ 3.0300e-02,  1.6022e-01,  1.9778e-02],
          [-3.0289e-02,  5.8729e-02, -8.7510e-03]],

         [[ 1.0432e-02, -6.5357e-02, -8.1067e-02],
          [-9.2461e-02,  1.8807e-01,  2.5712e-02],
          [ 6.3252e-02, -1.4847e-01, -5.5343e-03]],

         [[ 7.1972e-02, -8.2859e-02,  3.5418e-02],
          [-7.1892e-02,  9.1138e-02, -9.7911e-03],
          [-2.5063e-02, -8.5155e-03, -6.8637e-02]]],


        [[[-4.4243e-02,  1.6822e-01,  3.0474e-02],
          [ 1.2278e-02, -3.6934e-02,  1.5776e-02],
          [ 9.3130e-02, -2.6501e-02,  6.0843e-03]],

         [[ 6.5029e-02,  5.1315e-02,  1.1859e-02],
          [-1.5172e-01,  1.8501e-01,  1.2071e-01],
          [ 6.6259e-02,  2.7809e-02,  5.0358e-02]],

         [[ 1.4289e-01, -3.3299e-03,  4.9340e-02],
          [-1.8997e-01, -7.6563e-02,  8.4327e-02],
          [-1.9702e-01, -3.3417e-02, -5.2616e-03]],

         ...,

         [[ 4.7977e-02, -1.2885e-01, -5.3332e-02],
          [-2.4081e-04, -2.6891e-02, -3.3097e-02],
          [-1.2962e-01, -5.1186e-02,  3.5847e-02]],

         [[ 5.7802e-02, -5.8108e-02, -9.6481e-02],
          [ 3.8635e-02,  7.6062e-02, -1.5119e-01],
          [ 3.9356e-02,  3.1755e-02, -1.2878e-01]],

         [[-5.0874e-03, -1.7638e-02,  2.2504e-02],
          [ 3.0943e-02,  1.5732e-02, -8.1387e-02],
          [-9.2129e-02,  1.0117e-02, -1.1274e-02]]]], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 2.5575e-01, -2.2616e-01,  1.2138e-02, -1.6143e-01, -1.7688e-01,
          1.5881e-01, -7.4864e-02,  3.5174e-02,  2.3956e-02,  1.0910e-02,
         -1.4965e-01, -1.4741e-01,  1.1070e-01,  9.9303e-02, -3.1104e-02,
          1.4512e-02,  3.4215e-02,  4.8780e-02, -2.9296e-02,  1.2748e-01,
         -1.4367e-01, -1.6873e-01, -1.6357e-01, -3.1748e-01,  1.6566e-01,
         -1.1441e-01,  1.4219e-01,  1.2814e-01, -2.5925e-01,  4.5496e-02,
         -8.7233e-02,  4.7634e-03,  7.8850e-02,  4.1030e-02, -1.0476e-01,
          1.5469e-01, -1.4354e-01, -8.5767e-02,  2.9858e-01,  1.0415e-01,
          1.2047e-01,  5.7294e-02, -9.8294e-03, -1.6791e-01,  4.3873e-02,
         -8.3593e-02,  4.8709e-02, -1.2077e-02, -4.2557e-02, -6.5276e-02,
          2.2373e-02, -4.4221e-02,  3.4812e-02,  3.5149e-01, -9.3448e-02,
          7.3604e-02, -3.0058e-02,  1.8004e-01,  1.7967e-01,  7.7336e-02,
         -9.4257e-03,  7.9969e-02,  3.5855e-02,  1.8041e-01, -1.1465e-01,
         -7.7587e-02,  2.0403e-01, -9.4056e-02,  8.4127e-02,  6.9546e-02,
         -9.6174e-02,  7.4340e-02, -5.4610e-02,  9.4245e-02, -1.8666e-01,
         -1.0451e-01,  1.0442e-01, -2.4095e-02,  1.3845e-02, -6.3597e-02,
         -3.6908e-01, -1.3472e-01, -6.3855e-02,  1.1438e-01,  2.0847e-01,
          6.2104e-02, -1.0528e-01,  2.2277e-02,  8.7963e-02,  1.8373e-01,
          2.5729e-01, -1.4770e-01,  8.1434e-02, -6.6826e-02, -4.4727e-02,
          1.0756e-04,  1.6524e-02, -3.8258e-03,  1.3119e-02, -1.6126e-01,
         -1.1613e-01, -1.9819e-01,  2.5213e-02, -5.3448e-02, -1.5438e-01,
          1.0856e-01, -1.0054e-01,  9.3646e-02,  1.1423e-01, -2.9621e-02,
         -2.0772e-02,  1.7359e-01, -8.6479e-02,  1.9506e-01,  1.0916e-01,
         -1.9131e-02,  5.8115e-02, -1.1329e-02,  1.3510e-01,  3.2573e-02,
         -1.3178e-01,  2.3433e-01, -1.5194e-01, -1.8360e-01,  2.3054e-01,
         -7.0329e-02, -8.5447e-02, -2.4089e-01],
        [ 5.9469e-02, -1.2745e-01, -1.5625e-01, -3.3221e-01, -8.0214e-02,
         -4.7599e-02, -8.3304e-02, -2.6363e-01, -7.3996e-02, -3.8923e-02,
          6.4608e-02, -2.2493e-01, -3.3524e-01,  1.0346e-01,  5.7675e-02,
          3.0249e-01,  3.3993e-02,  1.6674e-01,  1.3949e-01,  2.7732e-02,
         -1.1300e-01, -7.3135e-03, -2.2404e-01, -9.7525e-02,  4.5376e-03,
          4.9188e-02,  3.2993e-02, -3.9405e-03, -5.9755e-02,  2.2019e-01,
         -4.9122e-02,  4.4703e-02, -1.0026e-01, -1.4430e-01,  3.4100e-02,
          1.3424e-02,  2.1625e-02, -1.0911e-01,  7.4844e-02, -2.1272e-02,
          1.5466e-01, -1.9877e-01,  1.8198e-01, -5.6346e-02, -9.4978e-02,
         -1.6764e-01, -8.7375e-02,  3.8393e-03, -2.3359e-02, -5.8611e-02,
          7.9721e-02, -2.8836e-02,  1.9736e-01,  1.5739e-01, -4.0419e-02,
          2.7019e-01,  5.2480e-02, -7.5247e-02, -2.4144e-01,  7.1823e-03,
         -3.5973e-01, -1.1894e-01, -1.7337e-02,  2.3442e-02, -7.2292e-02,
         -1.5525e-01, -6.4865e-02, -4.0929e-02,  4.5251e-02,  1.2874e-01,
         -1.6703e-01,  1.0933e-01,  9.1289e-02, -9.2225e-02,  7.1061e-02,
         -1.3583e-02,  8.2706e-02, -8.7374e-02, -1.6590e-01, -8.5730e-02,
         -3.7106e-01,  7.2929e-02,  7.5556e-02,  1.5043e-01,  1.0557e-01,
         -2.4288e-01,  1.0302e-01,  4.6611e-02,  2.0177e-01,  3.2565e-02,
          1.5733e-02, -6.8659e-02,  1.4560e-01,  8.3242e-03, -1.3555e-01,
          6.6565e-02, -5.8277e-04,  4.3378e-02, -2.3053e-01,  1.1223e-01,
          2.0319e-01, -1.7594e-02, -2.4993e-02, -3.3843e-02,  6.4526e-02,
         -6.8080e-02,  1.9605e-01, -5.9024e-02, -1.9106e-02,  1.1911e-01,
         -1.0595e-01, -1.7967e-02,  1.8193e-01,  2.5832e-02,  2.2918e-01,
          1.8263e-01, -2.2156e-01, -1.3463e-01,  5.9043e-02,  3.2417e-02,
          7.7828e-02,  2.6523e-02,  2.2741e-01,  2.5747e-01, -2.1174e-01,
         -2.0535e-02,  7.2730e-02,  2.0499e-02],
        [ 3.4486e-02,  1.5890e-01, -1.1134e-01,  3.0476e-02,  2.1700e-01,
          2.0899e-02,  1.2558e-01,  1.0423e-01, -3.0554e-01,  9.6042e-02,
         -9.6281e-02, -1.7612e-01,  8.6795e-02, -1.9303e-02,  2.1304e-01,
         -5.8016e-02,  4.6974e-02,  1.5681e-01,  8.2659e-02,  2.5119e-02,
         -2.8859e-02,  7.3164e-02,  6.2171e-02, -1.5929e-01,  1.4137e-01,
          1.3150e-01, -1.0224e-01, -1.1662e-01,  7.7323e-02,  9.8404e-02,
          7.3025e-02, -6.0586e-03,  1.0574e-01,  1.3038e-01, -1.5708e-01,
         -6.4498e-02, -4.8999e-04, -1.5435e-01,  9.2656e-03,  1.0558e-01,
          1.3882e-01,  1.9543e-01,  1.4502e-01,  8.2647e-02, -2.8289e-02,
         -1.3265e-01, -6.0056e-02, -1.0034e-01, -5.5794e-02, -1.4654e-01,
         -2.2549e-02,  1.0981e-01, -7.8435e-02, -9.2881e-02, -1.1099e-01,
          1.2837e-01, -5.8888e-02,  7.9652e-02, -5.9574e-03,  1.6948e-01,
          4.5601e-03, -3.8854e-02, -1.0899e-01,  1.2331e-01,  6.1927e-02,
          1.4176e-01,  2.0743e-02,  8.4228e-02,  1.6973e-02, -3.8464e-02,
         -7.2107e-03, -1.9130e-01,  1.4258e-02, -1.1620e-01, -5.2369e-02,
          1.1886e-02,  3.3900e-02,  1.4430e-02,  2.9286e-02,  8.2881e-02,
         -2.6055e-01, -6.7293e-02, -6.5320e-02,  9.1373e-02,  1.5635e-02,
         -2.4901e-01, -7.7248e-02, -1.7645e-01,  2.6387e-01, -7.3437e-02,
          1.7392e-01,  6.5803e-02, -1.8490e-02, -3.2779e-02,  1.2121e-01,
          9.1414e-02,  1.1608e-01, -4.2179e-02, -7.0516e-02, -1.2003e-01,
         -2.2821e-01,  1.0018e-01, -9.2981e-02,  4.3827e-02, -1.1745e-01,
         -8.9839e-02,  1.6520e-03,  1.4335e-01, -7.7978e-02,  8.7757e-02,
          1.3496e-01, -6.8013e-02,  1.8518e-01,  1.5685e-01, -1.9760e-01,
          1.0389e-01,  1.4636e-01,  1.8588e-01,  1.0327e-01,  3.2741e-02,
         -3.0538e-01,  1.9996e-01, -1.5884e-01,  1.0731e-02, -1.6038e-02,
          1.6349e-01, -3.7836e-03,  1.3348e-01],
        [ 6.8690e-03,  7.2087e-02, -1.5526e-01,  7.3822e-02, -5.8680e-02,
         -4.0340e-02, -1.1945e-01,  6.9484e-02, -3.9268e-02, -7.7164e-02,
          2.0623e-01,  2.4375e-01,  1.1428e-01, -1.4062e-01, -7.2481e-02,
          3.1798e-02,  7.5243e-02, -2.6661e-01,  2.2656e-02,  2.9298e-03,
          2.0764e-01, -8.6185e-02,  1.0863e-01, -2.0940e-01,  2.3635e-01,
         -1.1738e-01, -2.8758e-01,  1.9744e-01,  1.9385e-01, -2.1574e-01,
          1.0534e-02,  1.2591e-01, -3.7922e-02, -3.4872e-02,  1.9237e-01,
          5.3939e-02, -3.8142e-02, -1.0599e-01,  1.2186e-01, -1.7594e-01,
          2.9242e-01, -5.3643e-02,  1.3493e-02,  1.8698e-03, -1.0989e-01,
         -2.2383e-01, -9.3132e-02,  2.2563e-01,  2.1078e-01,  7.0553e-02,
         -2.0231e-02,  3.7130e-02, -5.4324e-02, -4.6622e-02, -8.4576e-02,
          1.5842e-01, -9.4806e-02, -2.6993e-02, -3.7102e-03,  1.3321e-02,
         -1.6757e-01, -6.4263e-03, -1.5198e-02,  9.5173e-02, -7.1552e-02,
         -1.5192e-01,  4.5290e-02, -5.9917e-02, -5.4929e-02,  2.8933e-01,
         -1.7404e-01,  1.2791e-01,  1.2825e-01,  9.8895e-03,  1.5561e-01,
          1.0077e-03,  1.4449e-01,  1.5280e-01,  2.1799e-02,  1.2699e-02,
          8.7028e-02, -1.1017e-01,  2.3765e-01, -1.7179e-01, -1.3168e-01,
         -1.0791e-02,  1.1125e-01, -1.0051e-01, -9.0571e-02,  8.7187e-02,
          1.8668e-01,  2.0252e-01, -3.6115e-01,  1.9891e-02,  2.6947e-01,
         -1.0789e-01, -1.9999e-01, -2.2529e-01, -1.6325e-01,  1.7260e-01,
         -1.2162e-01, -8.7780e-02,  9.6194e-04,  2.8729e-01,  8.4374e-02,
          1.2589e-02, -1.4081e-01,  2.7723e-01, -1.0439e-01, -1.3682e-01,
         -1.8903e-01,  2.2141e-01,  1.1629e-01, -8.2116e-02,  2.5889e-02,
          2.9053e-01, -1.8890e-01,  1.0653e-01,  1.4178e-01,  4.6680e-02,
         -1.0455e-01, -1.3223e-01, -1.6340e-01, -1.4298e-01,  4.3268e-02,
          2.7385e-03,  2.1968e-01, -4.5721e-03],
        [-1.1135e-01, -2.0247e-01, -6.1960e-03, -1.0051e-02, -1.0711e-01,
          2.8310e-02, -2.7357e-02,  1.4975e-01,  2.4116e-01,  7.0758e-02,
          1.8812e-01, -1.5485e-01, -5.1438e-02, -1.1685e-01, -2.2138e-02,
         -1.8574e-03, -5.9382e-02,  3.2014e-02,  3.1197e-01,  1.2623e-02,
          1.1200e-01,  1.7455e-02,  1.5443e-01, -1.3444e-01,  1.0365e-01,
          1.2530e-01,  4.5665e-01,  4.4347e-02,  2.4304e-01,  1.1284e-02,
         -3.0097e-02, -4.8599e-02,  1.1148e-01, -3.5880e-02, -7.4801e-02,
          1.5372e-02,  2.0527e-01, -2.0952e-02, -1.8737e-01,  1.3361e-01,
          2.9763e-02,  2.3260e-01,  1.2081e-01, -2.4930e-02,  2.2497e-02,
          5.7749e-02,  1.3542e-01,  2.1825e-02,  1.1850e-01, -6.5299e-02,
          6.8391e-02, -1.5084e-01,  4.4161e-02,  3.4822e-02,  7.7926e-02,
          5.4040e-02,  8.8060e-02,  1.0937e-01, -2.1010e-01, -3.0415e-02,
          7.7391e-02, -2.9974e-01,  4.1121e-02, -8.2669e-02,  1.8157e-01,
          3.6435e-03,  6.3757e-02, -4.0017e-03, -1.5571e-01,  6.9980e-02,
         -2.6533e-02,  1.1554e-01, -8.6663e-02, -5.9925e-02, -1.3396e-01,
         -1.6075e-02, -2.0927e-01,  1.8834e-01, -2.9465e-03, -8.4523e-02,
         -1.2590e-01,  9.9004e-02,  9.5817e-02,  1.7482e-01, -1.3512e-01,
          1.3345e-01,  1.0236e-01, -4.6653e-02,  2.8500e-02,  1.1198e-01,
          1.3646e-01, -1.6913e-01,  1.6797e-01, -2.1254e-01,  3.1688e-01,
         -2.5531e-01, -1.2419e-01,  1.1792e-01, -2.2336e-02,  2.0475e-01,
         -1.0549e-01, -1.6870e-01, -3.0296e-02,  2.3503e-01, -1.3500e-02,
         -5.9394e-02,  1.7685e-01,  2.1291e-01, -2.8371e-02, -3.2097e-01,
          1.4259e-01, -2.9379e-02,  1.1744e-01, -7.9471e-02,  8.1902e-02,
         -4.5400e-02,  9.4454e-02,  3.0616e-02,  1.5046e-01, -1.4002e-01,
         -1.2277e-02, -1.7352e-01,  1.8550e-01, -4.9171e-02, -3.4671e-01,
          1.3016e-01,  1.2294e-01,  9.8981e-02]], device='cuda:0',
       requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)]
(Pdb) Traceback (most recent call last):
  File "train_psgd_new.py", line 619, in <module>
    main()
  File "train_psgd_new.py", line 303, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 381, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    for name,param in enumerate(model.net.parameters()):
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    for name,param in enumerate(model.net.parameters()):
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
> /home/amax/luoqin/MAML-Pytorch/train_psgd_new.py(168)get_model_grad_vec()
-> for name,param in enumerate(model.net.parameters()):
(Pdb) Traceback (most recent call last):
  File "train_psgd_new.py", line 619, in <module>
    main()
  File "train_psgd_new.py", line 303, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File "train_psgd_new.py", line 381, in train
    gk = get_model_grad_vec(model)
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    for name,param in enumerate(model.net.parameters()):
  File "train_psgd_new.py", line 168, in get_model_grad_vec
    for name,param in enumerate(model.net.parameters()):
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/amax/anaconda3/envs/luoqin/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
Epoch: [0][0/20]	Time 1.106 (1.106)	Data 0.109 (0.109)	Loss 2.3456 (2.3456)	Prec@1 24.219 (24.219)
Epoch: [0][19/20]	Time 0.033 (0.062)	Data 0.000 (0.006)	Loss 1.1672 (1.4869)	Prec@1 54.412 (47.960)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.1141 (1.1141)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
58.66666793823242
Epoch: [1][0/20]	Time 0.152 (0.152)	Data 0.138 (0.138)	Loss 1.1472 (1.1472)	Prec@1 57.812 (57.812)
Epoch: [1][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 1.3062 (1.1533)	Prec@1 54.412 (56.280)
Test: [0/1]	Time 0.132 (0.132)	Loss 0.9152 (0.9152)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
65.33333587646484
Epoch: [2][0/20]	Time 0.159 (0.159)	Data 0.146 (0.146)	Loss 1.3711 (1.3711)	Prec@1 53.906 (53.906)
Epoch: [2][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 1.2043 (1.0833)	Prec@1 57.353 (58.040)
Test: [0/1]	Time 0.147 (0.147)	Loss 0.9156 (0.9156)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [3][0/20]	Time 0.171 (0.171)	Data 0.163 (0.163)	Loss 1.0418 (1.0418)	Prec@1 56.250 (56.250)
Epoch: [3][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 1.0618 (1.0365)	Prec@1 54.412 (59.720)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.8807 (0.8807)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
65.33333587646484
Epoch: [4][0/20]	Time 0.129 (0.129)	Data 0.121 (0.121)	Loss 1.1323 (1.1323)	Prec@1 54.688 (54.688)
Epoch: [4][19/20]	Time 0.006 (0.015)	Data 0.000 (0.007)	Loss 0.8797 (1.0140)	Prec@1 63.235 (59.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 0.8754 (0.8754)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [5][0/20]	Time 0.157 (0.157)	Data 0.145 (0.145)	Loss 1.1169 (1.1169)	Prec@1 57.031 (57.031)
Epoch: [5][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 1.3302 (1.0384)	Prec@1 50.000 (58.520)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.8794 (0.8794)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [6][0/20]	Time 0.151 (0.151)	Data 0.138 (0.138)	Loss 0.9831 (0.9831)	Prec@1 57.812 (57.812)
Epoch: [6][19/20]	Time 0.008 (0.015)	Data 0.000 (0.007)	Loss 1.1868 (1.0299)	Prec@1 54.412 (58.720)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.8846 (0.8846)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [7][0/20]	Time 0.159 (0.159)	Data 0.150 (0.150)	Loss 1.1244 (1.1244)	Prec@1 57.031 (57.031)
Epoch: [7][19/20]	Time 0.007 (0.015)	Data 0.000 (0.008)	Loss 1.0298 (1.0264)	Prec@1 60.294 (57.960)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.8573 (0.8573)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [8][0/20]	Time 0.157 (0.157)	Data 0.151 (0.151)	Loss 1.0523 (1.0523)	Prec@1 58.594 (58.594)
Epoch: [8][19/20]	Time 0.005 (0.015)	Data 0.000 (0.009)	Loss 1.1043 (1.0255)	Prec@1 54.412 (59.600)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.8906 (0.8906)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [9][0/20]	Time 0.150 (0.150)	Data 0.142 (0.142)	Loss 0.9079 (0.9079)	Prec@1 64.844 (64.844)
Epoch: [9][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.9951 (1.0324)	Prec@1 54.412 (59.200)
Test: [0/1]	Time 0.130 (0.130)	Loss 0.9259 (0.9259)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [10][0/20]	Time 0.162 (0.162)	Data 0.151 (0.151)	Loss 0.8923 (0.8923)	Prec@1 56.250 (56.250)
Epoch: [10][19/20]	Time 0.004 (0.013)	Data 0.000 (0.008)	Loss 1.2995 (0.9944)	Prec@1 51.471 (59.360)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.9159 (0.9159)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [11][0/20]	Time 0.158 (0.158)	Data 0.152 (0.152)	Loss 0.9910 (0.9910)	Prec@1 59.375 (59.375)
Epoch: [11][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 0.9424 (1.0109)	Prec@1 58.824 (59.520)
Test: [0/1]	Time 0.119 (0.119)	Loss 0.9076 (0.9076)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [12][0/20]	Time 0.154 (0.154)	Data 0.145 (0.145)	Loss 1.0499 (1.0499)	Prec@1 58.594 (58.594)
Epoch: [12][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.8001 (1.0100)	Prec@1 63.235 (59.320)
Test: [0/1]	Time 0.121 (0.121)	Loss 0.8935 (0.8935)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [13][0/20]	Time 0.158 (0.158)	Data 0.145 (0.145)	Loss 1.0173 (1.0173)	Prec@1 60.156 (60.156)
Epoch: [13][19/20]	Time 0.007 (0.015)	Data 0.000 (0.007)	Loss 1.0451 (1.0149)	Prec@1 50.000 (56.800)
Test: [0/1]	Time 0.115 (0.115)	Loss 0.8996 (0.8996)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [14][0/20]	Time 0.351 (0.351)	Data 0.346 (0.346)	Loss 1.0301 (1.0301)	Prec@1 59.375 (59.375)
Epoch: [14][19/20]	Time 0.003 (0.029)	Data 0.000 (0.025)	Loss 0.8468 (0.9858)	Prec@1 67.647 (61.720)
Test: [0/1]	Time 0.222 (0.222)	Loss 0.9061 (0.9061)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [15][0/20]	Time 0.338 (0.338)	Data 0.324 (0.324)	Loss 0.9728 (0.9728)	Prec@1 65.625 (65.625)
Epoch: [15][19/20]	Time 0.005 (0.026)	Data 0.000 (0.020)	Loss 1.0794 (0.9786)	Prec@1 57.353 (60.120)
Test: [0/1]	Time 0.192 (0.192)	Loss 0.9060 (0.9060)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [16][0/20]	Time 0.357 (0.357)	Data 0.349 (0.349)	Loss 0.9637 (0.9637)	Prec@1 55.469 (55.469)
Epoch: [16][19/20]	Time 0.008 (0.029)	Data 0.000 (0.021)	Loss 1.0689 (0.9713)	Prec@1 57.353 (60.160)
Test: [0/1]	Time 0.171 (0.171)	Loss 0.9129 (0.9129)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [17][0/20]	Time 0.148 (0.148)	Data 0.137 (0.137)	Loss 1.0967 (1.0967)	Prec@1 59.375 (59.375)
Epoch: [17][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.9624 (0.9720)	Prec@1 52.941 (60.600)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.9208 (0.9208)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [18][0/20]	Time 0.147 (0.147)	Data 0.133 (0.133)	Loss 0.8056 (0.8056)	Prec@1 67.188 (67.188)
Epoch: [18][19/20]	Time 0.008 (0.014)	Data 0.000 (0.007)	Loss 0.9589 (0.9812)	Prec@1 63.235 (60.800)
Test: [0/1]	Time 0.131 (0.131)	Loss 0.9306 (0.9306)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [19][0/20]	Time 0.162 (0.162)	Data 0.152 (0.152)	Loss 0.9931 (0.9931)	Prec@1 61.719 (61.719)
Epoch: [19][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 1.1091 (0.9743)	Prec@1 51.471 (61.040)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9118 (0.9118)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [20][0/20]	Time 0.194 (0.194)	Data 0.187 (0.187)	Loss 1.0615 (1.0615)	Prec@1 57.812 (57.812)
Epoch: [20][19/20]	Time 0.004 (0.016)	Data 0.000 (0.009)	Loss 0.9872 (0.9751)	Prec@1 55.882 (60.920)
Test: [0/1]	Time 0.121 (0.121)	Loss 0.9122 (0.9122)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [21][0/20]	Time 0.158 (0.158)	Data 0.145 (0.145)	Loss 1.0318 (1.0318)	Prec@1 60.156 (60.156)
Epoch: [21][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.9798 (0.9747)	Prec@1 58.824 (61.200)
Test: [0/1]	Time 0.126 (0.126)	Loss 0.9430 (0.9430)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [22][0/20]	Time 0.150 (0.150)	Data 0.142 (0.142)	Loss 0.9454 (0.9454)	Prec@1 60.156 (60.156)
Epoch: [22][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 1.0900 (0.9532)	Prec@1 58.824 (62.080)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9423 (0.9423)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
65.33333587646484
Epoch: [23][0/20]	Time 0.148 (0.148)	Data 0.134 (0.134)	Loss 0.8981 (0.8981)	Prec@1 64.844 (64.844)
Epoch: [23][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.9959 (0.9916)	Prec@1 60.294 (60.320)
Test: [0/1]	Time 0.134 (0.134)	Loss 0.9526 (0.9526)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
65.33333587646484
Epoch: [24][0/20]	Time 0.152 (0.152)	Data 0.139 (0.139)	Loss 0.9349 (0.9349)	Prec@1 64.844 (64.844)
Epoch: [24][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 0.8951 (0.9766)	Prec@1 70.588 (61.600)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9298 (0.9298)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [25][0/20]	Time 0.168 (0.168)	Data 0.155 (0.155)	Loss 0.8493 (0.8493)	Prec@1 64.844 (64.844)
Epoch: [25][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.8436 (0.9603)	Prec@1 73.529 (60.920)
Test: [0/1]	Time 0.121 (0.121)	Loss 0.9565 (0.9565)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
65.33333587646484
Epoch: [26][0/20]	Time 0.163 (0.163)	Data 0.152 (0.152)	Loss 0.9199 (0.9199)	Prec@1 61.719 (61.719)
Epoch: [26][19/20]	Time 0.007 (0.015)	Data 0.000 (0.008)	Loss 0.8997 (0.9697)	Prec@1 64.706 (60.480)
Test: [0/1]	Time 0.126 (0.126)	Loss 0.9579 (0.9579)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [27][0/20]	Time 0.164 (0.164)	Data 0.151 (0.151)	Loss 0.9633 (0.9633)	Prec@1 63.281 (63.281)
Epoch: [27][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 1.0799 (0.9541)	Prec@1 57.353 (62.040)
Test: [0/1]	Time 0.121 (0.121)	Loss 0.9402 (0.9402)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [28][0/20]	Time 0.152 (0.152)	Data 0.140 (0.140)	Loss 1.0072 (1.0072)	Prec@1 60.156 (60.156)
Epoch: [28][19/20]	Time 0.006 (0.015)	Data 0.000 (0.007)	Loss 1.1511 (0.9663)	Prec@1 58.824 (61.080)
Test: [0/1]	Time 0.130 (0.130)	Loss 0.9845 (0.9845)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [29][0/20]	Time 0.154 (0.154)	Data 0.143 (0.143)	Loss 1.0111 (1.0111)	Prec@1 55.469 (55.469)
Epoch: [29][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 1.0960 (0.9711)	Prec@1 57.353 (61.160)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9705 (0.9705)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [30][0/20]	Time 0.148 (0.148)	Data 0.140 (0.140)	Loss 1.0519 (1.0519)	Prec@1 57.031 (57.031)
Epoch: [30][19/20]	Time 0.007 (0.014)	Data 0.000 (0.007)	Loss 0.9182 (0.9570)	Prec@1 60.294 (61.600)
Test: [0/1]	Time 0.131 (0.131)	Loss 0.9437 (0.9437)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [31][0/20]	Time 0.133 (0.133)	Data 0.126 (0.126)	Loss 0.8776 (0.8776)	Prec@1 63.281 (63.281)
Epoch: [31][19/20]	Time 0.007 (0.014)	Data 0.000 (0.007)	Loss 1.0945 (0.9490)	Prec@1 54.412 (61.760)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9354 (0.9354)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [32][0/20]	Time 0.139 (0.139)	Data 0.133 (0.133)	Loss 0.9032 (0.9032)	Prec@1 64.844 (64.844)
Epoch: [32][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.9996 (0.9793)	Prec@1 61.765 (60.120)
Test: [0/1]	Time 0.122 (0.122)	Loss 0.9232 (0.9232)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [33][0/20]	Time 0.149 (0.149)	Data 0.136 (0.136)	Loss 1.1218 (1.1218)	Prec@1 55.469 (55.469)
Epoch: [33][19/20]	Time 0.008 (0.014)	Data 0.000 (0.007)	Loss 0.9504 (0.9879)	Prec@1 66.176 (60.240)
Test: [0/1]	Time 0.126 (0.126)	Loss 0.9256 (0.9256)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [34][0/20]	Time 0.160 (0.160)	Data 0.145 (0.145)	Loss 1.1779 (1.1779)	Prec@1 50.781 (50.781)
Epoch: [34][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.9974 (0.9388)	Prec@1 54.412 (60.800)
Test: [0/1]	Time 0.123 (0.123)	Loss 0.9230 (0.9230)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [35][0/20]	Time 0.166 (0.166)	Data 0.153 (0.153)	Loss 0.9780 (0.9780)	Prec@1 60.156 (60.156)
Epoch: [35][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 0.9340 (0.9418)	Prec@1 66.176 (62.280)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9213 (0.9213)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [36][0/20]	Time 0.151 (0.151)	Data 0.138 (0.138)	Loss 0.9750 (0.9750)	Prec@1 60.938 (60.938)
Epoch: [36][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.9038 (0.9424)	Prec@1 57.353 (61.400)
Test: [0/1]	Time 0.116 (0.116)	Loss 0.9191 (0.9191)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [37][0/20]	Time 0.163 (0.163)	Data 0.151 (0.151)	Loss 0.9462 (0.9462)	Prec@1 60.938 (60.938)
Epoch: [37][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 1.0521 (0.9562)	Prec@1 60.294 (60.640)
Test: [0/1]	Time 0.142 (0.142)	Loss 0.9209 (0.9209)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [38][0/20]	Time 0.165 (0.165)	Data 0.156 (0.156)	Loss 0.9101 (0.9101)	Prec@1 60.156 (60.156)
Epoch: [38][19/20]	Time 0.005 (0.015)	Data 0.000 (0.009)	Loss 0.9687 (0.9632)	Prec@1 58.824 (61.920)
Test: [0/1]	Time 0.143 (0.143)	Loss 0.9236 (0.9236)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [39][0/20]	Time 0.145 (0.145)	Data 0.137 (0.137)	Loss 0.8896 (0.8896)	Prec@1 65.625 (65.625)
Epoch: [39][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 0.8622 (0.9562)	Prec@1 60.294 (60.680)
Test: [0/1]	Time 0.142 (0.142)	Loss 0.9357 (0.9357)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [40][0/20]	Time 0.161 (0.161)	Data 0.148 (0.148)	Loss 0.9607 (0.9607)	Prec@1 63.281 (63.281)
Epoch: [40][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.8883 (0.9463)	Prec@1 57.353 (61.440)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9307 (0.9307)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [41][0/20]	Time 0.159 (0.159)	Data 0.146 (0.146)	Loss 0.9776 (0.9776)	Prec@1 51.562 (51.562)
Epoch: [41][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 1.0507 (0.9363)	Prec@1 51.471 (62.160)
Test: [0/1]	Time 0.117 (0.117)	Loss 0.9052 (0.9052)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [42][0/20]	Time 0.160 (0.160)	Data 0.153 (0.153)	Loss 1.0352 (1.0352)	Prec@1 55.469 (55.469)
Epoch: [42][19/20]	Time 0.006 (0.014)	Data 0.000 (0.008)	Loss 1.1268 (0.9642)	Prec@1 57.353 (61.520)
Test: [0/1]	Time 0.147 (0.147)	Loss 0.9177 (0.9177)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [43][0/20]	Time 0.353 (0.353)	Data 0.347 (0.347)	Loss 1.0490 (1.0490)	Prec@1 64.062 (64.062)
Epoch: [43][19/20]	Time 0.003 (0.026)	Data 0.000 (0.021)	Loss 1.0282 (0.9450)	Prec@1 52.941 (61.680)
Test: [0/1]	Time 0.115 (0.115)	Loss 0.9191 (0.9191)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [44][0/20]	Time 0.221 (0.221)	Data 0.213 (0.213)	Loss 0.9069 (0.9069)	Prec@1 57.031 (57.031)
Epoch: [44][19/20]	Time 0.006 (0.019)	Data 0.000 (0.013)	Loss 0.9339 (0.9632)	Prec@1 55.882 (59.920)
Test: [0/1]	Time 0.354 (0.354)	Loss 0.9180 (0.9180)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [45][0/20]	Time 0.446 (0.446)	Data 0.433 (0.433)	Loss 0.9424 (0.9424)	Prec@1 60.938 (60.938)
Epoch: [45][19/20]	Time 0.006 (0.028)	Data 0.000 (0.023)	Loss 1.0617 (0.9642)	Prec@1 58.824 (60.680)
Test: [0/1]	Time 0.119 (0.119)	Loss 0.9185 (0.9185)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [46][0/20]	Time 0.162 (0.162)	Data 0.149 (0.149)	Loss 1.0522 (1.0522)	Prec@1 57.812 (57.812)
Epoch: [46][19/20]	Time 0.005 (0.016)	Data 0.000 (0.008)	Loss 0.8400 (0.9560)	Prec@1 63.235 (61.800)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.9377 (0.9377)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [47][0/20]	Time 0.159 (0.159)	Data 0.149 (0.149)	Loss 0.8764 (0.8764)	Prec@1 67.188 (67.188)
Epoch: [47][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 1.0737 (0.9452)	Prec@1 61.765 (61.040)
Test: [0/1]	Time 0.132 (0.132)	Loss 0.9261 (0.9261)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [48][0/20]	Time 0.158 (0.158)	Data 0.148 (0.148)	Loss 0.8783 (0.8783)	Prec@1 60.938 (60.938)
Epoch: [48][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.8740 (0.9451)	Prec@1 64.706 (60.680)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9331 (0.9331)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [49][0/20]	Time 0.170 (0.170)	Data 0.160 (0.160)	Loss 0.9843 (0.9843)	Prec@1 59.375 (59.375)
Epoch: [49][19/20]	Time 0.006 (0.016)	Data 0.000 (0.009)	Loss 1.0480 (0.9556)	Prec@1 58.824 (61.160)
Test: [0/1]	Time 0.114 (0.114)	Loss 0.9500 (0.9500)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [50][0/20]	Time 0.168 (0.168)	Data 0.160 (0.160)	Loss 0.9548 (0.9548)	Prec@1 65.625 (65.625)
Epoch: [50][19/20]	Time 0.005 (0.015)	Data 0.000 (0.009)	Loss 0.7221 (0.9401)	Prec@1 69.118 (62.280)
Test: [0/1]	Time 0.123 (0.123)	Loss 0.9362 (0.9362)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [51][0/20]	Time 0.155 (0.155)	Data 0.144 (0.144)	Loss 0.9606 (0.9606)	Prec@1 58.594 (58.594)
Epoch: [51][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 1.0248 (0.9418)	Prec@1 63.235 (61.160)
Test: [0/1]	Time 0.123 (0.123)	Loss 0.9345 (0.9345)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [52][0/20]	Time 0.159 (0.159)	Data 0.146 (0.146)	Loss 1.0348 (1.0348)	Prec@1 59.375 (59.375)
Epoch: [52][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 0.8074 (0.9327)	Prec@1 67.647 (61.800)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.9303 (0.9303)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [53][0/20]	Time 0.162 (0.162)	Data 0.149 (0.149)	Loss 0.9049 (0.9049)	Prec@1 63.281 (63.281)
Epoch: [53][19/20]	Time 0.004 (0.015)	Data 0.000 (0.008)	Loss 0.8322 (0.9556)	Prec@1 60.294 (60.560)
Test: [0/1]	Time 0.112 (0.112)	Loss 0.9421 (0.9421)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [54][0/20]	Time 0.155 (0.155)	Data 0.145 (0.145)	Loss 0.8659 (0.8659)	Prec@1 64.844 (64.844)
Epoch: [54][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 1.1876 (0.9532)	Prec@1 52.941 (61.360)
Test: [0/1]	Time 0.143 (0.143)	Loss 0.9387 (0.9387)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [55][0/20]	Time 0.179 (0.179)	Data 0.168 (0.168)	Loss 1.1041 (1.1041)	Prec@1 49.219 (49.219)
Epoch: [55][19/20]	Time 0.004 (0.014)	Data 0.000 (0.009)	Loss 0.8105 (0.9392)	Prec@1 73.529 (61.440)
Test: [0/1]	Time 0.117 (0.117)	Loss 0.9519 (0.9519)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [56][0/20]	Time 0.155 (0.155)	Data 0.145 (0.145)	Loss 0.9758 (0.9758)	Prec@1 60.156 (60.156)
Epoch: [56][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.9548 (0.9502)	Prec@1 57.353 (61.320)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9560 (0.9560)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [57][0/20]	Time 0.200 (0.200)	Data 0.189 (0.189)	Loss 1.0286 (1.0286)	Prec@1 57.812 (57.812)
Epoch: [57][19/20]	Time 0.006 (0.016)	Data 0.000 (0.010)	Loss 0.8786 (0.9521)	Prec@1 66.176 (59.680)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9298 (0.9298)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
65.33333587646484
Epoch: [58][0/20]	Time 0.128 (0.128)	Data 0.122 (0.122)	Loss 1.0322 (1.0322)	Prec@1 55.469 (55.469)
Epoch: [58][19/20]	Time 0.007 (0.014)	Data 0.000 (0.007)	Loss 0.6797 (0.9593)	Prec@1 75.000 (60.840)
Test: [0/1]	Time 0.123 (0.123)	Loss 0.9489 (0.9489)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [59][0/20]	Time 0.149 (0.149)	Data 0.137 (0.137)	Loss 0.9132 (0.9132)	Prec@1 53.125 (53.125)
Epoch: [59][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 1.0378 (0.9489)	Prec@1 58.824 (60.600)
Test: [0/1]	Time 0.114 (0.114)	Loss 0.9269 (0.9269)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [60][0/20]	Time 0.157 (0.157)	Data 0.149 (0.149)	Loss 1.0072 (1.0072)	Prec@1 58.594 (58.594)
Epoch: [60][19/20]	Time 0.006 (0.014)	Data 0.000 (0.008)	Loss 0.9696 (0.9487)	Prec@1 63.235 (60.960)
Test: [0/1]	Time 0.151 (0.151)	Loss 0.9593 (0.9593)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [61][0/20]	Time 0.156 (0.156)	Data 0.144 (0.144)	Loss 0.8699 (0.8699)	Prec@1 60.156 (60.156)
Epoch: [61][19/20]	Time 0.007 (0.015)	Data 0.000 (0.007)	Loss 0.7986 (0.9224)	Prec@1 73.529 (61.960)
Test: [0/1]	Time 0.145 (0.145)	Loss 0.9587 (0.9587)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [62][0/20]	Time 0.152 (0.152)	Data 0.144 (0.144)	Loss 0.9451 (0.9451)	Prec@1 62.500 (62.500)
Epoch: [62][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.9243 (0.9699)	Prec@1 61.765 (59.480)
Test: [0/1]	Time 0.126 (0.126)	Loss 0.9625 (0.9625)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [63][0/20]	Time 0.155 (0.155)	Data 0.142 (0.142)	Loss 0.9779 (0.9779)	Prec@1 58.594 (58.594)
Epoch: [63][19/20]	Time 0.007 (0.015)	Data 0.000 (0.008)	Loss 0.9112 (0.9441)	Prec@1 60.294 (60.560)
Test: [0/1]	Time 0.124 (0.124)	Loss 0.9456 (0.9456)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [64][0/20]	Time 0.171 (0.171)	Data 0.165 (0.165)	Loss 0.8874 (0.8874)	Prec@1 61.719 (61.719)
Epoch: [64][19/20]	Time 0.007 (0.016)	Data 0.000 (0.009)	Loss 0.9162 (0.9383)	Prec@1 57.353 (61.800)
Test: [0/1]	Time 0.124 (0.124)	Loss 0.9339 (0.9339)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [65][0/20]	Time 0.158 (0.158)	Data 0.148 (0.148)	Loss 1.1042 (1.1042)	Prec@1 53.906 (53.906)
Epoch: [65][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.9875 (0.9595)	Prec@1 63.235 (60.280)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.9377 (0.9377)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
65.33333587646484
Epoch: [66][0/20]	Time 0.153 (0.153)	Data 0.139 (0.139)	Loss 1.0388 (1.0388)	Prec@1 56.250 (56.250)
Epoch: [66][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 0.6818 (0.9179)	Prec@1 77.941 (62.920)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9546 (0.9546)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [67][0/20]	Time 0.164 (0.164)	Data 0.153 (0.153)	Loss 1.0483 (1.0483)	Prec@1 57.812 (57.812)
Epoch: [67][19/20]	Time 0.005 (0.015)	Data 0.000 (0.010)	Loss 0.8986 (0.9444)	Prec@1 67.647 (61.680)
Test: [0/1]	Time 0.111 (0.111)	Loss 0.9425 (0.9425)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [68][0/20]	Time 0.143 (0.143)	Data 0.135 (0.135)	Loss 0.8731 (0.8731)	Prec@1 64.062 (64.062)
Epoch: [68][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 1.0110 (0.9360)	Prec@1 60.294 (61.400)
Test: [0/1]	Time 0.134 (0.134)	Loss 0.9299 (0.9299)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [69][0/20]	Time 0.160 (0.160)	Data 0.150 (0.150)	Loss 1.0017 (1.0017)	Prec@1 57.031 (57.031)
Epoch: [69][19/20]	Time 0.006 (0.016)	Data 0.000 (0.008)	Loss 1.0790 (0.9483)	Prec@1 54.412 (60.120)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9466 (0.9466)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [70][0/20]	Time 0.141 (0.141)	Data 0.131 (0.131)	Loss 0.9703 (0.9703)	Prec@1 63.281 (63.281)
Epoch: [70][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.9988 (0.9317)	Prec@1 61.765 (62.560)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9447 (0.9447)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [71][0/20]	Time 0.170 (0.170)	Data 0.157 (0.157)	Loss 0.9573 (0.9573)	Prec@1 61.719 (61.719)
Epoch: [71][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 0.9550 (0.9395)	Prec@1 55.882 (61.840)
Test: [0/1]	Time 0.198 (0.198)	Loss 0.9288 (0.9288)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [72][0/20]	Time 0.397 (0.397)	Data 0.391 (0.391)	Loss 1.0332 (1.0332)	Prec@1 57.812 (57.812)
Epoch: [72][19/20]	Time 0.003 (0.026)	Data 0.000 (0.021)	Loss 0.9544 (0.9359)	Prec@1 64.706 (61.880)
Test: [0/1]	Time 0.132 (0.132)	Loss 0.9297 (0.9297)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [73][0/20]	Time 0.302 (0.302)	Data 0.293 (0.293)	Loss 0.8999 (0.8999)	Prec@1 65.625 (65.625)
Epoch: [73][19/20]	Time 0.007 (0.025)	Data 0.000 (0.018)	Loss 0.8073 (0.9318)	Prec@1 69.118 (61.480)
Test: [0/1]	Time 0.324 (0.324)	Loss 0.9055 (0.9055)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [74][0/20]	Time 0.381 (0.381)	Data 0.372 (0.372)	Loss 0.9413 (0.9413)	Prec@1 57.031 (57.031)
Epoch: [74][19/20]	Time 0.004 (0.028)	Data 0.000 (0.021)	Loss 0.8455 (0.9311)	Prec@1 64.706 (61.320)
Test: [0/1]	Time 0.139 (0.139)	Loss 0.9216 (0.9216)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [75][0/20]	Time 0.173 (0.173)	Data 0.161 (0.161)	Loss 0.8616 (0.8616)	Prec@1 58.594 (58.594)
Epoch: [75][19/20]	Time 0.004 (0.016)	Data 0.000 (0.008)	Loss 0.9697 (0.9212)	Prec@1 60.294 (61.920)
Test: [0/1]	Time 0.142 (0.142)	Loss 0.9327 (0.9327)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [76][0/20]	Time 0.167 (0.167)	Data 0.155 (0.155)	Loss 0.8295 (0.8295)	Prec@1 64.844 (64.844)
Epoch: [76][19/20]	Time 0.007 (0.015)	Data 0.000 (0.008)	Loss 0.9148 (0.9232)	Prec@1 55.882 (62.240)
Test: [0/1]	Time 0.131 (0.131)	Loss 0.9315 (0.9315)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [77][0/20]	Time 0.171 (0.171)	Data 0.158 (0.158)	Loss 1.0787 (1.0787)	Prec@1 51.562 (51.562)
Epoch: [77][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.8441 (0.9511)	Prec@1 67.647 (59.800)
Test: [0/1]	Time 0.157 (0.157)	Loss 0.9114 (0.9114)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [78][0/20]	Time 0.161 (0.161)	Data 0.150 (0.150)	Loss 0.9958 (0.9958)	Prec@1 58.594 (58.594)
Epoch: [78][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.8249 (0.9500)	Prec@1 63.235 (60.680)
Test: [0/1]	Time 0.140 (0.140)	Loss 0.9321 (0.9321)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [79][0/20]	Time 0.153 (0.153)	Data 0.140 (0.140)	Loss 0.7980 (0.7980)	Prec@1 66.406 (66.406)
Epoch: [79][19/20]	Time 0.006 (0.015)	Data 0.000 (0.007)	Loss 0.8401 (0.9209)	Prec@1 66.176 (60.320)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9412 (0.9412)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [80][0/20]	Time 0.152 (0.152)	Data 0.142 (0.142)	Loss 0.8136 (0.8136)	Prec@1 71.094 (71.094)
Epoch: [80][19/20]	Time 0.008 (0.015)	Data 0.000 (0.007)	Loss 0.9210 (0.9294)	Prec@1 70.588 (63.160)
Test: [0/1]	Time 0.138 (0.138)	Loss 0.9364 (0.9364)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [81][0/20]	Time 0.174 (0.174)	Data 0.160 (0.160)	Loss 0.8735 (0.8735)	Prec@1 67.188 (67.188)
Epoch: [81][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 1.0030 (0.9290)	Prec@1 48.529 (61.200)
Test: [0/1]	Time 0.139 (0.139)	Loss 0.9428 (0.9428)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [82][0/20]	Time 0.162 (0.162)	Data 0.152 (0.152)	Loss 0.8447 (0.8447)	Prec@1 64.844 (64.844)
Epoch: [82][19/20]	Time 0.004 (0.013)	Data 0.000 (0.008)	Loss 0.9566 (0.9292)	Prec@1 63.235 (61.760)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.9295 (0.9295)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [83][0/20]	Time 0.176 (0.176)	Data 0.164 (0.164)	Loss 0.9081 (0.9081)	Prec@1 67.188 (67.188)
Epoch: [83][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 0.9531 (0.9434)	Prec@1 55.882 (60.560)
Test: [0/1]	Time 0.150 (0.150)	Loss 0.9243 (0.9243)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [84][0/20]	Time 0.162 (0.162)	Data 0.149 (0.149)	Loss 1.0514 (1.0514)	Prec@1 60.938 (60.938)
Epoch: [84][19/20]	Time 0.006 (0.016)	Data 0.000 (0.008)	Loss 0.8446 (0.9342)	Prec@1 66.176 (61.400)
Test: [0/1]	Time 0.135 (0.135)	Loss 0.9364 (0.9364)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [85][0/20]	Time 0.154 (0.154)	Data 0.146 (0.146)	Loss 0.9403 (0.9403)	Prec@1 60.938 (60.938)
Epoch: [85][19/20]	Time 0.007 (0.014)	Data 0.000 (0.007)	Loss 0.8030 (0.9587)	Prec@1 73.529 (60.400)
Test: [0/1]	Time 0.124 (0.124)	Loss 0.9272 (0.9272)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [86][0/20]	Time 0.168 (0.168)	Data 0.154 (0.154)	Loss 0.8671 (0.8671)	Prec@1 62.500 (62.500)
Epoch: [86][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.9755 (0.9292)	Prec@1 58.824 (60.800)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9368 (0.9368)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [87][0/20]	Time 0.148 (0.148)	Data 0.137 (0.137)	Loss 0.8992 (0.8992)	Prec@1 61.719 (61.719)
Epoch: [87][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.7267 (0.9362)	Prec@1 72.059 (61.480)
Test: [0/1]	Time 0.111 (0.111)	Loss 0.9376 (0.9376)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [88][0/20]	Time 0.155 (0.155)	Data 0.146 (0.146)	Loss 0.8505 (0.8505)	Prec@1 58.594 (58.594)
Epoch: [88][19/20]	Time 0.008 (0.015)	Data 0.000 (0.007)	Loss 0.7816 (0.9382)	Prec@1 70.588 (61.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 0.9296 (0.9296)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [89][0/20]	Time 0.167 (0.167)	Data 0.153 (0.153)	Loss 1.0594 (1.0594)	Prec@1 59.375 (59.375)
Epoch: [89][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 1.0626 (0.9404)	Prec@1 61.765 (61.160)
Test: [0/1]	Time 0.132 (0.132)	Loss 0.9191 (0.9191)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [90][0/20]	Time 0.170 (0.170)	Data 0.157 (0.157)	Loss 0.9657 (0.9657)	Prec@1 57.812 (57.812)
Epoch: [90][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.8553 (0.9378)	Prec@1 72.059 (60.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 0.9521 (0.9521)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [91][0/20]	Time 0.154 (0.154)	Data 0.143 (0.143)	Loss 0.9190 (0.9190)	Prec@1 64.062 (64.062)
Epoch: [91][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 1.1514 (0.9204)	Prec@1 52.941 (62.040)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9433 (0.9433)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [92][0/20]	Time 0.150 (0.150)	Data 0.141 (0.141)	Loss 0.9838 (0.9838)	Prec@1 62.500 (62.500)
Epoch: [92][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.8602 (0.9339)	Prec@1 61.765 (60.600)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9431 (0.9431)	Prec@1 58.667 (58.667)
 * Prec@1 58.667
65.33333587646484
Epoch: [93][0/20]	Time 0.149 (0.149)	Data 0.140 (0.140)	Loss 0.8947 (0.8947)	Prec@1 65.625 (65.625)
Epoch: [93][19/20]	Time 0.008 (0.014)	Data 0.000 (0.007)	Loss 0.9044 (0.9213)	Prec@1 60.294 (61.800)
Test: [0/1]	Time 0.118 (0.118)	Loss 0.9339 (0.9339)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
65.33333587646484
Epoch: [94][0/20]	Time 0.170 (0.170)	Data 0.158 (0.158)	Loss 1.1070 (1.1070)	Prec@1 48.438 (48.438)
Epoch: [94][19/20]	Time 0.005 (0.016)	Data 0.000 (0.008)	Loss 0.9887 (0.9424)	Prec@1 64.706 (61.600)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9286 (0.9286)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [95][0/20]	Time 0.154 (0.154)	Data 0.142 (0.142)	Loss 0.8043 (0.8043)	Prec@1 69.531 (69.531)
Epoch: [95][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.9860 (0.9463)	Prec@1 61.765 (60.560)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.9184 (0.9184)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [96][0/20]	Time 0.156 (0.156)	Data 0.148 (0.148)	Loss 0.8969 (0.8969)	Prec@1 63.281 (63.281)
Epoch: [96][19/20]	Time 0.006 (0.014)	Data 0.000 (0.008)	Loss 0.8554 (0.9402)	Prec@1 66.176 (61.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9183 (0.9183)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [97][0/20]	Time 0.156 (0.156)	Data 0.141 (0.141)	Loss 0.9223 (0.9223)	Prec@1 62.500 (62.500)
Epoch: [97][19/20]	Time 0.007 (0.015)	Data 0.000 (0.007)	Loss 1.2007 (0.9500)	Prec@1 52.941 (60.120)
Test: [0/1]	Time 0.118 (0.118)	Loss 0.9066 (0.9066)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [98][0/20]	Time 0.170 (0.170)	Data 0.157 (0.157)	Loss 0.8367 (0.8367)	Prec@1 64.062 (64.062)
Epoch: [98][19/20]	Time 0.004 (0.015)	Data 0.000 (0.008)	Loss 1.0794 (0.9294)	Prec@1 55.882 (60.960)
Test: [0/1]	Time 0.121 (0.121)	Loss 0.9181 (0.9181)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [99][0/20]	Time 0.148 (0.148)	Data 0.136 (0.136)	Loss 1.0215 (1.0215)	Prec@1 61.719 (61.719)
Epoch: [99][19/20]	Time 0.006 (0.013)	Data 0.000 (0.007)	Loss 1.1228 (0.9398)	Prec@1 60.294 (61.120)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9184 (0.9184)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [100][0/20]	Time 0.159 (0.159)	Data 0.148 (0.148)	Loss 0.9184 (0.9184)	Prec@1 64.062 (64.062)
Epoch: [100][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 1.0112 (0.9380)	Prec@1 58.824 (60.440)
Test: [0/1]	Time 0.117 (0.117)	Loss 0.9186 (0.9186)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
65.33333587646484
Epoch: [101][0/20]	Time 0.271 (0.271)	Data 0.266 (0.266)	Loss 0.8768 (0.8768)	Prec@1 63.281 (63.281)
Epoch: [101][19/20]	Time 0.003 (0.026)	Data 0.000 (0.022)	Loss 0.9260 (0.9299)	Prec@1 52.941 (61.440)
Test: [0/1]	Time 0.222 (0.222)	Loss 0.9189 (0.9189)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [102][0/20]	Time 0.341 (0.341)	Data 0.329 (0.329)	Loss 0.9735 (0.9735)	Prec@1 63.281 (63.281)
Epoch: [102][19/20]	Time 0.008 (0.026)	Data 0.000 (0.018)	Loss 1.1192 (0.9327)	Prec@1 51.471 (61.360)
Test: [0/1]	Time 0.385 (0.385)	Loss 0.9181 (0.9181)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [103][0/20]	Time 0.343 (0.343)	Data 0.335 (0.335)	Loss 0.9617 (0.9617)	Prec@1 60.938 (60.938)
Epoch: [103][19/20]	Time 0.008 (0.024)	Data 0.000 (0.017)	Loss 1.0353 (0.9270)	Prec@1 64.706 (61.960)
Test: [0/1]	Time 0.130 (0.130)	Loss 0.9201 (0.9201)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [104][0/20]	Time 0.162 (0.162)	Data 0.150 (0.150)	Loss 0.8804 (0.8804)	Prec@1 62.500 (62.500)
Epoch: [104][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 1.0656 (0.9304)	Prec@1 42.647 (60.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9215 (0.9215)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [105][0/20]	Time 0.162 (0.162)	Data 0.147 (0.147)	Loss 0.9593 (0.9593)	Prec@1 62.500 (62.500)
Epoch: [105][19/20]	Time 0.006 (0.015)	Data 0.000 (0.007)	Loss 0.8960 (0.9216)	Prec@1 60.294 (61.880)
Test: [0/1]	Time 0.143 (0.143)	Loss 0.9223 (0.9223)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [106][0/20]	Time 0.155 (0.155)	Data 0.142 (0.142)	Loss 0.8885 (0.8885)	Prec@1 64.844 (64.844)
Epoch: [106][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 0.8226 (0.9184)	Prec@1 52.941 (62.720)
Test: [0/1]	Time 0.123 (0.123)	Loss 0.9229 (0.9229)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [107][0/20]	Time 0.158 (0.158)	Data 0.148 (0.148)	Loss 0.9426 (0.9426)	Prec@1 57.812 (57.812)
Epoch: [107][19/20]	Time 0.006 (0.013)	Data 0.003 (0.008)	Loss 0.8455 (0.9311)	Prec@1 64.706 (60.720)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9226 (0.9226)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [108][0/20]	Time 0.151 (0.151)	Data 0.140 (0.140)	Loss 0.9842 (0.9842)	Prec@1 56.250 (56.250)
Epoch: [108][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 0.9333 (0.9154)	Prec@1 63.235 (61.240)
Test: [0/1]	Time 0.135 (0.135)	Loss 0.9243 (0.9243)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [109][0/20]	Time 0.167 (0.167)	Data 0.155 (0.155)	Loss 0.7977 (0.7977)	Prec@1 65.625 (65.625)
Epoch: [109][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 1.0002 (0.9431)	Prec@1 57.353 (60.520)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9267 (0.9267)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [110][0/20]	Time 0.174 (0.174)	Data 0.162 (0.162)	Loss 0.9313 (0.9313)	Prec@1 59.375 (59.375)
Epoch: [110][19/20]	Time 0.006 (0.015)	Data 0.000 (0.008)	Loss 0.7373 (0.9206)	Prec@1 72.059 (62.720)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9279 (0.9279)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [111][0/20]	Time 0.159 (0.159)	Data 0.146 (0.146)	Loss 0.8994 (0.8994)	Prec@1 64.062 (64.062)
Epoch: [111][19/20]	Time 0.008 (0.015)	Data 0.000 (0.008)	Loss 0.8028 (0.9159)	Prec@1 66.176 (62.360)
Test: [0/1]	Time 0.124 (0.124)	Loss 0.9281 (0.9281)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [112][0/20]	Time 0.158 (0.158)	Data 0.143 (0.143)	Loss 0.9662 (0.9662)	Prec@1 61.719 (61.719)
Epoch: [112][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.8831 (0.9294)	Prec@1 67.647 (62.080)
Test: [0/1]	Time 0.120 (0.120)	Loss 0.9290 (0.9290)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [113][0/20]	Time 0.177 (0.177)	Data 0.165 (0.165)	Loss 1.0497 (1.0497)	Prec@1 55.469 (55.469)
Epoch: [113][19/20]	Time 0.003 (0.014)	Data 0.000 (0.008)	Loss 1.0117 (0.9383)	Prec@1 55.882 (60.080)
Test: [0/1]	Time 0.138 (0.138)	Loss 0.9286 (0.9286)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [114][0/20]	Time 0.151 (0.151)	Data 0.139 (0.139)	Loss 0.8525 (0.8525)	Prec@1 66.406 (66.406)
Epoch: [114][19/20]	Time 0.007 (0.014)	Data 0.000 (0.007)	Loss 0.8394 (0.9324)	Prec@1 69.118 (60.840)
Test: [0/1]	Time 0.134 (0.134)	Loss 0.9271 (0.9271)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [115][0/20]	Time 0.158 (0.158)	Data 0.149 (0.149)	Loss 0.9508 (0.9508)	Prec@1 64.062 (64.062)
Epoch: [115][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.9532 (0.9240)	Prec@1 64.706 (62.560)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9270 (0.9270)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [116][0/20]	Time 0.134 (0.134)	Data 0.128 (0.128)	Loss 0.9776 (0.9776)	Prec@1 57.812 (57.812)
Epoch: [116][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 1.1237 (0.9370)	Prec@1 57.353 (60.840)
Test: [0/1]	Time 0.132 (0.132)	Loss 0.9269 (0.9269)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [117][0/20]	Time 0.167 (0.167)	Data 0.157 (0.157)	Loss 0.9936 (0.9936)	Prec@1 58.594 (58.594)
Epoch: [117][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.9875 (0.9512)	Prec@1 51.471 (60.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.9269 (0.9269)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [118][0/20]	Time 0.164 (0.164)	Data 0.152 (0.152)	Loss 0.8117 (0.8117)	Prec@1 63.281 (63.281)
Epoch: [118][19/20]	Time 0.006 (0.014)	Data 0.000 (0.008)	Loss 0.9042 (0.9289)	Prec@1 61.765 (60.960)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9265 (0.9265)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [119][0/20]	Time 0.170 (0.170)	Data 0.156 (0.156)	Loss 0.9671 (0.9671)	Prec@1 60.938 (60.938)
Epoch: [119][19/20]	Time 0.004 (0.015)	Data 0.000 (0.008)	Loss 0.7585 (0.9320)	Prec@1 69.118 (61.680)
Test: [0/1]	Time 0.129 (0.129)	Loss 0.9270 (0.9270)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [120][0/20]	Time 0.148 (0.148)	Data 0.140 (0.140)	Loss 0.9084 (0.9084)	Prec@1 60.938 (60.938)
Epoch: [120][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 0.8986 (0.9243)	Prec@1 64.706 (61.440)
Test: [0/1]	Time 0.148 (0.148)	Loss 0.9284 (0.9284)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [121][0/20]	Time 0.146 (0.146)	Data 0.140 (0.140)	Loss 0.8515 (0.8515)	Prec@1 65.625 (65.625)
Epoch: [121][19/20]	Time 0.007 (0.015)	Data 0.000 (0.008)	Loss 0.7133 (0.9063)	Prec@1 75.000 (62.880)
Test: [0/1]	Time 0.142 (0.142)	Loss 0.9284 (0.9284)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [122][0/20]	Time 0.158 (0.158)	Data 0.146 (0.146)	Loss 0.8844 (0.8844)	Prec@1 65.625 (65.625)
Epoch: [122][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.8443 (0.9483)	Prec@1 63.235 (60.080)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9281 (0.9281)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [123][0/20]	Time 0.159 (0.159)	Data 0.147 (0.147)	Loss 0.9446 (0.9446)	Prec@1 61.719 (61.719)
Epoch: [123][19/20]	Time 0.007 (0.015)	Data 0.000 (0.007)	Loss 0.7933 (0.9237)	Prec@1 72.059 (62.440)
Test: [0/1]	Time 0.134 (0.134)	Loss 0.9266 (0.9266)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [124][0/20]	Time 0.158 (0.158)	Data 0.146 (0.146)	Loss 0.9867 (0.9867)	Prec@1 58.594 (58.594)
Epoch: [124][19/20]	Time 0.006 (0.015)	Data 0.000 (0.007)	Loss 0.7690 (0.9164)	Prec@1 70.588 (60.800)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9278 (0.9278)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [125][0/20]	Time 0.143 (0.143)	Data 0.135 (0.135)	Loss 0.8903 (0.8903)	Prec@1 67.188 (67.188)
Epoch: [125][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 1.1002 (0.9283)	Prec@1 52.941 (61.560)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9292 (0.9292)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [126][0/20]	Time 0.163 (0.163)	Data 0.155 (0.155)	Loss 0.9512 (0.9512)	Prec@1 61.719 (61.719)
Epoch: [126][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.8114 (0.9247)	Prec@1 63.235 (60.760)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.9306 (0.9306)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [127][0/20]	Time 0.153 (0.153)	Data 0.140 (0.140)	Loss 1.0336 (1.0336)	Prec@1 58.594 (58.594)
Epoch: [127][19/20]	Time 0.004 (0.014)	Data 0.000 (0.007)	Loss 0.8938 (0.9231)	Prec@1 66.176 (61.960)
Test: [0/1]	Time 0.136 (0.136)	Loss 0.9309 (0.9309)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [128][0/20]	Time 0.158 (0.158)	Data 0.146 (0.146)	Loss 0.8716 (0.8716)	Prec@1 64.844 (64.844)
Epoch: [128][19/20]	Time 0.005 (0.015)	Data 0.000 (0.007)	Loss 0.9247 (0.9262)	Prec@1 60.294 (61.120)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9319 (0.9319)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [129][0/20]	Time 0.159 (0.159)	Data 0.146 (0.146)	Loss 0.8225 (0.8225)	Prec@1 61.719 (61.719)
Epoch: [129][19/20]	Time 0.006 (0.015)	Data 0.000 (0.007)	Loss 0.9027 (0.9240)	Prec@1 64.706 (60.840)
Test: [0/1]	Time 0.143 (0.143)	Loss 0.9328 (0.9328)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [130][0/20]	Time 0.339 (0.339)	Data 0.334 (0.334)	Loss 0.8362 (0.8362)	Prec@1 64.844 (64.844)
Epoch: [130][19/20]	Time 0.005 (0.026)	Data 0.000 (0.022)	Loss 0.9196 (0.9301)	Prec@1 60.294 (61.600)
Test: [0/1]	Time 0.393 (0.393)	Loss 0.9341 (0.9341)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [131][0/20]	Time 0.188 (0.188)	Data 0.179 (0.179)	Loss 0.9127 (0.9127)	Prec@1 60.938 (60.938)
Epoch: [131][19/20]	Time 0.005 (0.018)	Data 0.000 (0.011)	Loss 0.9787 (0.9241)	Prec@1 60.294 (61.480)
Test: [0/1]	Time 0.266 (0.266)	Loss 0.9308 (0.9308)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [132][0/20]	Time 0.430 (0.430)	Data 0.416 (0.416)	Loss 0.9735 (0.9735)	Prec@1 55.469 (55.469)
Epoch: [132][19/20]	Time 0.010 (0.033)	Data 0.000 (0.023)	Loss 0.9561 (0.9226)	Prec@1 58.824 (61.960)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.9306 (0.9306)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [133][0/20]	Time 0.146 (0.146)	Data 0.135 (0.135)	Loss 1.0173 (1.0173)	Prec@1 56.250 (56.250)
Epoch: [133][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 0.7989 (0.9178)	Prec@1 64.706 (61.160)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9285 (0.9285)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [134][0/20]	Time 0.131 (0.131)	Data 0.122 (0.122)	Loss 0.8501 (0.8501)	Prec@1 63.281 (63.281)
Epoch: [134][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 0.8991 (0.9307)	Prec@1 57.353 (60.520)
Test: [0/1]	Time 0.128 (0.128)	Loss 0.9279 (0.9279)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [135][0/20]	Time 0.153 (0.153)	Data 0.142 (0.142)	Loss 0.8759 (0.8759)	Prec@1 64.062 (64.062)
Epoch: [135][19/20]	Time 0.005 (0.014)	Data 0.000 (0.007)	Loss 0.9589 (0.9256)	Prec@1 58.824 (59.880)
Test: [0/1]	Time 0.124 (0.124)	Loss 0.9267 (0.9267)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [136][0/20]	Time 0.167 (0.167)	Data 0.156 (0.156)	Loss 0.8452 (0.8452)	Prec@1 67.188 (67.188)
Epoch: [136][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.8194 (0.9156)	Prec@1 66.176 (61.760)
Test: [0/1]	Time 0.109 (0.109)	Loss 0.9241 (0.9241)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [137][0/20]	Time 0.187 (0.187)	Data 0.176 (0.176)	Loss 1.0096 (1.0096)	Prec@1 58.594 (58.594)
Epoch: [137][19/20]	Time 0.003 (0.015)	Data 0.000 (0.009)	Loss 0.8711 (0.9226)	Prec@1 63.235 (60.880)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9241 (0.9241)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [138][0/20]	Time 0.176 (0.176)	Data 0.163 (0.163)	Loss 0.7001 (0.7001)	Prec@1 75.000 (75.000)
Epoch: [138][19/20]	Time 0.008 (0.016)	Data 0.000 (0.008)	Loss 0.7356 (0.9113)	Prec@1 70.588 (61.760)
Test: [0/1]	Time 0.127 (0.127)	Loss 0.9240 (0.9240)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [139][0/20]	Time 0.152 (0.152)	Data 0.138 (0.138)	Loss 0.9449 (0.9449)	Prec@1 57.031 (57.031)
Epoch: [139][19/20]	Time 0.006 (0.014)	Data 0.000 (0.007)	Loss 0.7066 (0.8956)	Prec@1 77.941 (62.280)
Test: [0/1]	Time 0.122 (0.122)	Loss 0.9239 (0.9239)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [140][0/20]	Time 0.167 (0.167)	Data 0.155 (0.155)	Loss 0.8854 (0.8854)	Prec@1 60.156 (60.156)
Epoch: [140][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 0.9401 (0.9284)	Prec@1 63.235 (61.800)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9232 (0.9232)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [141][0/20]	Time 0.163 (0.163)	Data 0.152 (0.152)	Loss 0.8833 (0.8833)	Prec@1 60.156 (60.156)
Epoch: [141][19/20]	Time 0.005 (0.015)	Data 0.000 (0.008)	Loss 1.0816 (0.9146)	Prec@1 54.412 (62.240)
Test: [0/1]	Time 0.135 (0.135)	Loss 0.9229 (0.9229)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [142][0/20]	Time 0.148 (0.148)	Data 0.141 (0.141)	Loss 0.8740 (0.8740)	Prec@1 64.062 (64.062)
Epoch: [142][19/20]	Time 0.003 (0.013)	Data 0.000 (0.008)	Loss 0.8271 (0.9150)	Prec@1 70.588 (62.120)
Test: [0/1]	Time 0.109 (0.109)	Loss 0.9224 (0.9224)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [143][0/20]	Time 0.160 (0.160)	Data 0.150 (0.150)	Loss 0.9042 (0.9042)	Prec@1 61.719 (61.719)
Epoch: [143][19/20]	Time 0.003 (0.014)	Data 0.000 (0.008)	Loss 0.8862 (0.9147)	Prec@1 60.294 (62.200)
Test: [0/1]	Time 0.125 (0.125)	Loss 0.9223 (0.9223)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [144][0/20]	Time 0.152 (0.152)	Data 0.142 (0.142)	Loss 0.9017 (0.9017)	Prec@1 63.281 (63.281)
Epoch: [144][19/20]	Time 0.004 (0.013)	Data 0.000 (0.007)	Loss 1.1108 (0.9210)	Prec@1 50.000 (61.760)
Test: [0/1]	Time 0.132 (0.132)	Loss 0.9237 (0.9237)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [145][0/20]	Time 0.167 (0.167)	Data 0.155 (0.155)	Loss 0.9418 (0.9418)	Prec@1 58.594 (58.594)
Epoch: [145][19/20]	Time 0.004 (0.014)	Data 0.000 (0.008)	Loss 0.8367 (0.9211)	Prec@1 69.118 (61.320)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9222 (0.9222)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [146][0/20]	Time 0.169 (0.169)	Data 0.157 (0.157)	Loss 0.9491 (0.9491)	Prec@1 60.156 (60.156)
Epoch: [146][19/20]	Time 0.005 (0.014)	Data 0.000 (0.008)	Loss 0.8933 (0.9367)	Prec@1 60.294 (60.160)
Test: [0/1]	Time 0.137 (0.137)	Loss 0.9214 (0.9214)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [147][0/20]	Time 0.150 (0.150)	Data 0.140 (0.140)	Loss 1.0000 (1.0000)	Prec@1 60.156 (60.156)
Epoch: [147][19/20]	Time 0.005 (0.013)	Data 0.000 (0.007)	Loss 0.9481 (0.9039)	Prec@1 61.765 (64.480)
Test: [0/1]	Time 0.133 (0.133)	Loss 0.9225 (0.9225)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
65.33333587646484
Epoch: [148][0/20]	Time 0.163 (0.163)	Data 0.152 (0.152)	Loss 0.7752 (0.7752)	Prec@1 67.188 (67.188)
Epoch: [148][19/20]	Time 0.004 (0.015)	Data 0.000 (0.008)	Loss 1.0625 (0.9285)	Prec@1 58.824 (61.240)
Test: [0/1]	Time 0.141 (0.141)	Loss 0.9238 (0.9238)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
Epoch: [149][0/20]	Time 0.169 (0.169)	Data 0.158 (0.158)	Loss 0.8833 (0.8833)	Prec@1 64.062 (64.062)
Epoch: [149][19/20]	Time 0.004 (0.015)	Data 0.000 (0.008)	Loss 0.9874 (0.9114)	Prec@1 52.941 (61.720)
Test: [0/1]	Time 0.114 (0.114)	Loss 0.9219 (0.9219)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
65.33333587646484
total time: 85.1583023071289
train loss:  [1.4868706077575684, 1.1533015995025635, 1.0832995586395264, 1.0365161615371703, 1.013966429901123, 1.0384231004714966, 1.0299103246688843, 1.0263512554168701, 1.0254653329849244, 1.0323674839019776, 0.994416202545166, 1.0108817624092101, 1.0099724498748779, 1.0149494590759278, 0.9857771299362182, 0.9785608150482178, 0.9712964076995849, 0.9720243462562561, 0.9812334008216858, 0.9742829906463623, 0.9751239479064941, 0.9747344047546387, 0.953236464881897, 0.9915647131919861, 0.9765836616516114, 0.9602914245605468, 0.9696855087280274, 0.9541092405319214, 0.9662942123413086, 0.9711202423095703, 0.9569909684181214, 0.9490183795928955, 0.9793202390670777, 0.9878826747894287, 0.9387595059394837, 0.9418122698783874, 0.9424004909515381, 0.9562483835220337, 0.9631699892997742, 0.9561662757873535, 0.9463489264488221, 0.936301902961731, 0.9642469301223755, 0.9449796333312989, 0.9632423463821411, 0.9642407821655273, 0.956018952178955, 0.9452409925460815, 0.9451361044883728, 0.9555713127136231, 0.9401009922027588, 0.9418331005096435, 0.9326716302871704, 0.9556417422294616, 0.9532441741943359, 0.9391829459190368, 0.9502410767555237, 0.9520657554626465, 0.9593166251182557, 0.9488502166748047, 0.9487338810920716, 0.9224339500427247, 0.9698521750450134, 0.9440975601196289, 0.9383449172973632, 0.9594746668815612, 0.9178782048225402, 0.9444294484138489, 0.9360359979629517, 0.9483279220581055, 0.9316571563720704, 0.9395279171943665, 0.9359259006500245, 0.9318055214881897, 0.9310571351051331, 0.9211896509170532, 0.9232386449813843, 0.9511457777976989, 0.9500270232200623, 0.9208943434715271, 0.9294083802223205, 0.9290496717453003, 0.9292152406692505, 0.943374486541748, 0.9341573172569275, 0.9587054769515991, 0.9291618903160095, 0.9362386612892151, 0.9381613306999207, 0.94043057346344, 0.9377911340713501, 0.9203753541946411, 0.9339353567123413, 0.9213406498908997, 0.942422119140625, 0.9462692076683045, 0.9402434560775756, 0.9500037794113159, 0.9293766986846924, 0.9397623125076294, 0.938040744972229, 0.9299155682563782, 0.9327220872879028, 0.9269873481750488, 0.9304491064071655, 0.9216374127388001, 0.9183509320259094, 0.9310968212127686, 0.9154183751106262, 0.9430682781219483, 0.9205915735244751, 0.9158652209281921, 0.9294364194869995, 0.9383063577651978, 0.932399659729004, 0.923962372303009, 0.9370495113372803, 0.9512215752601624, 0.9289225878715515, 0.9320269011497497, 0.9242633215904236, 0.9062973107337952, 0.9482909865379333, 0.9237433416366577, 0.9164390037536622, 0.9282621734619141, 0.9246612893104553, 0.9231372847557068, 0.9262375770568848, 0.9239961869239807, 0.9300737201690674, 0.9241289066314697, 0.9225645287513733, 0.9178134418487549, 0.9306945181846619, 0.9255564977645874, 0.915645051574707, 0.9225650478363037, 0.9113027271270752, 0.8956075898170471, 0.9283679845809937, 0.9146214298248291, 0.9149656920433045, 0.914682127571106, 0.9209594062805175, 0.9211076663017272, 0.9366903855323792, 0.9038888465881347, 0.9284912494659424, 0.9114275113105774]
train acc:  [47.95999993286133, 56.27999993286133, 58.03999995727539, 59.71999993286133, 59.00000000610351, 58.52, 58.71999993286133, 57.95999998168946, 59.59999993286133, 59.19999993286133, 59.36000001220703, 59.51999996948242, 59.32000000610351, 56.8, 61.71999993896485, 60.11999995727539, 60.15999995727539, 60.600000024414065, 60.80000000610352, 61.04000001220703, 60.91999994506836, 61.19999996948242, 62.07999996948242, 60.319999981689456, 61.599999963378906, 60.91999998779297, 60.47999991455078, 62.03999995727539, 61.07999996948242, 61.15999995727539, 61.59999998168945, 61.75999993286133, 60.119999993896485, 60.23999992675781, 60.799999932861326, 62.27999992675781, 61.39999995727539, 60.639999981689456, 61.91999996948242, 60.679999981689456, 61.43999995727539, 62.16000001220703, 61.51999995727539, 61.68000002441406, 59.91999994506836, 60.679999969482424, 61.80000000610352, 61.03999999389649, 60.679999914550784, 61.15999996948242, 62.279999951171874, 61.160000006103516, 61.799999938964845, 60.55999998168945, 61.36000002441406, 61.439999987792966, 61.31999995727539, 59.679999926757816, 60.84, 60.599999969482425, 60.96000000610351, 61.95999998779297, 59.479999993896485, 60.55999998168945, 61.79999995727539, 60.28000000610351, 62.920000024414065, 61.67999993896484, 61.399999981689454, 60.119999932861326, 62.55999999389648, 61.83999994506836, 61.87999991455078, 61.47999995117188, 61.319999914550785, 61.91999998168945, 62.23999994506836, 59.799999938964845, 60.68000000610351, 60.31999992675781, 63.15999996337891, 61.19999998779297, 61.76000000610352, 60.55999994506836, 61.399999926757815, 60.39999998779297, 60.79999996948242, 61.47999997558594, 60.999999963378905, 61.159999993896484, 59.999999975585936, 62.04000002441406, 60.59999999389648, 61.79999998168945, 61.59999991455078, 60.55999999389648, 60.99999992675781, 60.12000002441406, 60.95999994506836, 61.11999998168945, 60.43999996948242, 61.44000002441406, 61.36000001220703, 61.95999991455078, 59.99999993896484, 61.87999998168945, 62.72000002441406, 60.71999991455078, 61.240000006103514, 60.51999995727539, 62.719999975585935, 62.359999926757816, 62.079999938964846, 60.07999994506836, 60.839999951171876, 62.55999991455078, 60.83999995727539, 60.00000001220703, 60.95999999389648, 61.67999995117187, 61.43999991455078, 62.88, 60.08000000610352, 62.439999975585934, 60.79999996337891, 61.560000024414066, 60.76000000610352, 61.95999992675781, 61.11999998168945, 60.83999991455078, 61.59999998168945, 61.47999998168945, 61.959999969482425, 61.15999991455078, 60.51999995727539, 59.87999996948242, 61.759999926757814, 60.880000006103515, 61.75999996337891, 62.280000024414065, 61.80000000610352, 62.23999993286133, 62.11999996337891, 62.19999998168945, 61.76, 61.31999995117187, 60.15999998168945, 64.47999999389648, 61.23999996948242, 61.72000002441406]
test loss:  [1.1140503883361816, 0.915166437625885, 0.9155634641647339, 0.8806899189949036, 0.87540203332901, 0.8794295191764832, 0.8845550417900085, 0.8573406934738159, 0.8905695676803589, 0.9259461760520935, 0.9158899784088135, 0.9076303839683533, 0.8935044407844543, 0.899592399597168, 0.9061192870140076, 0.9059805274009705, 0.9129164814949036, 0.9208202362060547, 0.9306368827819824, 0.9117826223373413, 0.9121663570404053, 0.9430420994758606, 0.9423131346702576, 0.9526411890983582, 0.9297989010810852, 0.9564818143844604, 0.9578964114189148, 0.9402310252189636, 0.9845052361488342, 0.970541775226593, 0.943742573261261, 0.9353819489479065, 0.9232292771339417, 0.9256498217582703, 0.923000693321228, 0.9212963581085205, 0.9191043376922607, 0.9209045171737671, 0.9236323833465576, 0.9357409477233887, 0.9307062029838562, 0.9051673412322998, 0.9176543354988098, 0.9191045165061951, 0.9180400371551514, 0.9184606671333313, 0.9376522898674011, 0.9261303544044495, 0.933120608329773, 0.9500259160995483, 0.9362218379974365, 0.9345046877861023, 0.9302923679351807, 0.9421077370643616, 0.9387078881263733, 0.9519373774528503, 0.9559716582298279, 0.929776132106781, 0.9488997459411621, 0.9269225001335144, 0.9592862725257874, 0.9587213397026062, 0.962493896484375, 0.9456489086151123, 0.9338788986206055, 0.9376711249351501, 0.9545577764511108, 0.9424867033958435, 0.9298933148384094, 0.9466492533683777, 0.9447423219680786, 0.9287900924682617, 0.9296960234642029, 0.9054881930351257, 0.9215699434280396, 0.9327182769775391, 0.9315245747566223, 0.9113573431968689, 0.932079553604126, 0.9412215948104858, 0.9364156126976013, 0.9428340792655945, 0.9294916987419128, 0.9242517948150635, 0.9363684058189392, 0.9272140264511108, 0.936755359172821, 0.9375810027122498, 0.9296339750289917, 0.9190651178359985, 0.9520692229270935, 0.9432755708694458, 0.9430565237998962, 0.9339016675949097, 0.9285550117492676, 0.9184077978134155, 0.9182788133621216, 0.9065953493118286, 0.9181217551231384, 0.9183803200721741, 0.9186115264892578, 0.9188581109046936, 0.9181163311004639, 0.9200851321220398, 0.9215065240859985, 0.9223054051399231, 0.9228549003601074, 0.9226139187812805, 0.9243338108062744, 0.9267421364784241, 0.9278662204742432, 0.9281485080718994, 0.9290120601654053, 0.9285545945167542, 0.9270965456962585, 0.9269764423370361, 0.9269131422042847, 0.926932156085968, 0.9265433549880981, 0.926985502243042, 0.9284231662750244, 0.9283936619758606, 0.9280680418014526, 0.9266132712364197, 0.9277990460395813, 0.9292370676994324, 0.9305785894393921, 0.9309479594230652, 0.9318708181381226, 0.9328351020812988, 0.9341114163398743, 0.9307878613471985, 0.9305952787399292, 0.9285276532173157, 0.9278574585914612, 0.9266597628593445, 0.9241220951080322, 0.9240642189979553, 0.9239714741706848, 0.9238994121551514, 0.9231738448143005, 0.9228646755218506, 0.9224491119384766, 0.922302782535553, 0.9236992597579956, 0.9222029447555542, 0.9213736057281494, 0.9225233197212219, 0.923788845539093, 0.9219484329223633]
test acc:  [58.66666793823242, 65.33333587646484, 64.0, 65.33333587646484, 61.333335876464844, 62.66666793823242, 64.0, 62.66666793823242, 64.0, 61.333335876464844, 64.0, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 61.333335876464844, 62.66666793823242, 60.0, 61.333335876464844, 60.0, 58.66666793823242, 60.0, 57.333335876464844, 57.333335876464844, 60.0, 57.333335876464844, 58.66666793823242, 62.66666793823242, 61.333335876464844, 60.0, 61.333335876464844, 62.66666793823242, 62.66666793823242, 64.0, 62.66666793823242, 62.66666793823242, 60.0, 64.0, 61.333335876464844, 62.66666793823242, 61.333335876464844, 62.66666793823242, 64.0, 62.66666793823242, 62.66666793823242, 62.66666793823242, 61.333335876464844, 61.333335876464844, 58.66666793823242, 62.66666793823242, 60.0, 62.66666793823242, 61.333335876464844, 60.0, 60.0, 60.0, 61.333335876464844, 57.333335876464844, 60.0, 60.0, 60.0, 60.0, 61.333335876464844, 58.66666793823242, 61.333335876464844, 57.333335876464844, 58.66666793823242, 60.0, 62.66666793823242, 58.66666793823242, 61.333335876464844, 60.0, 60.0, 61.333335876464844, 62.66666793823242, 61.333335876464844, 60.0, 61.333335876464844, 58.66666793823242, 61.333335876464844, 61.333335876464844, 61.333335876464844, 64.0, 64.0, 60.0, 60.0, 60.0, 61.333335876464844, 58.66666793823242, 62.66666793823242, 61.333335876464844, 61.333335876464844, 58.66666793823242, 60.0, 61.333335876464844, 62.66666793823242, 61.333335876464844, 61.333335876464844, 64.0, 64.0, 64.0, 62.66666793823242, 62.66666793823242, 62.66666793823242, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 62.66666793823242, 61.333335876464844, 62.66666793823242, 62.66666793823242, 62.66666793823242, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 61.333335876464844, 62.66666793823242, 62.66666793823242, 62.66666793823242, 61.333335876464844, 61.333335876464844, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 62.66666793823242, 61.333335876464844, 61.333335876464844]
best prec: 65.33333587646484
params_end 70
W: (40, 29541)
P: (40, 29541)
[ 2 13  9 10 15]
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/model_best.pth.tar'
from  16051
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 16051)
./convnet_5way_5shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (16051, 150)
Epoch: [0][0/1]	Time 1.022 (1.022)	Data 0.084 (0.084)	Loss 2.2300 (2.2300)	Prec@1 32.000 (32.000)
Test: [0/1]	Time 0.184 (0.184)	Loss 2.5289 (2.5289)	Prec@1 25.333 (25.333)
 * Prec@1 25.333
25.33333396911621
Epoch: [1][0/1]	Time 0.173 (0.173)	Data 0.165 (0.165)	Loss 2.0842 (2.0842)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.212 (0.212)	Loss 2.2901 (2.2901)	Prec@1 28.000 (28.000)
 * Prec@1 28.000
28.0
Epoch: [2][0/1]	Time 0.327 (0.327)	Data 0.317 (0.317)	Loss 2.1757 (2.1757)	Prec@1 28.000 (28.000)
Test: [0/1]	Time 0.225 (0.225)	Loss 2.0751 (2.0751)	Prec@1 32.000 (32.000)
 * Prec@1 32.000
32.0
Epoch: [3][0/1]	Time 0.223 (0.223)	Data 0.214 (0.214)	Loss 1.4745 (1.4745)	Prec@1 36.000 (36.000)
Test: [0/1]	Time 0.254 (0.254)	Loss 1.9249 (1.9249)	Prec@1 36.000 (36.000)
 * Prec@1 36.000
36.0
Epoch: [4][0/1]	Time 0.338 (0.338)	Data 0.327 (0.327)	Loss 1.1994 (1.1994)	Prec@1 56.000 (56.000)
Test: [0/1]	Time 0.281 (0.281)	Loss 1.8372 (1.8372)	Prec@1 37.333 (37.333)
 * Prec@1 37.333
37.333335876464844
Epoch: [5][0/1]	Time 0.179 (0.179)	Data 0.172 (0.172)	Loss 1.0820 (1.0820)	Prec@1 48.000 (48.000)
Test: [0/1]	Time 0.170 (0.170)	Loss 1.8223 (1.8223)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
45.333335876464844
Epoch: [6][0/1]	Time 0.184 (0.184)	Data 0.176 (0.176)	Loss 1.2612 (1.2612)	Prec@1 44.000 (44.000)
Test: [0/1]	Time 0.166 (0.166)	Loss 1.8520 (1.8520)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
45.333335876464844
Epoch: [7][0/1]	Time 0.174 (0.174)	Data 0.166 (0.166)	Loss 0.7972 (0.7972)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.163 (0.163)	Loss 1.8875 (1.8875)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [8][0/1]	Time 0.176 (0.176)	Data 0.165 (0.165)	Loss 0.8174 (0.8174)	Prec@1 64.000 (64.000)
Test: [0/1]	Time 0.158 (0.158)	Loss 1.9297 (1.9297)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [9][0/1]	Time 0.154 (0.154)	Data 0.147 (0.147)	Loss 0.7481 (0.7481)	Prec@1 64.000 (64.000)
Test: [0/1]	Time 0.190 (0.190)	Loss 2.0092 (2.0092)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [10][0/1]	Time 0.161 (0.161)	Data 0.151 (0.151)	Loss 0.6646 (0.6646)	Prec@1 72.000 (72.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.0795 (2.0795)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [11][0/1]	Time 0.167 (0.167)	Data 0.160 (0.160)	Loss 0.5787 (0.5787)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 2.1404 (2.1404)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [12][0/1]	Time 0.174 (0.174)	Data 0.165 (0.165)	Loss 0.8299 (0.8299)	Prec@1 72.000 (72.000)
Test: [0/1]	Time 0.182 (0.182)	Loss 2.1840 (2.1840)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
45.333335876464844
Epoch: [13][0/1]	Time 0.183 (0.183)	Data 0.173 (0.173)	Loss 0.4900 (0.4900)	Prec@1 72.000 (72.000)
Test: [0/1]	Time 0.167 (0.167)	Loss 2.2202 (2.2202)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [14][0/1]	Time 0.162 (0.162)	Data 0.152 (0.152)	Loss 0.4824 (0.4824)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.164 (0.164)	Loss 2.2452 (2.2452)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
45.333335876464844
Epoch: [15][0/1]	Time 0.160 (0.160)	Data 0.152 (0.152)	Loss 0.4089 (0.4089)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.172 (0.172)	Loss 2.2591 (2.2591)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [16][0/1]	Time 0.187 (0.187)	Data 0.178 (0.178)	Loss 0.5855 (0.5855)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.189 (0.189)	Loss 2.2566 (2.2566)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
45.333335876464844
Epoch: [17][0/1]	Time 0.164 (0.164)	Data 0.158 (0.158)	Loss 0.5394 (0.5394)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.2474 (2.2474)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
46.66666793823242
Epoch: [18][0/1]	Time 0.148 (0.148)	Data 0.141 (0.141)	Loss 0.4336 (0.4336)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.189 (0.189)	Loss 2.2265 (2.2265)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
46.66666793823242
Epoch: [19][0/1]	Time 0.176 (0.176)	Data 0.168 (0.168)	Loss 0.3317 (0.3317)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.196 (0.196)	Loss 2.2010 (2.2010)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
46.66666793823242
Epoch: [20][0/1]	Time 0.180 (0.180)	Data 0.169 (0.169)	Loss 0.5979 (0.5979)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.1708 (2.1708)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
48.0
Epoch: [21][0/1]	Time 0.157 (0.157)	Data 0.147 (0.147)	Loss 0.3573 (0.3573)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.182 (0.182)	Loss 2.1379 (2.1379)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [22][0/1]	Time 0.155 (0.155)	Data 0.148 (0.148)	Loss 0.3958 (0.3958)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.179 (0.179)	Loss 2.1038 (2.1038)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [23][0/1]	Time 0.163 (0.163)	Data 0.155 (0.155)	Loss 0.6376 (0.6376)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.163 (0.163)	Loss 2.0712 (2.0712)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [24][0/1]	Time 0.154 (0.154)	Data 0.147 (0.147)	Loss 0.4411 (0.4411)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.185 (0.185)	Loss 2.0357 (2.0357)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
52.0
Epoch: [25][0/1]	Time 0.182 (0.182)	Data 0.171 (0.171)	Loss 0.3170 (0.3170)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.158 (0.158)	Loss 2.0026 (2.0026)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
52.0
Epoch: [26][0/1]	Time 0.180 (0.180)	Data 0.169 (0.169)	Loss 0.5281 (0.5281)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.167 (0.167)	Loss 1.9702 (1.9702)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
52.0
Epoch: [27][0/1]	Time 0.172 (0.172)	Data 0.161 (0.161)	Loss 0.3400 (0.3400)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.157 (0.157)	Loss 1.9435 (1.9435)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
52.0
Epoch: [28][0/1]	Time 0.175 (0.175)	Data 0.166 (0.166)	Loss 0.3573 (0.3573)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 1.9269 (1.9269)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [29][0/1]	Time 0.176 (0.176)	Data 0.168 (0.168)	Loss 0.4924 (0.4924)	Prec@1 72.000 (72.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 1.9124 (1.9124)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [30][0/1]	Time 0.159 (0.159)	Data 0.152 (0.152)	Loss 0.4365 (0.4365)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.153 (0.153)	Loss 1.8989 (1.8989)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [31][0/1]	Time 0.171 (0.171)	Data 0.159 (0.159)	Loss 0.3852 (0.3852)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 1.8864 (1.8864)	Prec@1 56.000 (56.000)
 * Prec@1 56.000
56.0
Epoch: [32][0/1]	Time 0.163 (0.163)	Data 0.153 (0.153)	Loss 0.3987 (0.3987)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.186 (0.186)	Loss 1.8772 (1.8772)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
56.0
Epoch: [33][0/1]	Time 0.170 (0.170)	Data 0.160 (0.160)	Loss 0.3822 (0.3822)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 1.8679 (1.8679)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
56.0
Epoch: [34][0/1]	Time 0.173 (0.173)	Data 0.164 (0.164)	Loss 0.4154 (0.4154)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 1.8637 (1.8637)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
56.0
Epoch: [35][0/1]	Time 0.165 (0.165)	Data 0.157 (0.157)	Loss 0.5703 (0.5703)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 1.8636 (1.8636)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
56.0
Epoch: [36][0/1]	Time 0.170 (0.170)	Data 0.160 (0.160)	Loss 0.3704 (0.3704)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.173 (0.173)	Loss 1.8658 (1.8658)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
56.0
Epoch: [37][0/1]	Time 0.164 (0.164)	Data 0.155 (0.155)	Loss 0.4275 (0.4275)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.169 (0.169)	Loss 1.8707 (1.8707)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
56.0
Epoch: [38][0/1]	Time 0.168 (0.168)	Data 0.159 (0.159)	Loss 0.5271 (0.5271)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.286 (0.286)	Loss 1.8815 (1.8815)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
56.0
Epoch: [39][0/1]	Time 0.278 (0.278)	Data 0.273 (0.273)	Loss 0.3625 (0.3625)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.195 (0.195)	Loss 1.8954 (1.8954)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
56.0
Epoch: [40][0/1]	Time 0.220 (0.220)	Data 0.213 (0.213)	Loss 0.4675 (0.4675)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.262 (0.262)	Loss 1.9125 (1.9125)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
56.0
Epoch: [41][0/1]	Time 0.272 (0.272)	Data 0.263 (0.263)	Loss 0.4523 (0.4523)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.377 (0.377)	Loss 1.9284 (1.9284)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
56.0
Epoch: [42][0/1]	Time 0.170 (0.170)	Data 0.159 (0.159)	Loss 0.4498 (0.4498)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.170 (0.170)	Loss 1.9465 (1.9465)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
56.0
Epoch: [43][0/1]	Time 0.180 (0.180)	Data 0.172 (0.172)	Loss 0.3116 (0.3116)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.179 (0.179)	Loss 1.9684 (1.9684)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [44][0/1]	Time 0.161 (0.161)	Data 0.154 (0.154)	Loss 0.3543 (0.3543)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.181 (0.181)	Loss 1.9894 (1.9894)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [45][0/1]	Time 0.153 (0.153)	Data 0.146 (0.146)	Loss 0.3234 (0.3234)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.158 (0.158)	Loss 2.0078 (2.0078)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [46][0/1]	Time 0.171 (0.171)	Data 0.163 (0.163)	Loss 0.4352 (0.4352)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.185 (0.185)	Loss 2.0219 (2.0219)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [47][0/1]	Time 0.162 (0.162)	Data 0.155 (0.155)	Loss 0.3900 (0.3900)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.159 (0.159)	Loss 2.0395 (2.0395)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [48][0/1]	Time 0.177 (0.177)	Data 0.170 (0.170)	Loss 0.4807 (0.4807)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.0556 (2.0556)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [49][0/1]	Time 0.183 (0.183)	Data 0.176 (0.176)	Loss 0.3536 (0.3536)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.163 (0.163)	Loss 2.0679 (2.0679)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [50][0/1]	Time 0.173 (0.173)	Data 0.165 (0.165)	Loss 0.3749 (0.3749)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 2.0794 (2.0794)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [51][0/1]	Time 0.170 (0.170)	Data 0.162 (0.162)	Loss 0.3795 (0.3795)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.172 (0.172)	Loss 2.0885 (2.0885)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [52][0/1]	Time 0.164 (0.164)	Data 0.154 (0.154)	Loss 0.2782 (0.2782)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.0970 (2.0970)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [53][0/1]	Time 0.154 (0.154)	Data 0.147 (0.147)	Loss 0.4043 (0.4043)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.177 (0.177)	Loss 2.1068 (2.1068)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [54][0/1]	Time 0.167 (0.167)	Data 0.159 (0.159)	Loss 0.2495 (0.2495)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.172 (0.172)	Loss 2.1148 (2.1148)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [55][0/1]	Time 0.174 (0.174)	Data 0.163 (0.163)	Loss 0.2823 (0.2823)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.172 (0.172)	Loss 2.1206 (2.1206)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [56][0/1]	Time 0.173 (0.173)	Data 0.165 (0.165)	Loss 0.4337 (0.4337)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.192 (0.192)	Loss 2.1255 (2.1255)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [57][0/1]	Time 0.171 (0.171)	Data 0.164 (0.164)	Loss 0.5247 (0.5247)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.1289 (2.1289)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [58][0/1]	Time 0.166 (0.166)	Data 0.158 (0.158)	Loss 0.5322 (0.5322)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.166 (0.166)	Loss 2.1287 (2.1287)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [59][0/1]	Time 0.161 (0.161)	Data 0.151 (0.151)	Loss 0.3164 (0.3164)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.168 (0.168)	Loss 2.1267 (2.1267)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [60][0/1]	Time 0.165 (0.165)	Data 0.158 (0.158)	Loss 0.3719 (0.3719)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.200 (0.200)	Loss 2.1267 (2.1267)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [61][0/1]	Time 0.186 (0.186)	Data 0.177 (0.177)	Loss 0.2415 (0.2415)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.172 (0.172)	Loss 2.1256 (2.1256)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [62][0/1]	Time 0.166 (0.166)	Data 0.157 (0.157)	Loss 0.3417 (0.3417)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 2.1281 (2.1281)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [63][0/1]	Time 0.169 (0.169)	Data 0.161 (0.161)	Loss 0.4644 (0.4644)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.1316 (2.1316)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [64][0/1]	Time 0.173 (0.173)	Data 0.163 (0.163)	Loss 0.4904 (0.4904)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.159 (0.159)	Loss 2.1354 (2.1354)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [65][0/1]	Time 0.166 (0.166)	Data 0.159 (0.159)	Loss 0.5708 (0.5708)	Prec@1 76.000 (76.000)
Test: [0/1]	Time 0.177 (0.177)	Loss 2.1406 (2.1406)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [66][0/1]	Time 0.176 (0.176)	Data 0.168 (0.168)	Loss 0.2210 (0.2210)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.179 (0.179)	Loss 2.1408 (2.1408)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [67][0/1]	Time 0.174 (0.174)	Data 0.167 (0.167)	Loss 0.5056 (0.5056)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.162 (0.162)	Loss 2.1358 (2.1358)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [68][0/1]	Time 0.170 (0.170)	Data 0.163 (0.163)	Loss 0.3197 (0.3197)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.1305 (2.1305)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [69][0/1]	Time 0.153 (0.153)	Data 0.147 (0.147)	Loss 0.3504 (0.3504)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.181 (0.181)	Loss 2.1259 (2.1259)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [70][0/1]	Time 0.163 (0.163)	Data 0.154 (0.154)	Loss 0.4080 (0.4080)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.168 (0.168)	Loss 2.1215 (2.1215)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [71][0/1]	Time 0.168 (0.168)	Data 0.161 (0.161)	Loss 0.3611 (0.3611)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.153 (0.153)	Loss 2.1142 (2.1142)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [72][0/1]	Time 0.202 (0.202)	Data 0.190 (0.190)	Loss 0.2896 (0.2896)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.170 (0.170)	Loss 2.1105 (2.1105)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [73][0/1]	Time 0.185 (0.185)	Data 0.177 (0.177)	Loss 0.3048 (0.3048)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.170 (0.170)	Loss 2.1058 (2.1058)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [74][0/1]	Time 0.173 (0.173)	Data 0.165 (0.165)	Loss 0.3239 (0.3239)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.176 (0.176)	Loss 2.1011 (2.1011)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [75][0/1]	Time 0.185 (0.185)	Data 0.177 (0.177)	Loss 0.2769 (0.2769)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.188 (0.188)	Loss 2.0996 (2.0996)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [76][0/1]	Time 0.210 (0.210)	Data 0.204 (0.204)	Loss 0.2738 (0.2738)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.267 (0.267)	Loss 2.1003 (2.1003)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [77][0/1]	Time 0.244 (0.244)	Data 0.232 (0.232)	Loss 0.3187 (0.3187)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.276 (0.276)	Loss 2.1052 (2.1052)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [78][0/1]	Time 0.253 (0.253)	Data 0.243 (0.243)	Loss 0.2922 (0.2922)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.394 (0.394)	Loss 2.1067 (2.1067)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [79][0/1]	Time 0.232 (0.232)	Data 0.224 (0.224)	Loss 0.2646 (0.2646)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.174 (0.174)	Loss 2.1036 (2.1036)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
56.0
Epoch: [80][0/1]	Time 0.163 (0.163)	Data 0.155 (0.155)	Loss 0.2603 (0.2603)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.187 (0.187)	Loss 2.0997 (2.0997)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [81][0/1]	Time 0.179 (0.179)	Data 0.168 (0.168)	Loss 0.3487 (0.3487)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.176 (0.176)	Loss 2.0955 (2.0955)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [82][0/1]	Time 0.166 (0.166)	Data 0.152 (0.152)	Loss 0.2513 (0.2513)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.162 (0.162)	Loss 2.0936 (2.0936)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [83][0/1]	Time 0.175 (0.175)	Data 0.166 (0.166)	Loss 0.2968 (0.2968)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.0899 (2.0899)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [84][0/1]	Time 0.175 (0.175)	Data 0.164 (0.164)	Loss 0.3430 (0.3430)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.174 (0.174)	Loss 2.0874 (2.0874)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [85][0/1]	Time 0.174 (0.174)	Data 0.162 (0.162)	Loss 0.2866 (0.2866)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.155 (0.155)	Loss 2.0858 (2.0858)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [86][0/1]	Time 0.170 (0.170)	Data 0.162 (0.162)	Loss 0.2696 (0.2696)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 2.0881 (2.0881)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [87][0/1]	Time 0.171 (0.171)	Data 0.164 (0.164)	Loss 0.4283 (0.4283)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.0899 (2.0899)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
56.0
Epoch: [88][0/1]	Time 0.179 (0.179)	Data 0.169 (0.169)	Loss 0.3180 (0.3180)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.186 (0.186)	Loss 2.0907 (2.0907)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [89][0/1]	Time 0.163 (0.163)	Data 0.155 (0.155)	Loss 0.3490 (0.3490)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.180 (0.180)	Loss 2.0898 (2.0898)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [90][0/1]	Time 0.184 (0.184)	Data 0.175 (0.175)	Loss 0.3099 (0.3099)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.161 (0.161)	Loss 2.0912 (2.0912)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [91][0/1]	Time 0.163 (0.163)	Data 0.154 (0.154)	Loss 0.2725 (0.2725)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.186 (0.186)	Loss 2.0922 (2.0922)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [92][0/1]	Time 0.176 (0.176)	Data 0.168 (0.168)	Loss 0.3887 (0.3887)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 2.0891 (2.0891)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [93][0/1]	Time 0.172 (0.172)	Data 0.164 (0.164)	Loss 0.3520 (0.3520)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.169 (0.169)	Loss 2.0901 (2.0901)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [94][0/1]	Time 0.174 (0.174)	Data 0.165 (0.165)	Loss 0.3892 (0.3892)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.172 (0.172)	Loss 2.0912 (2.0912)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [95][0/1]	Time 0.168 (0.168)	Data 0.160 (0.160)	Loss 0.3148 (0.3148)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.169 (0.169)	Loss 2.0966 (2.0966)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [96][0/1]	Time 0.183 (0.183)	Data 0.175 (0.175)	Loss 0.3001 (0.3001)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 2.1015 (2.1015)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [97][0/1]	Time 0.176 (0.176)	Data 0.169 (0.169)	Loss 0.3029 (0.3029)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.169 (0.169)	Loss 2.1097 (2.1097)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [98][0/1]	Time 0.163 (0.163)	Data 0.153 (0.153)	Loss 0.2455 (0.2455)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.1223 (2.1223)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
56.0
Epoch: [99][0/1]	Time 0.171 (0.171)	Data 0.162 (0.162)	Loss 0.4535 (0.4535)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.176 (0.176)	Loss 2.1285 (2.1285)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [100][0/1]	Time 0.164 (0.164)	Data 0.157 (0.157)	Loss 0.4172 (0.4172)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.1292 (2.1292)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [101][0/1]	Time 0.168 (0.168)	Data 0.160 (0.160)	Loss 0.3759 (0.3759)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.169 (0.169)	Loss 2.1296 (2.1296)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [102][0/1]	Time 0.165 (0.165)	Data 0.158 (0.158)	Loss 0.3111 (0.3111)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 2.1301 (2.1301)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [103][0/1]	Time 0.174 (0.174)	Data 0.166 (0.166)	Loss 0.2404 (0.2404)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.170 (0.170)	Loss 2.1304 (2.1304)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [104][0/1]	Time 0.183 (0.183)	Data 0.176 (0.176)	Loss 0.2079 (0.2079)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.170 (0.170)	Loss 2.1307 (2.1307)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [105][0/1]	Time 0.158 (0.158)	Data 0.151 (0.151)	Loss 0.3189 (0.3189)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.174 (0.174)	Loss 2.1310 (2.1310)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [106][0/1]	Time 0.177 (0.177)	Data 0.166 (0.166)	Loss 0.2499 (0.2499)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.159 (0.159)	Loss 2.1313 (2.1313)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [107][0/1]	Time 0.172 (0.172)	Data 0.165 (0.165)	Loss 0.2863 (0.2863)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.168 (0.168)	Loss 2.1318 (2.1318)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [108][0/1]	Time 0.174 (0.174)	Data 0.166 (0.166)	Loss 0.2617 (0.2617)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.167 (0.167)	Loss 2.1322 (2.1322)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [109][0/1]	Time 0.175 (0.175)	Data 0.168 (0.168)	Loss 0.2845 (0.2845)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.161 (0.161)	Loss 2.1322 (2.1322)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [110][0/1]	Time 0.174 (0.174)	Data 0.166 (0.166)	Loss 0.4132 (0.4132)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.193 (0.193)	Loss 2.1320 (2.1320)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [111][0/1]	Time 0.192 (0.192)	Data 0.182 (0.182)	Loss 0.3341 (0.3341)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.174 (0.174)	Loss 2.1320 (2.1320)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [112][0/1]	Time 0.151 (0.151)	Data 0.144 (0.144)	Loss 0.4001 (0.4001)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.180 (0.180)	Loss 2.1315 (2.1315)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [113][0/1]	Time 0.207 (0.207)	Data 0.202 (0.202)	Loss 0.4094 (0.4094)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.315 (0.315)	Loss 2.1312 (2.1312)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [114][0/1]	Time 0.158 (0.158)	Data 0.150 (0.150)	Loss 0.3463 (0.3463)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.275 (0.275)	Loss 2.1307 (2.1307)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [115][0/1]	Time 0.376 (0.376)	Data 0.364 (0.364)	Loss 0.2265 (0.2265)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.422 (0.422)	Loss 2.1301 (2.1301)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [116][0/1]	Time 0.167 (0.167)	Data 0.155 (0.155)	Loss 0.2655 (0.2655)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.179 (0.179)	Loss 2.1291 (2.1291)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [117][0/1]	Time 0.167 (0.167)	Data 0.158 (0.158)	Loss 0.1661 (0.1661)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.174 (0.174)	Loss 2.1282 (2.1282)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [118][0/1]	Time 0.168 (0.168)	Data 0.160 (0.160)	Loss 0.1521 (0.1521)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.177 (0.177)	Loss 2.1273 (2.1273)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [119][0/1]	Time 0.186 (0.186)	Data 0.178 (0.178)	Loss 0.3438 (0.3438)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.153 (0.153)	Loss 2.1264 (2.1264)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [120][0/1]	Time 0.165 (0.165)	Data 0.157 (0.157)	Loss 0.2109 (0.2109)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.167 (0.167)	Loss 2.1255 (2.1255)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [121][0/1]	Time 0.176 (0.176)	Data 0.169 (0.169)	Loss 0.2486 (0.2486)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.169 (0.169)	Loss 2.1243 (2.1243)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [122][0/1]	Time 0.166 (0.166)	Data 0.160 (0.160)	Loss 0.4227 (0.4227)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.175 (0.175)	Loss 2.1227 (2.1227)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [123][0/1]	Time 0.161 (0.161)	Data 0.154 (0.154)	Loss 0.2753 (0.2753)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.173 (0.173)	Loss 2.1212 (2.1212)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [124][0/1]	Time 0.173 (0.173)	Data 0.164 (0.164)	Loss 0.2184 (0.2184)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.160 (0.160)	Loss 2.1198 (2.1198)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [125][0/1]	Time 0.187 (0.187)	Data 0.179 (0.179)	Loss 0.2569 (0.2569)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.182 (0.182)	Loss 2.1185 (2.1185)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [126][0/1]	Time 0.170 (0.170)	Data 0.163 (0.163)	Loss 0.2688 (0.2688)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 2.1173 (2.1173)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [127][0/1]	Time 0.170 (0.170)	Data 0.162 (0.162)	Loss 0.2536 (0.2536)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.156 (0.156)	Loss 2.1162 (2.1162)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [128][0/1]	Time 0.172 (0.172)	Data 0.165 (0.165)	Loss 0.3611 (0.3611)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.186 (0.186)	Loss 2.1150 (2.1150)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [129][0/1]	Time 0.184 (0.184)	Data 0.176 (0.176)	Loss 0.2122 (0.2122)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.177 (0.177)	Loss 2.1139 (2.1139)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [130][0/1]	Time 0.183 (0.183)	Data 0.173 (0.173)	Loss 0.2517 (0.2517)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.166 (0.166)	Loss 2.1131 (2.1131)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [131][0/1]	Time 0.169 (0.169)	Data 0.162 (0.162)	Loss 0.3662 (0.3662)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.163 (0.163)	Loss 2.1125 (2.1125)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [132][0/1]	Time 0.176 (0.176)	Data 0.164 (0.164)	Loss 0.1996 (0.1996)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.1119 (2.1119)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [133][0/1]	Time 0.172 (0.172)	Data 0.161 (0.161)	Loss 0.3272 (0.3272)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.176 (0.176)	Loss 2.1114 (2.1114)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [134][0/1]	Time 0.153 (0.153)	Data 0.147 (0.147)	Loss 0.3391 (0.3391)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.171 (0.171)	Loss 2.1106 (2.1106)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [135][0/1]	Time 0.175 (0.175)	Data 0.164 (0.164)	Loss 0.3220 (0.3220)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.166 (0.166)	Loss 2.1097 (2.1097)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [136][0/1]	Time 0.163 (0.163)	Data 0.151 (0.151)	Loss 0.1700 (0.1700)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.185 (0.185)	Loss 2.1091 (2.1091)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [137][0/1]	Time 0.171 (0.171)	Data 0.164 (0.164)	Loss 0.3613 (0.3613)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.163 (0.163)	Loss 2.1085 (2.1085)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [138][0/1]	Time 0.168 (0.168)	Data 0.161 (0.161)	Loss 0.2424 (0.2424)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.163 (0.163)	Loss 2.1079 (2.1079)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [139][0/1]	Time 0.158 (0.158)	Data 0.152 (0.152)	Loss 0.4589 (0.4589)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 2.1073 (2.1073)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [140][0/1]	Time 0.179 (0.179)	Data 0.169 (0.169)	Loss 0.3066 (0.3066)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 2.1071 (2.1071)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [141][0/1]	Time 0.175 (0.175)	Data 0.166 (0.166)	Loss 0.3848 (0.3848)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 2.1071 (2.1071)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [142][0/1]	Time 0.184 (0.184)	Data 0.177 (0.177)	Loss 0.2175 (0.2175)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.180 (0.180)	Loss 2.1069 (2.1069)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [143][0/1]	Time 0.172 (0.172)	Data 0.165 (0.165)	Loss 0.2871 (0.2871)	Prec@1 88.000 (88.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 2.1069 (2.1069)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [144][0/1]	Time 0.164 (0.164)	Data 0.157 (0.157)	Loss 0.3433 (0.3433)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.176 (0.176)	Loss 2.1068 (2.1068)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [145][0/1]	Time 0.163 (0.163)	Data 0.155 (0.155)	Loss 0.2070 (0.2070)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 2.1070 (2.1070)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [146][0/1]	Time 0.171 (0.171)	Data 0.163 (0.163)	Loss 0.2686 (0.2686)	Prec@1 96.000 (96.000)
Test: [0/1]	Time 0.166 (0.166)	Loss 2.1072 (2.1072)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [147][0/1]	Time 0.161 (0.161)	Data 0.153 (0.153)	Loss 0.3558 (0.3558)	Prec@1 84.000 (84.000)
Test: [0/1]	Time 0.178 (0.178)	Loss 2.1075 (2.1075)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [148][0/1]	Time 0.188 (0.188)	Data 0.181 (0.181)	Loss 0.1449 (0.1449)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.153 (0.153)	Loss 2.1079 (2.1079)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
Epoch: [149][0/1]	Time 0.169 (0.169)	Data 0.159 (0.159)	Loss 0.2752 (0.2752)	Prec@1 92.000 (92.000)
Test: [0/1]	Time 0.274 (0.274)	Loss 2.1079 (2.1079)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
56.0
total time: 70.95017409324646
train loss:  [2.230008602142334, 2.0842320919036865, 2.175748109817505, 1.474461555480957, 1.199405550956726, 1.0820029973983765, 1.2612375020980835, 0.7971758842468262, 0.8173984289169312, 0.7481483221054077, 0.664630651473999, 0.5787442922592163, 0.8298617005348206, 0.4900118112564087, 0.4824327230453491, 0.4088835120201111, 0.5854766368865967, 0.5394244194030762, 0.43355968594551086, 0.3316677212715149, 0.5979189276695251, 0.35725098848342896, 0.39582768082618713, 0.6375623345375061, 0.4410698711872101, 0.31704241037368774, 0.5280798673629761, 0.3400422930717468, 0.3573038578033447, 0.4924026131629944, 0.4365122616291046, 0.38517528772354126, 0.3986678421497345, 0.38222798705101013, 0.4153553247451782, 0.5703188180923462, 0.3704293966293335, 0.4275479018688202, 0.5270839929580688, 0.36247071623802185, 0.467537522315979, 0.4523165822029114, 0.4498256742954254, 0.3116418123245239, 0.3543015718460083, 0.32343432307243347, 0.43516838550567627, 0.3900146782398224, 0.4806828796863556, 0.35357964038848877, 0.37492749094963074, 0.3794652819633484, 0.27821844816207886, 0.4043198525905609, 0.2494536191225052, 0.28227123618125916, 0.4336816370487213, 0.524686336517334, 0.53215092420578, 0.3163728415966034, 0.3718629479408264, 0.2415439784526825, 0.34170764684677124, 0.46441352367401123, 0.49043747782707214, 0.5707880854606628, 0.2209825962781906, 0.5055686235427856, 0.3197401463985443, 0.35040053725242615, 0.4080001711845398, 0.3610554039478302, 0.28958573937416077, 0.3047659695148468, 0.3239363133907318, 0.27687302231788635, 0.27382057905197144, 0.3186788260936737, 0.2921735644340515, 0.26463425159454346, 0.26026633381843567, 0.34865784645080566, 0.25126340985298157, 0.2967863976955414, 0.34301215410232544, 0.2866126000881195, 0.2696056663990021, 0.4282800555229187, 0.3180036246776581, 0.34896454215049744, 0.3098708689212799, 0.2725052535533905, 0.38865867257118225, 0.35203391313552856, 0.38921085000038147, 0.3147854506969452, 0.3001459538936615, 0.3029399812221527, 0.24553878605365753, 0.45348668098449707, 0.4172259569168091, 0.3759423792362213, 0.3111099600791931, 0.2404247671365738, 0.20791277289390564, 0.318856418132782, 0.2498997449874878, 0.28629249334335327, 0.26172369718551636, 0.28448739647865295, 0.4131779074668884, 0.3341276943683624, 0.4000908136367798, 0.4094286859035492, 0.3463040590286255, 0.22650942206382751, 0.26549264788627625, 0.16606777906417847, 0.15211814641952515, 0.3438200354576111, 0.2108634114265442, 0.24862103164196014, 0.4226762354373932, 0.27532094717025757, 0.21840086579322815, 0.25685521960258484, 0.26884695887565613, 0.25358134508132935, 0.36108216643333435, 0.2121875137090683, 0.2516627013683319, 0.36617276072502136, 0.1996137648820877, 0.32722634077072144, 0.339143306016922, 0.3220321238040924, 0.1699729859828949, 0.36125853657722473, 0.24236874282360077, 0.45889198780059814, 0.3065687119960785, 0.3847826421260834, 0.21752811968326569, 0.2870754599571228, 0.34325236082077026, 0.2069755494594574, 0.2686074376106262, 0.35577574372291565, 0.144911527633667, 0.2752131223678589]
train acc:  [32.0, 20.0, 28.0, 36.0, 56.0, 48.0, 44.0, 60.0, 64.0, 64.0, 72.0, 76.0, 72.0, 72.0, 80.0, 88.0, 76.0, 84.0, 80.0, 92.0, 76.0, 88.0, 88.0, 76.0, 88.0, 84.0, 80.0, 88.0, 84.0, 72.0, 84.0, 92.0, 92.0, 88.0, 84.0, 80.0, 92.0, 84.0, 76.0, 88.0, 84.0, 80.0, 84.0, 88.0, 92.0, 92.0, 76.0, 92.0, 80.0, 92.0, 88.0, 84.0, 92.0, 84.0, 92.0, 88.0, 88.0, 88.0, 84.0, 88.0, 76.0, 96.0, 88.0, 76.0, 80.0, 76.0, 100.0, 80.0, 88.0, 96.0, 84.0, 84.0, 88.0, 92.0, 84.0, 88.0, 88.0, 96.0, 92.0, 92.0, 92.0, 88.0, 92.0, 88.0, 92.0, 92.0, 88.0, 84.0, 92.0, 88.0, 88.0, 88.0, 88.0, 88.0, 84.0, 84.0, 92.0, 96.0, 96.0, 84.0, 92.0, 88.0, 88.0, 92.0, 96.0, 92.0, 96.0, 92.0, 92.0, 92.0, 80.0, 84.0, 84.0, 84.0, 92.0, 96.0, 92.0, 100.0, 100.0, 88.0, 96.0, 96.0, 84.0, 92.0, 96.0, 88.0, 96.0, 92.0, 80.0, 100.0, 96.0, 84.0, 100.0, 92.0, 84.0, 92.0, 100.0, 88.0, 92.0, 88.0, 84.0, 88.0, 96.0, 88.0, 84.0, 96.0, 96.0, 84.0, 100.0, 92.0]
test loss:  [2.528883934020996, 2.290055751800537, 2.0751101970672607, 1.9248803853988647, 1.8371633291244507, 1.8222527503967285, 1.8519986867904663, 1.887495756149292, 1.9297332763671875, 2.0092082023620605, 2.0795035362243652, 2.140350818634033, 2.183950185775757, 2.2201943397521973, 2.2451727390289307, 2.259051561355591, 2.2565791606903076, 2.247396945953369, 2.226546287536621, 2.201014518737793, 2.170848846435547, 2.1379082202911377, 2.103775978088379, 2.0712265968322754, 2.035743474960327, 2.0025644302368164, 1.9701546430587769, 1.9435304403305054, 1.9268784523010254, 1.912446141242981, 1.8988529443740845, 1.8863834142684937, 1.8771880865097046, 1.8679115772247314, 1.8637341260910034, 1.863609790802002, 1.8657543659210205, 1.8706797361373901, 1.8814780712127686, 1.8954366445541382, 1.9124985933303833, 1.9283778667449951, 1.9465234279632568, 1.9683843851089478, 1.989437460899353, 2.0077896118164062, 2.0218849182128906, 2.0395455360412598, 2.055554151535034, 2.067871332168579, 2.0793557167053223, 2.088500499725342, 2.0970096588134766, 2.1067683696746826, 2.1147620677948, 2.120603561401367, 2.125509023666382, 2.128873348236084, 2.128722667694092, 2.1266913414001465, 2.1267292499542236, 2.1255640983581543, 2.128052234649658, 2.131582736968994, 2.1354339122772217, 2.140622138977051, 2.140810489654541, 2.1357619762420654, 2.1304514408111572, 2.125872850418091, 2.1215405464172363, 2.114198923110962, 2.11047625541687, 2.1058197021484375, 2.1011159420013428, 2.099620819091797, 2.100282907485962, 2.105228900909424, 2.1066837310791016, 2.1036217212677, 2.0997250080108643, 2.0954651832580566, 2.0936338901519775, 2.0898520946502686, 2.087416887283325, 2.0858163833618164, 2.08805513381958, 2.089888334274292, 2.090730905532837, 2.089752674102783, 2.0912301540374756, 2.092223882675171, 2.0891237258911133, 2.0901036262512207, 2.091219186782837, 2.096623182296753, 2.1015326976776123, 2.109659433364868, 2.122272253036499, 2.128460168838501, 2.1292405128479004, 2.1296000480651855, 2.130079507827759, 2.1303863525390625, 2.1306748390197754, 2.1309964656829834, 2.1313085556030273, 2.131772994995117, 2.132150173187256, 2.1321773529052734, 2.132046937942505, 2.1320269107818604, 2.1315255165100098, 2.131211996078491, 2.13069224357605, 2.1300666332244873, 2.1291165351867676, 2.128178596496582, 2.1272687911987305, 2.1264400482177734, 2.1254847049713135, 2.124290704727173, 2.1227054595947266, 2.1211540699005127, 2.1197919845581055, 2.118515968322754, 2.1172871589660645, 2.1161928176879883, 2.115001916885376, 2.1138932704925537, 2.1131319999694824, 2.112537384033203, 2.1119437217712402, 2.1114084720611572, 2.110572576522827, 2.1096718311309814, 2.109060764312744, 2.1085143089294434, 2.107927083969116, 2.107259511947632, 2.1070926189422607, 2.107079267501831, 2.1068930625915527, 2.106853485107422, 2.1068246364593506, 2.1070289611816406, 2.107243299484253, 2.1074838638305664, 2.1078567504882812, 2.1079342365264893]
test acc:  [25.33333396911621, 28.0, 32.0, 36.0, 37.333335876464844, 45.333335876464844, 45.333335876464844, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 41.333335876464844, 42.66666793823242, 41.333335876464844, 42.66666793823242, 42.66666793823242, 46.66666793823242, 45.333335876464844, 46.66666793823242, 48.0, 49.333335876464844, 49.333335876464844, 50.66666793823242, 52.0, 52.0, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 54.66666793823242, 56.0, 54.66666793823242, 54.66666793823242, 54.66666793823242, 54.66666793823242, 54.66666793823242, 53.333335876464844, 54.66666793823242, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 49.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 50.66666793823242, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 48.0, 49.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 49.333335876464844, 49.333335876464844, 48.0, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242]
best prec: 56.0
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 1.004 (1.004)	Data 0.078 (0.078)	Loss 2.4870 (2.4870)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 2.1375 (2.1375)	Prec@1 26.667 (26.667)
 * Prec@1 26.667
26.666667938232422
Epoch: [1][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 0.7644 (0.7644)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.7698 (1.7698)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
40.0
Epoch: [2][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 2.7904 (2.7904)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5663 (1.5663)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
46.66666793823242
Epoch: [3][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.1750 (0.1750)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.142 (0.142)	Loss 1.5074 (1.5074)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [4][0/1]	Time 0.134 (0.134)	Data 0.127 (0.127)	Loss 0.2860 (0.2860)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 1.5705 (1.5705)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
53.333335876464844
Epoch: [5][0/1]	Time 0.124 (0.124)	Data 0.119 (0.119)	Loss 0.3112 (0.3112)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.4977 (1.4977)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [6][0/1]	Time 0.111 (0.111)	Data 0.106 (0.106)	Loss 0.2468 (0.2468)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4569 (1.4569)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [7][0/1]	Time 0.139 (0.139)	Data 0.130 (0.130)	Loss 0.0805 (0.0805)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4932 (1.4932)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [8][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0881 (0.0881)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5424 (1.5424)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [9][0/1]	Time 0.134 (0.134)	Data 0.126 (0.126)	Loss 0.0646 (0.0646)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.118 (0.118)	Loss 1.5621 (1.5621)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [10][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.0410 (0.0410)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.5647 (1.5647)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [11][0/1]	Time 0.114 (0.114)	Data 0.104 (0.104)	Loss 0.0284 (0.0284)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5866 (1.5866)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [12][0/1]	Time 0.130 (0.130)	Data 0.121 (0.121)	Loss 0.1056 (0.1056)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.5540 (1.5540)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [13][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.1993 (0.1993)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.5519 (1.5519)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [14][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.1450 (0.1450)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.7148 (1.7148)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [15][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0350 (0.0350)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.7331 (1.7331)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [16][0/1]	Time 0.117 (0.117)	Data 0.107 (0.107)	Loss 0.1209 (0.1209)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.8181 (1.8181)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [17][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0123 (0.0123)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.8183 (1.8183)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [18][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0143 (0.0143)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 1.8264 (1.8264)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [19][0/1]	Time 0.133 (0.133)	Data 0.127 (0.127)	Loss 0.0601 (0.0601)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7918 (1.7918)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [20][0/1]	Time 0.122 (0.122)	Data 0.117 (0.117)	Loss 0.0154 (0.0154)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.7896 (1.7896)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [21][0/1]	Time 0.130 (0.130)	Data 0.120 (0.120)	Loss 0.0064 (0.0064)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.7893 (1.7893)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [22][0/1]	Time 0.116 (0.116)	Data 0.106 (0.106)	Loss 0.0317 (0.0317)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.7731 (1.7731)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [23][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 0.0108 (0.0108)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.7752 (1.7752)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [24][0/1]	Time 0.136 (0.136)	Data 0.130 (0.130)	Loss 0.2206 (0.2206)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.8039 (1.8039)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [25][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.3651 (0.3651)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 1.7453 (1.7453)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [26][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0532 (0.0532)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.7340 (1.7340)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [27][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0182 (0.0182)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 1.7292 (1.7292)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [28][0/1]	Time 0.123 (0.123)	Data 0.114 (0.114)	Loss 0.0418 (0.0418)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.7636 (1.7636)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [29][0/1]	Time 0.117 (0.117)	Data 0.108 (0.108)	Loss 0.2877 (0.2877)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.8465 (1.8465)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [30][0/1]	Time 0.132 (0.132)	Data 0.123 (0.123)	Loss 0.0153 (0.0153)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.8483 (1.8483)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [31][0/1]	Time 0.133 (0.133)	Data 0.124 (0.124)	Loss 0.0738 (0.0738)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.8221 (1.8221)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [32][0/1]	Time 0.120 (0.120)	Data 0.110 (0.110)	Loss 0.1547 (0.1547)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.8281 (1.8281)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [33][0/1]	Time 0.135 (0.135)	Data 0.128 (0.128)	Loss 0.0267 (0.0267)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.8369 (1.8369)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [34][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0473 (0.0473)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8555 (1.8555)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [35][0/1]	Time 0.119 (0.119)	Data 0.110 (0.110)	Loss 0.5279 (0.5279)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.8408 (1.8408)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [36][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.0383 (0.0383)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.8312 (1.8312)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [37][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0327 (0.0327)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8595 (1.8595)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [38][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0988 (0.0988)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8097 (1.8097)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [39][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0693 (0.0693)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.8063 (1.8063)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [40][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0859 (0.0859)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.8257 (1.8257)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [41][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.0216 (0.0216)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 1.8193 (1.8193)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [42][0/1]	Time 0.142 (0.142)	Data 0.133 (0.133)	Loss 0.1931 (0.1931)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.8302 (1.8302)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [43][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0841 (0.0841)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.8647 (1.8647)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [44][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0128 (0.0128)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.8760 (1.8760)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [45][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0201 (0.0201)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8753 (1.8753)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [46][0/1]	Time 0.135 (0.135)	Data 0.126 (0.126)	Loss 0.0504 (0.0504)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.8815 (1.8815)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [47][0/1]	Time 0.111 (0.111)	Data 0.104 (0.104)	Loss 0.1089 (0.1089)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9220 (1.9220)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [48][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0373 (0.0373)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9200 (1.9200)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [49][0/1]	Time 0.123 (0.123)	Data 0.114 (0.114)	Loss 0.0787 (0.0787)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.9854 (1.9854)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [50][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0505 (0.0505)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.0011 (2.0011)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [51][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0118 (0.0118)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 2.0040 (2.0040)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [52][0/1]	Time 0.117 (0.117)	Data 0.107 (0.107)	Loss 0.0174 (0.0174)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.0138 (2.0138)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [53][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.0513 (0.0513)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 2.0636 (2.0636)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [54][0/1]	Time 0.126 (0.126)	Data 0.117 (0.117)	Loss 0.0277 (0.0277)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 2.0647 (2.0647)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [55][0/1]	Time 0.121 (0.121)	Data 0.111 (0.111)	Loss 0.0220 (0.0220)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.0589 (2.0589)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [56][0/1]	Time 0.112 (0.112)	Data 0.107 (0.107)	Loss 0.0136 (0.0136)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2.0548 (2.0548)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [57][0/1]	Time 0.134 (0.134)	Data 0.128 (0.128)	Loss 0.0055 (0.0055)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.0578 (2.0578)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [58][0/1]	Time 0.130 (0.130)	Data 0.121 (0.121)	Loss 0.0271 (0.0271)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.0709 (2.0709)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [59][0/1]	Time 0.135 (0.135)	Data 0.129 (0.129)	Loss 0.0094 (0.0094)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 2.0708 (2.0708)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [60][0/1]	Time 0.127 (0.127)	Data 0.118 (0.118)	Loss 0.0103 (0.0103)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.0658 (2.0658)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [61][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0059 (0.0059)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.0682 (2.0682)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [62][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.0070 (0.0070)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 2.0722 (2.0722)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [63][0/1]	Time 0.118 (0.118)	Data 0.108 (0.108)	Loss 0.0326 (0.0326)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.0182 (2.0182)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [64][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.0350 (0.0350)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9644 (1.9644)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [65][0/1]	Time 0.142 (0.142)	Data 0.132 (0.132)	Loss 0.0336 (0.0336)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.9639 (1.9639)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [66][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.1301 (0.1301)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.9722 (1.9722)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [67][0/1]	Time 0.123 (0.123)	Data 0.114 (0.114)	Loss 0.0060 (0.0060)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 1.9715 (1.9715)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [68][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.1063 (0.1063)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.9682 (1.9682)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [69][0/1]	Time 0.128 (0.128)	Data 0.119 (0.119)	Loss 0.0292 (0.0292)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.0042 (2.0042)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
54.66666793823242
Epoch: [70][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0057 (0.0057)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 2.0086 (2.0086)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
54.66666793823242
Epoch: [71][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0098 (0.0098)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.0153 (2.0153)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
54.66666793823242
Epoch: [72][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0129 (0.0129)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.0162 (2.0162)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [73][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.0038 (0.0038)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.0182 (2.0182)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [74][0/1]	Time 0.115 (0.115)	Data 0.106 (0.106)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 2.0318 (2.0318)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [75][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.3342 (0.3342)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9225 (1.9225)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [76][0/1]	Time 0.117 (0.117)	Data 0.108 (0.108)	Loss 0.0227 (0.0227)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9207 (1.9207)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [77][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0755 (0.0755)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9446 (1.9446)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [78][0/1]	Time 0.121 (0.121)	Data 0.112 (0.112)	Loss 0.0612 (0.0612)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9173 (1.9173)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [79][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 0.0595 (0.0595)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.8739 (1.8739)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [80][0/1]	Time 0.111 (0.111)	Data 0.106 (0.106)	Loss 0.0406 (0.0406)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 1.8655 (1.8655)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [81][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.0267 (0.0267)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.8751 (1.8751)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [82][0/1]	Time 0.120 (0.120)	Data 0.115 (0.115)	Loss 0.0075 (0.0075)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8732 (1.8732)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [83][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0088 (0.0088)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8768 (1.8768)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [84][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0071 (0.0071)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8764 (1.8764)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [85][0/1]	Time 0.128 (0.128)	Data 0.120 (0.120)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.8822 (1.8822)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [86][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0086 (0.0086)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.8896 (1.8896)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [87][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0219 (0.0219)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9109 (1.9109)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [88][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0100 (0.0100)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9167 (1.9167)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [89][0/1]	Time 0.129 (0.129)	Data 0.124 (0.124)	Loss 0.0404 (0.0404)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.9237 (1.9237)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [90][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0124 (0.0124)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.9339 (1.9339)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [91][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0686 (0.0686)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.8821 (1.8821)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [92][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0089 (0.0089)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.8836 (1.8836)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [93][0/1]	Time 0.135 (0.135)	Data 0.129 (0.129)	Loss 0.0074 (0.0074)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.8873 (1.8873)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [94][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0068 (0.0068)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8863 (1.8863)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [95][0/1]	Time 0.135 (0.135)	Data 0.129 (0.129)	Loss 0.0083 (0.0083)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.173 (0.173)	Loss 1.8950 (1.8950)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [96][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0159 (0.0159)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.142 (0.142)	Loss 1.9085 (1.9085)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [97][0/1]	Time 0.121 (0.121)	Data 0.116 (0.116)	Loss 0.1101 (0.1101)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.9508 (1.9508)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [98][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 0.0389 (0.0389)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9407 (1.9407)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [99][0/1]	Time 0.115 (0.115)	Data 0.110 (0.110)	Loss 0.0348 (0.0348)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.9438 (1.9438)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [100][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0036 (0.0036)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.9437 (1.9437)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [101][0/1]	Time 0.131 (0.131)	Data 0.126 (0.126)	Loss 0.0127 (0.0127)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.9429 (1.9429)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [102][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0890 (0.0890)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9469 (1.9469)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [103][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0338 (0.0338)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9483 (1.9483)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [104][0/1]	Time 0.118 (0.118)	Data 0.110 (0.110)	Loss 0.0086 (0.0086)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.9487 (1.9487)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [105][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.0234 (0.0234)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9488 (1.9488)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [106][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.2547 (0.2547)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9418 (1.9418)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [107][0/1]	Time 0.132 (0.132)	Data 0.124 (0.124)	Loss 0.0232 (0.0232)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.9451 (1.9451)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [108][0/1]	Time 0.117 (0.117)	Data 0.107 (0.107)	Loss 0.0234 (0.0234)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9464 (1.9464)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [109][0/1]	Time 0.126 (0.126)	Data 0.118 (0.118)	Loss 0.0127 (0.0127)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.9471 (1.9471)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [110][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 0.0044 (0.0044)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 1.9472 (1.9472)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [111][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0105 (0.0105)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9488 (1.9488)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [112][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0305 (0.0305)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9484 (1.9484)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [113][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0038 (0.0038)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.9486 (1.9486)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [114][0/1]	Time 0.123 (0.123)	Data 0.118 (0.118)	Loss 0.0082 (0.0082)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9494 (1.9494)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [115][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0046 (0.0046)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9493 (1.9493)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [116][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.0287 (0.0287)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9491 (1.9491)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [117][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 0.0357 (0.0357)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9480 (1.9480)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [118][0/1]	Time 0.127 (0.127)	Data 0.119 (0.119)	Loss 0.0075 (0.0075)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.9481 (1.9481)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [119][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.0104 (0.0104)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.9481 (1.9481)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [120][0/1]	Time 0.124 (0.124)	Data 0.119 (0.119)	Loss 0.0130 (0.0130)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9483 (1.9483)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [121][0/1]	Time 0.116 (0.116)	Data 0.107 (0.107)	Loss 0.0696 (0.0696)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9450 (1.9450)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [122][0/1]	Time 0.134 (0.134)	Data 0.128 (0.128)	Loss 0.0404 (0.0404)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.9475 (1.9475)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [123][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0444 (0.0444)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.9486 (1.9486)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [124][0/1]	Time 0.159 (0.159)	Data 0.150 (0.150)	Loss 0.0071 (0.0071)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.158 (0.158)	Loss 1.9484 (1.9484)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [125][0/1]	Time 0.119 (0.119)	Data 0.111 (0.111)	Loss 0.0042 (0.0042)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.9487 (1.9487)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [126][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0065 (0.0065)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.9493 (1.9493)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [127][0/1]	Time 0.123 (0.123)	Data 0.118 (0.118)	Loss 0.0053 (0.0053)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9494 (1.9494)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [128][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0062 (0.0062)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.9497 (1.9497)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [129][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0075 (0.0075)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 1.9498 (1.9498)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [130][0/1]	Time 0.130 (0.130)	Data 0.121 (0.121)	Loss 0.0132 (0.0132)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.9511 (1.9511)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [131][0/1]	Time 0.116 (0.116)	Data 0.111 (0.111)	Loss 0.0107 (0.0107)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.9511 (1.9511)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [132][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss 0.0288 (0.0288)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9533 (1.9533)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [133][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.0128 (0.0128)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9531 (1.9531)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [134][0/1]	Time 0.134 (0.134)	Data 0.128 (0.128)	Loss 0.0071 (0.0071)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.9525 (1.9525)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [135][0/1]	Time 0.126 (0.126)	Data 0.117 (0.117)	Loss 0.0225 (0.0225)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.9530 (1.9530)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [136][0/1]	Time 0.113 (0.113)	Data 0.108 (0.108)	Loss 0.0361 (0.0361)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.9556 (1.9556)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [137][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0667 (0.0667)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9514 (1.9514)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [138][0/1]	Time 0.118 (0.118)	Data 0.108 (0.108)	Loss 0.0204 (0.0204)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9533 (1.9533)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [139][0/1]	Time 0.134 (0.134)	Data 0.125 (0.125)	Loss 0.2842 (0.2842)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.9595 (1.9595)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [140][0/1]	Time 0.118 (0.118)	Data 0.110 (0.110)	Loss 0.0072 (0.0072)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.9593 (1.9593)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [141][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0009 (0.0009)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9593 (1.9593)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [142][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0040 (0.0040)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 1.9593 (1.9593)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [143][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0264 (0.0264)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 1.9582 (1.9582)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [144][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.0134 (0.0134)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9579 (1.9579)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [145][0/1]	Time 0.110 (0.110)	Data 0.104 (0.104)	Loss 0.0738 (0.0738)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.9532 (1.9532)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [146][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0114 (0.0114)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 1.9538 (1.9538)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [147][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0066 (0.0066)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.9546 (1.9546)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [148][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.0027 (0.0027)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9546 (1.9546)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [149][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 0.0884 (0.0884)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9539 (1.9539)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
total time: 53.9340455532074
train loss:  [2.4869697093963623, 0.764398992061615, 2.790433645248413, 0.1749817579984665, 0.2859737277030945, 0.3112102448940277, 0.24684318900108337, 0.08047740161418915, 0.08814239501953125, 0.06462478637695312, 0.04100070148706436, 0.02837216854095459, 0.10555772483348846, 0.19927915930747986, 0.14500322937965393, 0.034967757761478424, 0.12090416252613068, 0.012308454141020775, 0.014311695471405983, 0.060115862637758255, 0.01538858376443386, 0.006395530886948109, 0.03168203681707382, 0.010825872421264648, 0.22058983147144318, 0.3650943636894226, 0.05319356918334961, 0.018216991797089577, 0.04180047661066055, 0.28766193985939026, 0.015288352966308594, 0.07378105819225311, 0.15473265945911407, 0.026714850217103958, 0.04731140285730362, 0.5279488563537598, 0.03833331912755966, 0.032746028155088425, 0.09875287860631943, 0.0693337470293045, 0.08585095405578613, 0.021642232313752174, 0.19311609864234924, 0.0840996727347374, 0.012750434689223766, 0.02011556550860405, 0.050434112548828125, 0.10892269760370255, 0.03730068355798721, 0.07867169380187988, 0.0504944808781147, 0.01181859988719225, 0.017357254400849342, 0.05130653455853462, 0.027712058275938034, 0.02197246626019478, 0.013641595840454102, 0.005522346589714289, 0.0271119587123394, 0.009435844607651234, 0.010318231768906116, 0.0058934688568115234, 0.007015419192612171, 0.032573796808719635, 0.03496222570538521, 0.033577919006347656, 0.13005618751049042, 0.006010008044540882, 0.10633949935436249, 0.0291626937687397, 0.005727291107177734, 0.009798574261367321, 0.012905120849609375, 0.0038085461128503084, 0.017742538824677467, 0.33420494198799133, 0.02268047258257866, 0.07550577819347382, 0.061171818524599075, 0.05951886251568794, 0.04056067392230034, 0.026741409674286842, 0.007466554641723633, 0.008791160769760609, 0.007080888841301203, 0.01773858070373535, 0.008601903915405273, 0.021939849480986595, 0.010010575875639915, 0.040430307388305664, 0.012371206656098366, 0.06855201721191406, 0.008932304568588734, 0.007410907652229071, 0.006801414303481579, 0.008342075161635876, 0.015877246856689453, 0.11012516170740128, 0.03888244554400444, 0.03478055074810982, 0.0035959244705736637, 0.012741947546601295, 0.08898322284221649, 0.03376879543066025, 0.008646202273666859, 0.023380089551210403, 0.2547334134578705, 0.023160934448242188, 0.023376893252134323, 0.012735891155898571, 0.004355430603027344, 0.010487079620361328, 0.030503178015351295, 0.003810882568359375, 0.008185004815459251, 0.004631089977920055, 0.028706789016723633, 0.035718392580747604, 0.0074554444290697575, 0.010371064767241478, 0.013008070178329945, 0.0696166530251503, 0.04039764404296875, 0.044443272054195404, 0.007090854458510876, 0.004221534822136164, 0.006498909089714289, 0.005332136061042547, 0.0062427520751953125, 0.007505607791244984, 0.013152885250747204, 0.010669708251953125, 0.028820419684052467, 0.012843990698456764, 0.007134771440178156, 0.022545481100678444, 0.03611164167523384, 0.06671271473169327, 0.020356129854917526, 0.2842089533805847, 0.007197094149887562, 0.0009435653919354081, 0.003996086306869984, 0.026372481137514114, 0.013405418023467064, 0.07380056381225586, 0.01135950069874525, 0.006585121154785156, 0.002715968992561102, 0.08840970695018768]
train acc:  [40.0, 60.0, 40.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
test loss:  [2.1375277042388916, 1.7697898149490356, 1.5663026571273804, 1.507391095161438, 1.5705121755599976, 1.4976621866226196, 1.4568867683410645, 1.4932115077972412, 1.5423716306686401, 1.5621435642242432, 1.5647121667861938, 1.5866342782974243, 1.5540060997009277, 1.5519410371780396, 1.714762568473816, 1.7330763339996338, 1.8180729150772095, 1.818339228630066, 1.8263903856277466, 1.791809320449829, 1.7896026372909546, 1.7892686128616333, 1.773118257522583, 1.7752093076705933, 1.8039450645446777, 1.7452986240386963, 1.734012246131897, 1.729223608970642, 1.763636827468872, 1.8464646339416504, 1.8483068943023682, 1.8220934867858887, 1.828141689300537, 1.836944341659546, 1.8555409908294678, 1.8408396244049072, 1.8311991691589355, 1.859487533569336, 1.8096684217453003, 1.806306004524231, 1.8257259130477905, 1.8192806243896484, 1.8301769495010376, 1.8647290468215942, 1.8760087490081787, 1.875251293182373, 1.8815189599990845, 1.9220134019851685, 1.920023798942566, 1.9854034185409546, 2.0011260509490967, 2.0040245056152344, 2.013789176940918, 2.0636374950408936, 2.064668655395508, 2.0589230060577393, 2.0547826290130615, 2.0578463077545166, 2.070930242538452, 2.070847272872925, 2.0658316612243652, 2.068223237991333, 2.0721755027770996, 2.0182487964630127, 1.964389443397522, 1.9639028310775757, 1.9721794128417969, 1.97148597240448, 1.9681732654571533, 2.004167318344116, 2.008573293685913, 2.0152528285980225, 2.016242504119873, 2.018214225769043, 2.031818151473999, 1.9224605560302734, 1.9206539392471313, 1.9446393251419067, 1.917271375656128, 1.8739404678344727, 1.8654597997665405, 1.8750911951065063, 1.8731805086135864, 1.8767930269241333, 1.8763551712036133, 1.8821560144424438, 1.889589786529541, 1.9108647108078003, 1.9167293310165405, 1.9236723184585571, 1.9338873624801636, 1.88214910030365, 1.8836030960083008, 1.8873040676116943, 1.8863035440444946, 1.8949562311172485, 1.9084692001342773, 1.9508137702941895, 1.9406554698944092, 1.9437707662582397, 1.943682074546814, 1.9428834915161133, 1.946935772895813, 1.9483015537261963, 1.9486806392669678, 1.948797345161438, 1.9417541027069092, 1.9450856447219849, 1.9463814496994019, 1.9471228122711182, 1.9472472667694092, 1.9487906694412231, 1.9483821392059326, 1.9486348628997803, 1.9493721723556519, 1.9493097066879272, 1.9490692615509033, 1.9479637145996094, 1.9480676651000977, 1.948119878768921, 1.9482574462890625, 1.944980263710022, 1.9475181102752686, 1.9486092329025269, 1.9484175443649292, 1.9487494230270386, 1.9493025541305542, 1.949442982673645, 1.9496537446975708, 1.9498380422592163, 1.9510886669158936, 1.9511494636535645, 1.9532718658447266, 1.9531329870224, 1.9524908065795898, 1.9530037641525269, 1.9555995464324951, 1.9514042139053345, 1.953285574913025, 1.9594557285308838, 1.9593009948730469, 1.9592968225479126, 1.9593302011489868, 1.9582051038742065, 1.9578526020050049, 1.9531888961791992, 1.9537512063980103, 1.9545916318893433, 1.9545906782150269, 1.953850269317627]
test acc:  [26.666667938232422, 40.0, 46.66666793823242, 53.333335876464844, 49.333335876464844, 52.0, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 50.66666793823242, 52.0, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 52.0, 50.66666793823242, 52.0, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 52.0, 52.0, 54.66666793823242, 54.66666793823242, 54.66666793823242, 54.66666793823242, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 50.66666793823242, 53.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 49.333335876464844, 49.333335876464844, 49.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 54.66666793823242, 54.66666793823242, 54.66666793823242, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]
best prec: 54.66666793823242
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 1.003 (1.003)	Data 0.079 (0.079)	Loss 2.4870 (2.4870)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 2.1375 (2.1375)	Prec@1 26.667 (26.667)
 * Prec@1 26.667
26.666667938232422
Epoch: [1][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 0.7644 (0.7644)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7698 (1.7698)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
40.0
Epoch: [2][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 2.7904 (2.7904)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5663 (1.5663)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
46.66666793823242
Epoch: [3][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.1750 (0.1750)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.5074 (1.5074)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [4][0/1]	Time 0.139 (0.139)	Data 0.130 (0.130)	Loss 0.2860 (0.2860)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.5705 (1.5705)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
53.333335876464844
Epoch: [5][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.3112 (0.3112)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4977 (1.4977)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [6][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.2468 (0.2468)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4569 (1.4569)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [7][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0805 (0.0805)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4932 (1.4932)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [8][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss 0.0881 (0.0881)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 1.5424 (1.5424)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [9][0/1]	Time 0.107 (0.107)	Data 0.101 (0.101)	Loss 0.0646 (0.0646)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5621 (1.5621)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [10][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.0410 (0.0410)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.5647 (1.5647)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [11][0/1]	Time 0.122 (0.122)	Data 0.113 (0.113)	Loss 0.0284 (0.0284)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5866 (1.5866)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [12][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.1056 (0.1056)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 1.5540 (1.5540)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [13][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.1993 (0.1993)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5519 (1.5519)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [14][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.1450 (0.1450)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7148 (1.7148)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [15][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0350 (0.0350)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.7331 (1.7331)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [16][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.1209 (0.1209)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.162 (0.162)	Loss 1.8181 (1.8181)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [17][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0123 (0.0123)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.8183 (1.8183)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [18][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0143 (0.0143)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.8264 (1.8264)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [19][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0601 (0.0601)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.7918 (1.7918)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [20][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0154 (0.0154)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 1.7896 (1.7896)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [21][0/1]	Time 0.112 (0.112)	Data 0.107 (0.107)	Loss 0.0064 (0.0064)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7893 (1.7893)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [22][0/1]	Time 0.137 (0.137)	Data 0.128 (0.128)	Loss 0.0317 (0.0317)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.7731 (1.7731)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [23][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0108 (0.0108)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.7752 (1.7752)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [24][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss 0.2206 (0.2206)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.8039 (1.8039)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [25][0/1]	Time 0.115 (0.115)	Data 0.106 (0.106)	Loss 0.3651 (0.3651)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.7453 (1.7453)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
53.333335876464844
Epoch: [26][0/1]	Time 0.116 (0.116)	Data 0.107 (0.107)	Loss 0.0532 (0.0532)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7340 (1.7340)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [27][0/1]	Time 0.119 (0.119)	Data 0.109 (0.109)	Loss 0.0182 (0.0182)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.7292 (1.7292)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [28][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0418 (0.0418)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.7636 (1.7636)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [29][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.2877 (0.2877)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.156 (0.156)	Loss 1.8465 (1.8465)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [30][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0153 (0.0153)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.8483 (1.8483)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [31][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0738 (0.0738)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.8221 (1.8221)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [32][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.1547 (0.1547)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8281 (1.8281)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [33][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.0267 (0.0267)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.8369 (1.8369)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [34][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0473 (0.0473)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.8555 (1.8555)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
53.333335876464844
Epoch: [35][0/1]	Time 0.118 (0.118)	Data 0.113 (0.113)	Loss 0.5279 (0.5279)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.8408 (1.8408)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [36][0/1]	Time 0.135 (0.135)	Data 0.128 (0.128)	Loss 0.0383 (0.0383)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.8312 (1.8312)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [37][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0327 (0.0327)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.8595 (1.8595)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [38][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0988 (0.0988)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.8097 (1.8097)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [39][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0693 (0.0693)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.8063 (1.8063)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [40][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0859 (0.0859)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.8257 (1.8257)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [41][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0216 (0.0216)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.8193 (1.8193)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [42][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss 0.1931 (0.1931)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.8302 (1.8302)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [43][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0841 (0.0841)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.8647 (1.8647)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [44][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0128 (0.0128)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.8760 (1.8760)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [45][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.0201 (0.0201)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 1.8753 (1.8753)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [46][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.0504 (0.0504)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.8815 (1.8815)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [47][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.1089 (0.1089)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.9220 (1.9220)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [48][0/1]	Time 0.138 (0.138)	Data 0.132 (0.132)	Loss 0.0373 (0.0373)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.9200 (1.9200)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [49][0/1]	Time 0.115 (0.115)	Data 0.110 (0.110)	Loss 0.0787 (0.0787)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.9854 (1.9854)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [50][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0505 (0.0505)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 2.0011 (2.0011)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [51][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.0118 (0.0118)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 2.0040 (2.0040)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [52][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.0174 (0.0174)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.0138 (2.0138)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [53][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0513 (0.0513)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.0636 (2.0636)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [54][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0277 (0.0277)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.0647 (2.0647)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [55][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0220 (0.0220)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.0589 (2.0589)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [56][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0136 (0.0136)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 2.0548 (2.0548)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [57][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0055 (0.0055)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 2.0578 (2.0578)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [58][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0271 (0.0271)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.0709 (2.0709)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [59][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.0094 (0.0094)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.0708 (2.0708)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [60][0/1]	Time 0.121 (0.121)	Data 0.116 (0.116)	Loss 0.0103 (0.0103)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.0658 (2.0658)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [61][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0059 (0.0059)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.0682 (2.0682)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [62][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0070 (0.0070)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.0722 (2.0722)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [63][0/1]	Time 0.108 (0.108)	Data 0.103 (0.103)	Loss 0.0326 (0.0326)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.0182 (2.0182)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [64][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0350 (0.0350)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9644 (1.9644)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [65][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss 0.0336 (0.0336)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9639 (1.9639)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [66][0/1]	Time 0.109 (0.109)	Data 0.103 (0.103)	Loss 0.1301 (0.1301)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9722 (1.9722)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [67][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0060 (0.0060)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.9715 (1.9715)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [68][0/1]	Time 0.136 (0.136)	Data 0.130 (0.130)	Loss 0.1063 (0.1063)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9682 (1.9682)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [69][0/1]	Time 0.127 (0.127)	Data 0.119 (0.119)	Loss 0.0292 (0.0292)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 2.0042 (2.0042)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
54.66666793823242
Epoch: [70][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0057 (0.0057)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 2.0086 (2.0086)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
54.66666793823242
Epoch: [71][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0098 (0.0098)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.0153 (2.0153)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
54.66666793823242
Epoch: [72][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.0129 (0.0129)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 2.0162 (2.0162)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [73][0/1]	Time 0.110 (0.110)	Data 0.104 (0.104)	Loss 0.0038 (0.0038)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.0182 (2.0182)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [74][0/1]	Time 0.110 (0.110)	Data 0.100 (0.100)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 2.0318 (2.0318)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
54.66666793823242
Epoch: [75][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.3342 (0.3342)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.9225 (1.9225)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [76][0/1]	Time 0.138 (0.138)	Data 0.130 (0.130)	Loss 0.0227 (0.0227)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.9207 (1.9207)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [77][0/1]	Time 0.116 (0.116)	Data 0.106 (0.106)	Loss 0.0755 (0.0755)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.9446 (1.9446)	Prec@1 54.667 (54.667)
 * Prec@1 54.667
54.66666793823242
Epoch: [78][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.0612 (0.0612)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9173 (1.9173)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [79][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0595 (0.0595)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.8739 (1.8739)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [80][0/1]	Time 0.109 (0.109)	Data 0.103 (0.103)	Loss 0.0406 (0.0406)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.8655 (1.8655)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [81][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0267 (0.0267)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8751 (1.8751)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [82][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0075 (0.0075)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8732 (1.8732)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [83][0/1]	Time 0.135 (0.135)	Data 0.129 (0.129)	Loss 0.0088 (0.0088)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.8768 (1.8768)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [84][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0071 (0.0071)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.8764 (1.8764)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [85][0/1]	Time 0.108 (0.108)	Data 0.102 (0.102)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.8822 (1.8822)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [86][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0086 (0.0086)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.8896 (1.8896)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [87][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.0219 (0.0219)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9109 (1.9109)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [88][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 0.0100 (0.0100)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9167 (1.9167)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [89][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.0404 (0.0404)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.9237 (1.9237)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [90][0/1]	Time 0.118 (0.118)	Data 0.110 (0.110)	Loss 0.0124 (0.0124)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.9339 (1.9339)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [91][0/1]	Time 0.125 (0.125)	Data 0.120 (0.120)	Loss 0.0686 (0.0686)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.8821 (1.8821)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [92][0/1]	Time 0.125 (0.125)	Data 0.116 (0.116)	Loss 0.0089 (0.0089)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8836 (1.8836)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [93][0/1]	Time 0.138 (0.138)	Data 0.132 (0.132)	Loss 0.0074 (0.0074)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.8873 (1.8873)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [94][0/1]	Time 0.112 (0.112)	Data 0.103 (0.103)	Loss 0.0068 (0.0068)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.8863 (1.8863)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
54.66666793823242
Epoch: [95][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0083 (0.0083)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.8950 (1.8950)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [96][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0159 (0.0159)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9085 (1.9085)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [97][0/1]	Time 0.117 (0.117)	Data 0.106 (0.106)	Loss 0.1101 (0.1101)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.9508 (1.9508)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [98][0/1]	Time 0.137 (0.137)	Data 0.127 (0.127)	Loss 0.0389 (0.0389)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.9407 (1.9407)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [99][0/1]	Time 0.123 (0.123)	Data 0.114 (0.114)	Loss 0.0348 (0.0348)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.119 (0.119)	Loss 1.9438 (1.9438)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [100][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0036 (0.0036)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.9437 (1.9437)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [101][0/1]	Time 0.126 (0.126)	Data 0.118 (0.118)	Loss 0.0127 (0.0127)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 1.9429 (1.9429)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [102][0/1]	Time 0.134 (0.134)	Data 0.128 (0.128)	Loss 0.0890 (0.0890)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9469 (1.9469)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [103][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.0338 (0.0338)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.9483 (1.9483)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [104][0/1]	Time 0.132 (0.132)	Data 0.123 (0.123)	Loss 0.0086 (0.0086)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.155 (0.155)	Loss 1.9487 (1.9487)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [105][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.0234 (0.0234)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 1.9488 (1.9488)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [106][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.2547 (0.2547)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.9418 (1.9418)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [107][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0232 (0.0232)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.9451 (1.9451)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [108][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0234 (0.0234)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9464 (1.9464)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [109][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0127 (0.0127)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9471 (1.9471)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [110][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0044 (0.0044)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9472 (1.9472)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [111][0/1]	Time 0.128 (0.128)	Data 0.122 (0.122)	Loss 0.0105 (0.0105)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.142 (0.142)	Loss 1.9488 (1.9488)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [112][0/1]	Time 0.117 (0.117)	Data 0.112 (0.112)	Loss 0.0305 (0.0305)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.9484 (1.9484)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [113][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0038 (0.0038)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9486 (1.9486)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [114][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0082 (0.0082)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9494 (1.9494)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [115][0/1]	Time 0.114 (0.114)	Data 0.105 (0.105)	Loss 0.0046 (0.0046)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9493 (1.9493)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [116][0/1]	Time 0.133 (0.133)	Data 0.127 (0.127)	Loss 0.0287 (0.0287)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.9491 (1.9491)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [117][0/1]	Time 0.108 (0.108)	Data 0.102 (0.102)	Loss 0.0357 (0.0357)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9480 (1.9480)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [118][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0075 (0.0075)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.9481 (1.9481)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [119][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0104 (0.0104)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.9481 (1.9481)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [120][0/1]	Time 0.136 (0.136)	Data 0.130 (0.130)	Loss 0.0130 (0.0130)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.9483 (1.9483)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [121][0/1]	Time 0.135 (0.135)	Data 0.126 (0.126)	Loss 0.0696 (0.0696)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9450 (1.9450)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [122][0/1]	Time 0.128 (0.128)	Data 0.120 (0.120)	Loss 0.0404 (0.0404)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9475 (1.9475)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [123][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0444 (0.0444)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9486 (1.9486)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [124][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0071 (0.0071)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9484 (1.9484)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [125][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0042 (0.0042)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9487 (1.9487)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [126][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss 0.0065 (0.0065)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 1.9493 (1.9493)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [127][0/1]	Time 0.127 (0.127)	Data 0.119 (0.119)	Loss 0.0053 (0.0053)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9494 (1.9494)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [128][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.0062 (0.0062)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.111 (0.111)	Loss 1.9497 (1.9497)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [129][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0075 (0.0075)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9498 (1.9498)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [130][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0132 (0.0132)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9511 (1.9511)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [131][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.0107 (0.0107)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.9511 (1.9511)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [132][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0288 (0.0288)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.9533 (1.9533)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [133][0/1]	Time 0.142 (0.142)	Data 0.136 (0.136)	Loss 0.0128 (0.0128)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9531 (1.9531)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [134][0/1]	Time 0.137 (0.137)	Data 0.127 (0.127)	Loss 0.0071 (0.0071)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.9525 (1.9525)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [135][0/1]	Time 0.141 (0.141)	Data 0.133 (0.133)	Loss 0.0225 (0.0225)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.9530 (1.9530)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [136][0/1]	Time 0.120 (0.120)	Data 0.115 (0.115)	Loss 0.0361 (0.0361)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9556 (1.9556)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [137][0/1]	Time 0.124 (0.124)	Data 0.115 (0.115)	Loss 0.0667 (0.0667)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9514 (1.9514)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [138][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.0204 (0.0204)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.9533 (1.9533)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [139][0/1]	Time 0.112 (0.112)	Data 0.105 (0.105)	Loss 0.2842 (0.2842)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.9595 (1.9595)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [140][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.0072 (0.0072)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.9593 (1.9593)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [141][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0009 (0.0009)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9593 (1.9593)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [142][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0040 (0.0040)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9593 (1.9593)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [143][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0264 (0.0264)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 1.9582 (1.9582)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [144][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0134 (0.0134)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.9579 (1.9579)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [145][0/1]	Time 0.116 (0.116)	Data 0.106 (0.106)	Loss 0.0738 (0.0738)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.9532 (1.9532)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [146][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss 0.0114 (0.0114)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9538 (1.9538)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [147][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 0.0066 (0.0066)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9546 (1.9546)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [148][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.0027 (0.0027)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.9546 (1.9546)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
Epoch: [149][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0884 (0.0884)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.9538 (1.9538)	Prec@1 52.000 (52.000)
 * Prec@1 52.000
54.66666793823242
total time: 53.46768140792847
train loss:  [2.4869697093963623, 0.764398992061615, 2.790433645248413, 0.1749817579984665, 0.2859737277030945, 0.3112102448940277, 0.24684318900108337, 0.08047740161418915, 0.08814239501953125, 0.06462478637695312, 0.04100070148706436, 0.02837216854095459, 0.10555772483348846, 0.19927915930747986, 0.14500322937965393, 0.034967757761478424, 0.12090416252613068, 0.012308454141020775, 0.014311695471405983, 0.060115862637758255, 0.01538858376443386, 0.006395530886948109, 0.03168203681707382, 0.010825872421264648, 0.22058983147144318, 0.3650943636894226, 0.05319356918334961, 0.018216991797089577, 0.04180047661066055, 0.28766193985939026, 0.015288352966308594, 0.07378105819225311, 0.15473265945911407, 0.026714850217103958, 0.04731140285730362, 0.5279488563537598, 0.03833331912755966, 0.032746028155088425, 0.09875287860631943, 0.0693337470293045, 0.08585095405578613, 0.021642232313752174, 0.19311609864234924, 0.0840996727347374, 0.012750434689223766, 0.02011556550860405, 0.050434112548828125, 0.10892269760370255, 0.03730068355798721, 0.07867169380187988, 0.0504944808781147, 0.01181859988719225, 0.017357254400849342, 0.05130653455853462, 0.027712058275938034, 0.02197246626019478, 0.013641595840454102, 0.005522346589714289, 0.0271119587123394, 0.009435844607651234, 0.010318231768906116, 0.0058934688568115234, 0.007015419192612171, 0.032573796808719635, 0.03496222570538521, 0.033577919006347656, 0.13005618751049042, 0.006010008044540882, 0.10633949935436249, 0.0291626937687397, 0.005727291107177734, 0.009798574261367321, 0.012905120849609375, 0.0038085461128503084, 0.017742538824677467, 0.33420494198799133, 0.02268047258257866, 0.07550577819347382, 0.061171818524599075, 0.05951886251568794, 0.04056067392230034, 0.026741409674286842, 0.007466554641723633, 0.008791160769760609, 0.007080888841301203, 0.01773858070373535, 0.008601903915405273, 0.021939849480986595, 0.010010575875639915, 0.040430307388305664, 0.012371206656098366, 0.06855201721191406, 0.008932304568588734, 0.007410907652229071, 0.006801414303481579, 0.008342075161635876, 0.015877246856689453, 0.11012516170740128, 0.03888244554400444, 0.03478055074810982, 0.0035959244705736637, 0.012741947546601295, 0.08898322284221649, 0.03376879543066025, 0.008646202273666859, 0.023380089551210403, 0.2547334134578705, 0.023160934448242188, 0.023376893252134323, 0.012735891155898571, 0.004355430603027344, 0.010487079620361328, 0.030503178015351295, 0.003810882568359375, 0.008185004815459251, 0.004631089977920055, 0.028706789016723633, 0.035718392580747604, 0.0074554444290697575, 0.010371064767241478, 0.013008070178329945, 0.0696166530251503, 0.04039764404296875, 0.044443272054195404, 0.007090854458510876, 0.004221534822136164, 0.006498909089714289, 0.005332136061042547, 0.0062427520751953125, 0.007505607791244984, 0.013152885250747204, 0.010669708251953125, 0.028820419684052467, 0.012843990698456764, 0.007134771440178156, 0.022545481100678444, 0.03611164167523384, 0.06671271473169327, 0.020356129854917526, 0.2842089533805847, 0.007197094149887562, 0.0009435653919354081, 0.003996086306869984, 0.026372481137514114, 0.013405418023467064, 0.07380056381225586, 0.01135950069874525, 0.006585121154785156, 0.002715968992561102, 0.08840970695018768]
train acc:  [40.0, 60.0, 40.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
test loss:  [2.1375277042388916, 1.769789457321167, 1.5663026571273804, 1.5073907375335693, 1.5705121755599976, 1.4976624250411987, 1.456886649131775, 1.4932119846343994, 1.542371153831482, 1.562144160270691, 1.5647119283676147, 1.5866336822509766, 1.5540060997009277, 1.5519405603408813, 1.7147631645202637, 1.7330747842788696, 1.8180735111236572, 1.8183374404907227, 1.8263897895812988, 1.7918084859848022, 1.7896020412445068, 1.789268970489502, 1.7731187343597412, 1.7752093076705933, 1.8039453029632568, 1.7452986240386963, 1.7340134382247925, 1.7292240858078003, 1.7636370658874512, 1.8464646339416504, 1.8483068943023682, 1.8220932483673096, 1.8281422853469849, 1.8369433879852295, 1.8555418252944946, 1.8408398628234863, 1.8311995267868042, 1.8594863414764404, 1.809667944908142, 1.8063052892684937, 1.8257269859313965, 1.8192806243896484, 1.8301780223846436, 1.8647290468215942, 1.8760091066360474, 1.8752516508102417, 1.8815191984176636, 1.9220134019851685, 1.9200232028961182, 1.985403060913086, 2.001124620437622, 2.0040252208709717, 2.0137882232666016, 2.063638210296631, 2.0646684169769287, 2.0589234828948975, 2.054781675338745, 2.0578465461730957, 2.0709304809570312, 2.070847272872925, 2.0658299922943115, 2.068223237991333, 2.0721747875213623, 2.0182487964630127, 1.9643880128860474, 1.9639028310775757, 1.9721791744232178, 1.971486210823059, 1.9681733846664429, 2.004167318344116, 2.008572578430176, 2.0152525901794434, 2.0162429809570312, 2.018214464187622, 2.031818389892578, 1.922460675239563, 1.9206537008285522, 1.9446390867233276, 1.917271375656128, 1.8739405870437622, 1.8654603958129883, 1.875090479850769, 1.8731809854507446, 1.8767932653427124, 1.8763539791107178, 1.8821555376052856, 1.8895912170410156, 1.9108654260635376, 1.9167295694351196, 1.9236714839935303, 1.9338860511779785, 1.8821492195129395, 1.8836028575897217, 1.8873034715652466, 1.8863030672073364, 1.8949567079544067, 1.9084696769714355, 1.9508137702941895, 1.9406548738479614, 1.9437705278396606, 1.9436836242675781, 1.9428857564926147, 1.9469351768493652, 1.9483026266098022, 1.9486818313598633, 1.9487967491149902, 1.9417543411254883, 1.9450864791870117, 1.946380615234375, 1.9471228122711182, 1.9472469091415405, 1.948791265487671, 1.9483827352523804, 1.9486356973648071, 1.949372410774231, 1.949310302734375, 1.9490702152252197, 1.9479634761810303, 1.9480671882629395, 1.9481197595596313, 1.9482570886611938, 1.944980502128601, 1.9475187063217163, 1.948609471321106, 1.9484177827835083, 1.9487488269805908, 1.9493039846420288, 1.9494431018829346, 1.94965398311615, 1.9498382806777954, 1.9510878324508667, 1.9511497020721436, 1.9532729387283325, 1.9531329870224, 1.9524906873703003, 1.953002691268921, 1.9555985927581787, 1.9514033794403076, 1.9532865285873413, 1.9594556093215942, 1.9593013525009155, 1.9592978954315186, 1.9593310356140137, 1.958204746246338, 1.957852840423584, 1.9531886577606201, 1.9537508487701416, 1.9545921087265015, 1.9545904397964478, 1.9538488388061523]
test acc:  [26.666667938232422, 40.0, 46.66666793823242, 53.333335876464844, 49.333335876464844, 52.0, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 50.66666793823242, 52.0, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 52.0, 50.66666793823242, 52.0, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 52.0, 52.0, 54.66666793823242, 54.66666793823242, 54.66666793823242, 54.66666793823242, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 50.66666793823242, 53.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 49.333335876464844, 49.333335876464844, 49.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 54.66666793823242, 54.66666793823242, 54.66666793823242, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 53.333335876464844, 53.333335876464844, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0, 52.0]
best prec: 54.66666793823242
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 0.999 (0.999)	Data 0.078 (0.078)	Loss 2.4870 (2.4870)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.174 (0.174)	Loss 752.1954 (752.1954)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
20.0
Epoch: [1][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 633.0273 (633.0273)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2442.9893 (2442.9893)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
21.33333396911621
Epoch: [2][0/1]	Time 0.137 (0.137)	Data 0.130 (0.130)	Loss 2223.6311 (2223.6311)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 2678.4109 (2678.4109)	Prec@1 22.667 (22.667)
 * Prec@1 22.667
22.666667938232422
Epoch: [3][0/1]	Time 0.133 (0.133)	Data 0.126 (0.126)	Loss 1357.8326 (1357.8326)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 28175.5293 (28175.5293)	Prec@1 16.000 (16.000)
 * Prec@1 16.000
22.666667938232422
Epoch: [4][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss 29820.0039 (29820.0039)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 165382.5000 (165382.5000)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
22.666667938232422
Epoch: [5][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 157008.2969 (157008.2969)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 207449.1406 (207449.1406)	Prec@1 16.000 (16.000)
 * Prec@1 16.000
22.666667938232422
Epoch: [6][0/1]	Time 0.132 (0.132)	Data 0.125 (0.125)	Loss 100681.5859 (100681.5859)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 133476.7344 (133476.7344)	Prec@1 14.667 (14.667)
 * Prec@1 14.667
22.666667938232422
Epoch: [7][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 111361.1250 (111361.1250)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 813458.2500 (813458.2500)	Prec@1 17.333 (17.333)
 * Prec@1 17.333
22.666667938232422
Epoch: [8][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 757155.4375 (757155.4375)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.145 (0.145)	Loss 298018.4688 (298018.4688)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [9][0/1]	Time 0.128 (0.128)	Data 0.121 (0.121)	Loss 283587.1250 (283587.1250)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1911841.5000 (1911841.5000)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
22.666667938232422
Epoch: [10][0/1]	Time 0.142 (0.142)	Data 0.131 (0.131)	Loss 1841186.7500 (1841186.7500)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 3958997.2500 (3958997.2500)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [11][0/1]	Time 0.130 (0.130)	Data 0.123 (0.123)	Loss 4228958.0000 (4228958.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 40163008.0000 (40163008.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [12][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 40180252.0000 (40180252.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 233467792.0000 (233467792.0000)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
22.666667938232422
Epoch: [13][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 218846816.0000 (218846816.0000)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 356556896.0000 (356556896.0000)	Prec@1 18.667 (18.667)
 * Prec@1 18.667
22.666667938232422
Epoch: [14][0/1]	Time 0.134 (0.134)	Data 0.126 (0.126)	Loss 281865408.0000 (281865408.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2639619584.0000 (2639619584.0000)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
22.666667938232422
Epoch: [15][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 3192869632.0000 (3192869632.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 16168890368.0000 (16168890368.0000)	Prec@1 18.667 (18.667)
 * Prec@1 18.667
22.666667938232422
Epoch: [16][0/1]	Time 0.120 (0.120)	Data 0.109 (0.109)	Loss 17935712256.0000 (17935712256.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 56464281600.0000 (56464281600.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [17][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 57006129152.0000 (57006129152.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 272355180544.0000 (272355180544.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [18][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 261578244096.0000 (261578244096.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1529083985920.0000 (1529083985920.0000)	Prec@1 14.667 (14.667)
 * Prec@1 14.667
22.666667938232422
Epoch: [19][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 1555127205888.0000 (1555127205888.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 4752737828864.0000 (4752737828864.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [20][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 5396917387264.0000 (5396917387264.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 70504429912064.0000 (70504429912064.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [21][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 77880012832768.0000 (77880012832768.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 260605965500416.0000 (260605965500416.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [22][0/1]	Time 0.172 (0.172)	Data 0.165 (0.165)	Loss 288853227208704.0000 (288853227208704.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2004927375212544.0000 (2004927375212544.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
22.666667938232422
Epoch: [23][0/1]	Time 0.134 (0.134)	Data 0.127 (0.127)	Loss 1952730100793344.0000 (1952730100793344.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 8229782256877568.0000 (8229782256877568.0000)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
22.666667938232422
Epoch: [24][0/1]	Time 0.132 (0.132)	Data 0.125 (0.125)	Loss 9504259041132544.0000 (9504259041132544.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 18400786652332032.0000 (18400786652332032.0000)	Prec@1 17.333 (17.333)
 * Prec@1 17.333
22.666667938232422
Epoch: [25][0/1]	Time 0.129 (0.129)	Data 0.118 (0.118)	Loss 17278556795043840.0000 (17278556795043840.0000)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 40746015434735616.0000 (40746015434735616.0000)	Prec@1 25.333 (25.333)
 * Prec@1 25.333
25.33333396911621
Epoch: [26][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 32816910953349120.0000 (32816910953349120.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 19077561829031936.0000 (19077561829031936.0000)	Prec@1 25.333 (25.333)
 * Prec@1 25.333
25.33333396911621
Epoch: [27][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 24941635297083392.0000 (24941635297083392.0000)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 75063375360425984.0000 (75063375360425984.0000)	Prec@1 25.333 (25.333)
 * Prec@1 25.333
25.33333396911621
Epoch: [28][0/1]	Time 0.118 (0.118)	Data 0.107 (0.107)	Loss 73012906433708032.0000 (73012906433708032.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 239027591585988608.0000 (239027591585988608.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
25.33333396911621
Epoch: [29][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 236842999420551168.0000 (236842999420551168.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 497095116792528896.0000 (497095116792528896.0000)	Prec@1 29.333 (29.333)
 * Prec@1 29.333
29.33333396911621
Epoch: [30][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 551847531680628736.0000 (551847531680628736.0000)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 5015205686309552128.0000 (5015205686309552128.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [31][0/1]	Time 0.141 (0.141)	Data 0.134 (0.134)	Loss 5063302722954985472.0000 (5063302722954985472.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 2682495285837430784.0000 (2682495285837430784.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [32][0/1]	Time 0.142 (0.142)	Data 0.134 (0.134)	Loss 2026556472959172608.0000 (2026556472959172608.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 10642386350499692544.0000 (10642386350499692544.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [33][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 13176434097569923072.0000 (13176434097569923072.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 83984348508135096320.0000 (83984348508135096320.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [34][0/1]	Time 0.128 (0.128)	Data 0.120 (0.120)	Loss 83984339712042074112.0000 (83984339712042074112.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 337585746680530075648.0000 (337585746680530075648.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [35][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 337585817049274253312.0000 (337585817049274253312.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1653632977815293919232.0000 (1653632977815293919232.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [36][0/1]	Time 0.127 (0.127)	Data 0.119 (0.119)	Loss 1653632555602828853248.0000 (1653632555602828853248.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 7017327669483705204736.0000 (7017327669483705204736.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [37][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 7017327669483705204736.0000 (7017327669483705204736.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 13538393562928293871616.0000 (13538393562928293871616.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [38][0/1]	Time 0.127 (0.127)	Data 0.116 (0.116)	Loss 13538393562928293871616.0000 (13538393562928293871616.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 305390099538309864751104.0000 (305390099538309864751104.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [39][0/1]	Time 0.128 (0.128)	Data 0.120 (0.120)	Loss 305390099538309864751104.0000 (305390099538309864751104.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 423372015869242336346112.0000 (423372015869242336346112.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [40][0/1]	Time 0.119 (0.119)	Data 0.110 (0.110)	Loss 423371979840445317382144.0000 (423371979840445317382144.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 9499747497635367647969280.0000 (9499747497635367647969280.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [41][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 9499746921174615344545792.0000 (9499746921174615344545792.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 9912157319666025969483776.0000 (9912157319666025969483776.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [42][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 9912156166744521362636800.0000 (9912156166744521362636800.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 84252109355758814448582656.0000 (84252109355758814448582656.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [43][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 84252109355758814448582656.0000 (84252109355758814448582656.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 350954766238570037521678336.0000 (350954766238570037521678336.0000)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
29.33333396911621
Epoch: [44][0/1]	Time 0.129 (0.129)	Data 0.122 (0.122)	Loss 350954729345081890102575104.0000 (350954729345081890102575104.0000)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.116 (0.116)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [45][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss nan (nan)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.141 (0.141)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [46][0/1]	Time 0.129 (0.129)	Data 0.121 (0.121)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.127 (0.127)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [47][0/1]	Time 0.138 (0.138)	Data 0.130 (0.130)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.144 (0.144)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [48][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.129 (0.129)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [49][0/1]	Time 0.118 (0.118)	Data 0.109 (0.109)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.128 (0.128)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [50][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.142 (0.142)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [51][0/1]	Time 0.118 (0.118)	Data 0.107 (0.107)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.117 (0.117)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [52][0/1]	Time 0.124 (0.124)	Data 0.113 (0.113)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.126 (0.126)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [53][0/1]	Time 0.117 (0.117)	Data 0.109 (0.109)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.119 (0.119)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [54][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.127 (0.127)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [55][0/1]	Time 0.123 (0.123)	Data 0.115 (0.115)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.139 (0.139)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [56][0/1]	Time 0.128 (0.128)	Data 0.121 (0.121)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.115 (0.115)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [57][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.140 (0.140)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [58][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.137 (0.137)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [59][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.141 (0.141)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [60][0/1]	Time 0.129 (0.129)	Data 0.120 (0.120)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.137 (0.137)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
Epoch: [61][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss nan (nan)	Prec@1 0.000 (0.000)
Test: [0/1]	Time 0.139 (0.139)	Loss nan (nan)	Prec@1 0.000 (0.000)
 * Prec@1 0.000
29.33333396911621
=> loading checkpoint './save_final/cifarfs/subspace/5_way_1_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.4524
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 1.001 (1.001)	Data 0.080 (0.080)	Loss 2.4870 (2.4870)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 206.0753 (206.0753)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
20.0
Epoch: [1][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 231.9542 (231.9542)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 734.2628 (734.2628)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
20.0
Epoch: [2][0/1]	Time 0.133 (0.133)	Data 0.124 (0.124)	Loss 732.6510 (732.6510)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 208.0466 (208.0466)	Prec@1 14.667 (14.667)
 * Prec@1 14.667
20.0
Epoch: [3][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 131.2421 (131.2421)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 524.2668 (524.2668)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
20.0
Epoch: [4][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 546.5222 (546.5222)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 241.9295 (241.9295)	Prec@1 24.000 (24.000)
 * Prec@1 24.000
24.0
Epoch: [5][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 217.5313 (217.5313)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 797.0651 (797.0651)	Prec@1 24.000 (24.000)
 * Prec@1 24.000
24.0
Epoch: [6][0/1]	Time 0.127 (0.127)	Data 0.118 (0.118)	Loss 805.4496 (805.4496)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 3074.7151 (3074.7151)	Prec@1 18.667 (18.667)
 * Prec@1 18.667
24.0
Epoch: [7][0/1]	Time 0.110 (0.110)	Data 0.103 (0.103)	Loss 2884.3777 (2884.3777)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1519.7723 (1519.7723)	Prec@1 21.333 (21.333)
 * Prec@1 21.333
24.0
Epoch: [8][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 1635.7173 (1635.7173)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1963.1666 (1963.1666)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
24.0
Epoch: [9][0/1]	Time 0.132 (0.132)	Data 0.125 (0.125)	Loss 1832.0121 (1832.0121)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 8898.8018 (8898.8018)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
24.0
Epoch: [10][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 8800.5918 (8800.5918)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1035.7859 (1035.7859)	Prec@1 26.667 (26.667)
 * Prec@1 26.667
26.666667938232422
Epoch: [11][0/1]	Time 0.134 (0.134)	Data 0.127 (0.127)	Loss 874.2197 (874.2197)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1461.9646 (1461.9646)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [12][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 1539.4304 (1539.4304)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 4791.5376 (4791.5376)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [13][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss 5017.5649 (5017.5649)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.145 (0.145)	Loss 3006.3025 (3006.3025)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [14][0/1]	Time 0.131 (0.131)	Data 0.124 (0.124)	Loss 2944.3245 (2944.3245)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2301.5476 (2301.5476)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [15][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 2215.8582 (2215.8582)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 15764.1250 (15764.1250)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [16][0/1]	Time 0.117 (0.117)	Data 0.109 (0.109)	Loss 14965.6670 (14965.6670)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 4403.2534 (4403.2534)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [17][0/1]	Time 0.112 (0.112)	Data 0.105 (0.105)	Loss 4432.3945 (4432.3945)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 4284.2588 (4284.2588)	Prec@1 17.333 (17.333)
 * Prec@1 17.333
26.666667938232422
Epoch: [18][0/1]	Time 0.141 (0.141)	Data 0.135 (0.135)	Loss 3832.4224 (3832.4224)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.156 (0.156)	Loss 10458.9609 (10458.9609)	Prec@1 20.000 (20.000)
 * Prec@1 20.000
26.666667938232422
Epoch: [19][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 10549.7764 (10549.7764)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 12416.9346 (12416.9346)	Prec@1 17.333 (17.333)
 * Prec@1 17.333
26.666667938232422
=> loading checkpoint './save_final/cifarfs/subspace/5_way_1_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.4524
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 1.014 (1.014)	Data 0.079 (0.079)	Loss 2.4870 (2.4870)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.4229 (1.4229)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
40.0
Epoch: [1][0/1]	Time 0.120 (0.120)	Data 0.110 (0.110)	Loss 0.9859 (0.9859)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.4154 (1.4154)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
41.333335876464844
Epoch: [2][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 1.2404 (1.2404)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.3772 (1.3772)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
42.66666793823242
Epoch: [3][0/1]	Time 0.128 (0.128)	Data 0.122 (0.122)	Loss 1.0424 (1.0424)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.3878 (1.3878)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
42.66666793823242
Epoch: [4][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.9986 (0.9986)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.3906 (1.3906)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [5][0/1]	Time 0.117 (0.117)	Data 0.112 (0.112)	Loss 0.6863 (0.6863)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.4057 (1.4057)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [6][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.8398 (0.8398)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.3811 (1.3811)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [7][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.8274 (0.8274)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.3911 (1.3911)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [8][0/1]	Time 0.133 (0.133)	Data 0.126 (0.126)	Loss 0.8289 (0.8289)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.3964 (1.3964)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [9][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.5728 (0.5728)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.3989 (1.3989)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [10][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.6719 (0.6719)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.3954 (1.3954)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [11][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.5582 (0.5582)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4040 (1.4040)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [12][0/1]	Time 0.116 (0.116)	Data 0.107 (0.107)	Loss 0.6308 (0.6308)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.4151 (1.4151)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [13][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.6642 (0.6642)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.4075 (1.4075)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [14][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.4383 (0.4383)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.4253 (1.4253)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [15][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.5038 (0.5038)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4367 (1.4367)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [16][0/1]	Time 0.126 (0.126)	Data 0.117 (0.117)	Loss 0.5466 (0.5466)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.4249 (1.4249)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [17][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.3436 (0.3436)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.154 (0.154)	Loss 1.4348 (1.4348)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [18][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.5506 (0.5506)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 1.4409 (1.4409)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [19][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.6536 (0.6536)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 1.4451 (1.4451)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [20][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.4457 (0.4457)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.4509 (1.4509)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [21][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.7404 (0.7404)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.4518 (1.4518)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [22][0/1]	Time 0.138 (0.138)	Data 0.130 (0.130)	Loss 0.5215 (0.5215)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4574 (1.4574)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
50.66666793823242
Epoch: [23][0/1]	Time 0.122 (0.122)	Data 0.117 (0.117)	Loss 0.3001 (0.3001)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.4610 (1.4610)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
50.66666793823242
Epoch: [24][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.3799 (0.3799)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.4812 (1.4812)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
50.66666793823242
Epoch: [25][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.3842 (0.3842)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.5110 (1.5110)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
50.66666793823242
Epoch: [26][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.5109 (0.5109)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.4948 (1.4948)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [27][0/1]	Time 0.116 (0.116)	Data 0.111 (0.111)	Loss 0.2147 (0.2147)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.5045 (1.5045)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [28][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.3046 (0.3046)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5145 (1.5145)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [29][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.4429 (0.4429)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.5421 (1.5421)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [30][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.2134 (0.2134)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 1.5557 (1.5557)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [31][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.3145 (0.3145)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5769 (1.5769)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [32][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.2996 (0.2996)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.145 (0.145)	Loss 1.5971 (1.5971)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [33][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.2716 (0.2716)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.5997 (1.5997)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [34][0/1]	Time 0.122 (0.122)	Data 0.117 (0.117)	Loss 0.2496 (0.2496)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.6067 (1.6067)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [35][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.3597 (0.3597)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.161 (0.161)	Loss 1.6401 (1.6401)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [36][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.3584 (0.3584)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.6592 (1.6592)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [37][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.2387 (0.2387)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.6777 (1.6777)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [38][0/1]	Time 0.115 (0.115)	Data 0.106 (0.106)	Loss 0.3394 (0.3394)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7038 (1.7038)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [39][0/1]	Time 0.120 (0.120)	Data 0.115 (0.115)	Loss 0.2948 (0.2948)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.7202 (1.7202)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [40][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.2034 (0.2034)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.7314 (1.7314)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [41][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.2406 (0.2406)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.7361 (1.7361)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [42][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.2632 (0.2632)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.7504 (1.7504)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [43][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.2813 (0.2813)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.7700 (1.7700)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [44][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.1484 (0.1484)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.7805 (1.7805)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [45][0/1]	Time 0.112 (0.112)	Data 0.107 (0.107)	Loss 0.2387 (0.2387)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.7776 (1.7776)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [46][0/1]	Time 0.120 (0.120)	Data 0.115 (0.115)	Loss 0.2199 (0.2199)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.7766 (1.7766)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [47][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.1645 (0.1645)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.7912 (1.7912)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [48][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.1931 (0.1931)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.8053 (1.8053)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [49][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.2023 (0.2023)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.8184 (1.8184)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [50][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.1253 (0.1253)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.8246 (1.8246)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [51][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.1501 (0.1501)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.142 (0.142)	Loss 1.8417 (1.8417)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [52][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.2617 (0.2617)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.8470 (1.8470)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [53][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.3178 (0.3178)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.8636 (1.8636)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [54][0/1]	Time 0.120 (0.120)	Data 0.115 (0.115)	Loss 0.1284 (0.1284)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.8620 (1.8620)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
50.66666793823242
Epoch: [55][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 0.1828 (0.1828)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.8532 (1.8532)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [56][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.2010 (0.2010)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8502 (1.8502)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [57][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.1906 (0.1906)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.8762 (1.8762)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [58][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.1561 (0.1561)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8938 (1.8938)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [59][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.2821 (0.2821)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.8948 (1.8948)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [60][0/1]	Time 0.109 (0.109)	Data 0.103 (0.103)	Loss 0.1237 (0.1237)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.9080 (1.9080)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
50.66666793823242
Epoch: [61][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.2231 (0.2231)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.9292 (1.9292)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
50.66666793823242
Epoch: [62][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.1025 (0.1025)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.118 (0.118)	Loss 1.9373 (1.9373)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
50.66666793823242
Epoch: [63][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss 0.2726 (0.2726)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9193 (1.9193)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [64][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0717 (0.0717)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9251 (1.9251)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [65][0/1]	Time 0.139 (0.139)	Data 0.134 (0.134)	Loss 0.1372 (0.1372)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9327 (1.9327)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [66][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1414 (0.1414)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.9492 (1.9492)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [67][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.2304 (0.2304)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9334 (1.9334)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [68][0/1]	Time 0.136 (0.136)	Data 0.131 (0.131)	Loss 0.2002 (0.2002)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.9461 (1.9461)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [69][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.1434 (0.1434)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.9566 (1.9566)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [70][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.4304 (0.4304)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.9275 (1.9275)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [71][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.0807 (0.0807)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.9363 (1.9363)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [72][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.2013 (0.2013)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.118 (0.118)	Loss 1.9388 (1.9388)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [73][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0866 (0.0866)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.9484 (1.9484)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [74][0/1]	Time 0.119 (0.119)	Data 0.110 (0.110)	Loss 0.1533 (0.1533)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.9502 (1.9502)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [75][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 0.2188 (0.2188)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.9744 (1.9744)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [76][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.1792 (0.1792)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 1.9970 (1.9970)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [77][0/1]	Time 0.107 (0.107)	Data 0.101 (0.101)	Loss 0.2138 (0.2138)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 1.9918 (1.9918)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [78][0/1]	Time 0.127 (0.127)	Data 0.118 (0.118)	Loss 0.2746 (0.2746)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 2.0080 (2.0080)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [79][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.0762 (0.0762)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.0164 (2.0164)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [80][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0911 (0.0911)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.0232 (2.0232)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [81][0/1]	Time 0.136 (0.136)	Data 0.130 (0.130)	Loss 0.2008 (0.2008)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 2.0272 (2.0272)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [82][0/1]	Time 0.116 (0.116)	Data 0.111 (0.111)	Loss 0.1056 (0.1056)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.0273 (2.0273)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [83][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.1026 (0.1026)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 2.0288 (2.0288)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
50.66666793823242
Epoch: [84][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.0677 (0.0677)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.0333 (2.0333)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
50.66666793823242
Epoch: [85][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 0.0932 (0.0932)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 2.0311 (2.0311)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
50.66666793823242
Epoch: [86][0/1]	Time 0.135 (0.135)	Data 0.127 (0.127)	Loss 0.1196 (0.1196)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.0388 (2.0388)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
50.66666793823242
Epoch: [87][0/1]	Time 0.111 (0.111)	Data 0.106 (0.106)	Loss 0.1940 (0.1940)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.0473 (2.0473)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
50.66666793823242
Epoch: [88][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.1127 (0.1127)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 2.0499 (2.0499)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
50.66666793823242
Epoch: [89][0/1]	Time 0.120 (0.120)	Data 0.111 (0.111)	Loss 0.0855 (0.0855)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.0597 (2.0597)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
50.66666793823242
Epoch: [90][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.1694 (0.1694)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 2.0691 (2.0691)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [91][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.1591 (0.1591)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 2.0681 (2.0681)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [92][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 0.1454 (0.1454)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.0639 (2.0639)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [93][0/1]	Time 0.118 (0.118)	Data 0.113 (0.113)	Loss 0.0729 (0.0729)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 2.0735 (2.0735)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [94][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0646 (0.0646)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2.0798 (2.0798)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [95][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.1210 (0.1210)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.0762 (2.0762)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [96][0/1]	Time 0.134 (0.134)	Data 0.126 (0.126)	Loss 0.0642 (0.0642)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 2.0865 (2.0865)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [97][0/1]	Time 0.134 (0.134)	Data 0.126 (0.126)	Loss 0.0619 (0.0619)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 2.0926 (2.0926)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
50.66666793823242
Epoch: [98][0/1]	Time 0.126 (0.126)	Data 0.118 (0.118)	Loss 0.1813 (0.1813)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 2.1089 (2.1089)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [99][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0737 (0.0737)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.1144 (2.1144)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [100][0/1]	Time 0.116 (0.116)	Data 0.106 (0.106)	Loss 0.3626 (0.3626)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 2.1124 (2.1124)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [101][0/1]	Time 0.118 (0.118)	Data 0.113 (0.113)	Loss 0.2007 (0.2007)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 2.1123 (2.1123)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [102][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.1181 (0.1181)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 2.1135 (2.1135)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [103][0/1]	Time 0.134 (0.134)	Data 0.126 (0.126)	Loss 0.1474 (0.1474)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.1151 (2.1151)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [104][0/1]	Time 0.124 (0.124)	Data 0.119 (0.119)	Loss 0.0827 (0.0827)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.1156 (2.1156)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [105][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.1588 (0.1588)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2.1157 (2.1157)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [106][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.2517 (0.2517)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 2.1157 (2.1157)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [107][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0731 (0.0731)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.1170 (2.1170)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [108][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.1105 (0.1105)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 2.1170 (2.1170)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [109][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0935 (0.0935)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.1182 (2.1182)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [110][0/1]	Time 0.132 (0.132)	Data 0.124 (0.124)	Loss 0.1120 (0.1120)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.142 (0.142)	Loss 2.1197 (2.1197)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [111][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 0.0761 (0.0761)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 2.1205 (2.1205)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [112][0/1]	Time 0.134 (0.134)	Data 0.128 (0.128)	Loss 0.0699 (0.0699)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 2.1212 (2.1212)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [113][0/1]	Time 0.109 (0.109)	Data 0.103 (0.103)	Loss 0.0485 (0.0485)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2.1220 (2.1220)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [114][0/1]	Time 0.116 (0.116)	Data 0.108 (0.108)	Loss 0.0633 (0.0633)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.1228 (2.1228)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [115][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.2365 (0.2365)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 2.1209 (2.1209)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [116][0/1]	Time 0.136 (0.136)	Data 0.127 (0.127)	Loss 0.1589 (0.1589)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 2.1206 (2.1206)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [117][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.1241 (0.1241)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 2.1215 (2.1215)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [118][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.1455 (0.1455)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 2.1228 (2.1228)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [119][0/1]	Time 0.142 (0.142)	Data 0.134 (0.134)	Loss 0.1015 (0.1015)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.1239 (2.1239)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [120][0/1]	Time 0.126 (0.126)	Data 0.118 (0.118)	Loss 0.0581 (0.0581)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.1242 (2.1242)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [121][0/1]	Time 0.137 (0.137)	Data 0.129 (0.129)	Loss 0.1933 (0.1933)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 2.1260 (2.1260)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [122][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.1132 (0.1132)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 2.1259 (2.1259)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [123][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0352 (0.0352)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.1265 (2.1265)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [124][0/1]	Time 0.111 (0.111)	Data 0.106 (0.106)	Loss 0.0777 (0.0777)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.1267 (2.1267)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [125][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.3030 (0.3030)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.1233 (2.1233)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [126][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.1020 (0.1020)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.1242 (2.1242)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [127][0/1]	Time 0.130 (0.130)	Data 0.122 (0.122)	Loss 0.0580 (0.0580)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 2.1248 (2.1248)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [128][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.2392 (0.2392)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 2.1216 (2.1216)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [129][0/1]	Time 0.115 (0.115)	Data 0.110 (0.110)	Loss 0.1854 (0.1854)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 2.1236 (2.1236)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [130][0/1]	Time 0.115 (0.115)	Data 0.110 (0.110)	Loss 0.1335 (0.1335)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.1245 (2.1245)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [131][0/1]	Time 0.124 (0.124)	Data 0.115 (0.115)	Loss 0.0435 (0.0435)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.1249 (2.1249)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [132][0/1]	Time 0.137 (0.137)	Data 0.131 (0.131)	Loss 0.0702 (0.0702)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 2.1256 (2.1256)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [133][0/1]	Time 0.112 (0.112)	Data 0.106 (0.106)	Loss 0.2852 (0.2852)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.1241 (2.1241)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [134][0/1]	Time 0.129 (0.129)	Data 0.120 (0.120)	Loss 0.1027 (0.1027)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 2.1244 (2.1244)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [135][0/1]	Time 0.120 (0.120)	Data 0.115 (0.115)	Loss 0.2029 (0.2029)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 2.1236 (2.1236)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [136][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.0909 (0.0909)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 2.1242 (2.1242)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [137][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.1017 (0.1017)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 2.1245 (2.1245)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [138][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.1178 (0.1178)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 2.1255 (2.1255)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [139][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.2201 (0.2201)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 2.1281 (2.1281)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [140][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0900 (0.0900)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 2.1272 (2.1272)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [141][0/1]	Time 0.119 (0.119)	Data 0.114 (0.114)	Loss 0.1392 (0.1392)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.1278 (2.1278)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [142][0/1]	Time 0.111 (0.111)	Data 0.106 (0.106)	Loss 0.0991 (0.0991)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.1288 (2.1288)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [143][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.0500 (0.0500)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 2.1293 (2.1293)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [144][0/1]	Time 0.136 (0.136)	Data 0.128 (0.128)	Loss 0.0607 (0.0607)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2.1294 (2.1294)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [145][0/1]	Time 0.124 (0.124)	Data 0.119 (0.119)	Loss 0.0585 (0.0585)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 2.1296 (2.1296)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [146][0/1]	Time 0.124 (0.124)	Data 0.119 (0.119)	Loss 0.2344 (0.2344)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.1299 (2.1299)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [147][0/1]	Time 0.117 (0.117)	Data 0.112 (0.112)	Loss 0.0965 (0.0965)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 2.1303 (2.1303)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [148][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0607 (0.0607)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.1311 (2.1311)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
Epoch: [149][0/1]	Time 0.119 (0.119)	Data 0.111 (0.111)	Loss 0.1532 (0.1532)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 2.1319 (2.1319)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
50.66666793823242
total time: 53.0712251663208
train loss:  [2.4869697093963623, 0.9858671426773071, 1.2403810024261475, 1.0423616170883179, 0.9985928535461426, 0.6863268613815308, 0.8398334383964539, 0.827419102191925, 0.8288982510566711, 0.5728215575218201, 0.6719315052032471, 0.5582167506217957, 0.6307544708251953, 0.664158821105957, 0.4382573664188385, 0.5037878155708313, 0.5466336607933044, 0.34357815980911255, 0.5506084561347961, 0.6535623073577881, 0.44567155838012695, 0.7403773069381714, 0.5215110778808594, 0.30007997155189514, 0.3798564076423645, 0.384225457906723, 0.5109308958053589, 0.21472246944904327, 0.30464091897010803, 0.44285592436790466, 0.21335569024085999, 0.31454700231552124, 0.29962021112442017, 0.27158939838409424, 0.2495664805173874, 0.35967856645584106, 0.35835593938827515, 0.23867753148078918, 0.33944058418273926, 0.29484573006629944, 0.20338201522827148, 0.24058572947978973, 0.26321083307266235, 0.2813088595867157, 0.1483686864376068, 0.238716721534729, 0.21993491053581238, 0.164454847574234, 0.19306261837482452, 0.20227201282978058, 0.12532123923301697, 0.15013833343982697, 0.2616737484931946, 0.31784576177597046, 0.1284499168395996, 0.1827552616596222, 0.20095351338386536, 0.19058600068092346, 0.15609793365001678, 0.2820821702480316, 0.12366995960474014, 0.22314539551734924, 0.10254273563623428, 0.27258965373039246, 0.07166056334972382, 0.13723386824131012, 0.14141854643821716, 0.23043422400951385, 0.2002391517162323, 0.1434078961610794, 0.43038874864578247, 0.0806581974029541, 0.20130375027656555, 0.08657362312078476, 0.1532706767320633, 0.21884307265281677, 0.17919890582561493, 0.2138126790523529, 0.2746005952358246, 0.07624159008264542, 0.09108336269855499, 0.20075221359729767, 0.10561637580394745, 0.10260377079248428, 0.06770451366901398, 0.09318764507770538, 0.11964650452136993, 0.19401276111602783, 0.11267714202404022, 0.0855133980512619, 0.16941003501415253, 0.15910720825195312, 0.14544975757598877, 0.07288231700658798, 0.06455521285533905, 0.12099428474903107, 0.06421351432800293, 0.06189422681927681, 0.1813095510005951, 0.07369089126586914, 0.3625882863998413, 0.20068323612213135, 0.11813603341579437, 0.14741520583629608, 0.0826999694108963, 0.1587987095117569, 0.25167909264564514, 0.07312850654125214, 0.1104835495352745, 0.09352278709411621, 0.11204066127538681, 0.07608332484960556, 0.06993086636066437, 0.048476122319698334, 0.06333748996257782, 0.23645010590553284, 0.15888915956020355, 0.12406058609485626, 0.14552569389343262, 0.1015312671661377, 0.058132052421569824, 0.1933223307132721, 0.11320982128381729, 0.035201407968997955, 0.07770834118127823, 0.30296745896339417, 0.1019691675901413, 0.05798895284533501, 0.23919221758842468, 0.18541677296161652, 0.13345925509929657, 0.043469954282045364, 0.07016720622777939, 0.2852180600166321, 0.10268883407115936, 0.20292501151561737, 0.0908801332116127, 0.10173492133617401, 0.11778914928436279, 0.22014422714710236, 0.08996877819299698, 0.13916859030723572, 0.099082350730896, 0.04995274543762207, 0.06072814390063286, 0.0585448257625103, 0.2344239503145218, 0.09645893424749374, 0.06074466556310654, 0.15318076312541962]
train acc:  [40.0, 60.0, 60.0, 80.0, 40.0, 100.0, 80.0, 60.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 80.0, 80.0, 80.0, 80.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
test loss:  [1.4229038953781128, 1.4153649806976318, 1.3772482872009277, 1.387809157371521, 1.3906021118164062, 1.405698299407959, 1.3811264038085938, 1.3911455869674683, 1.3964271545410156, 1.3988691568374634, 1.395387887954712, 1.4040080308914185, 1.4150664806365967, 1.4075311422348022, 1.425268530845642, 1.4367352724075317, 1.4248723983764648, 1.4348418712615967, 1.4408732652664185, 1.4450690746307373, 1.4509061574935913, 1.4518331289291382, 1.4574261903762817, 1.4609705209732056, 1.4811536073684692, 1.510995626449585, 1.494794487953186, 1.504517912864685, 1.514538288116455, 1.5421178340911865, 1.555681824684143, 1.5769476890563965, 1.5970604419708252, 1.5996919870376587, 1.6066747903823853, 1.640069603919983, 1.6592155694961548, 1.6776539087295532, 1.7037873268127441, 1.7201863527297974, 1.731394648551941, 1.7360656261444092, 1.7503647804260254, 1.7700445652008057, 1.7805253267288208, 1.7776339054107666, 1.7766245603561401, 1.7912096977233887, 1.805301308631897, 1.81843101978302, 1.8245999813079834, 1.8416564464569092, 1.847014307975769, 1.8635843992233276, 1.861981987953186, 1.853159785270691, 1.8501776456832886, 1.8762092590332031, 1.8938393592834473, 1.89482581615448, 1.907952904701233, 1.929198980331421, 1.9372831583023071, 1.9192798137664795, 1.9251145124435425, 1.9327234029769897, 1.9492037296295166, 1.933410882949829, 1.946143388748169, 1.95659601688385, 1.9275245666503906, 1.9362879991531372, 1.9387702941894531, 1.9483832120895386, 1.950249195098877, 1.9744032621383667, 1.996954321861267, 1.9918466806411743, 2.0079567432403564, 2.0163919925689697, 2.023164749145508, 2.027172803878784, 2.0272581577301025, 2.0287764072418213, 2.0332517623901367, 2.031135320663452, 2.0388283729553223, 2.0472805500030518, 2.0499463081359863, 2.0597445964813232, 2.0690534114837646, 2.068115472793579, 2.0638861656188965, 2.073507308959961, 2.0797698497772217, 2.076207399368286, 2.086494207382202, 2.092559814453125, 2.1089398860931396, 2.1143808364868164, 2.112440586090088, 2.1123239994049072, 2.1135330200195312, 2.1151351928710938, 2.1155970096588135, 2.1156861782073975, 2.11568546295166, 2.1170263290405273, 2.11704683303833, 2.118192672729492, 2.119720935821533, 2.120530843734741, 2.121152877807617, 2.1219966411590576, 2.1228322982788086, 2.1209194660186768, 2.1206159591674805, 2.1215171813964844, 2.1228325366973877, 2.123915672302246, 2.124203681945801, 2.1260433197021484, 2.1259429454803467, 2.126505136489868, 2.1267173290252686, 2.1232926845550537, 2.124237060546875, 2.124803304672241, 2.1216042041778564, 2.123565912246704, 2.1245336532592773, 2.1248722076416016, 2.1255905628204346, 2.124093532562256, 2.1244089603424072, 2.1235551834106445, 2.1242146492004395, 2.1244630813598633, 2.1254844665527344, 2.1280665397644043, 2.127150535583496, 2.127786636352539, 2.128840446472168, 2.1293160915374756, 2.1293723583221436, 2.1295671463012695, 2.1299431324005127, 2.1303157806396484, 2.1311275959014893, 2.1318533420562744]
test acc:  [40.0, 41.333335876464844, 42.66666793823242, 42.66666793823242, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 48.0, 49.333335876464844, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 50.66666793823242, 49.333335876464844, 49.333335876464844, 50.66666793823242, 49.333335876464844, 50.66666793823242, 49.333335876464844, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 48.0, 48.0, 49.333335876464844, 48.0, 48.0, 49.333335876464844, 49.333335876464844, 48.0, 49.333335876464844, 48.0, 49.333335876464844, 48.0, 48.0, 49.333335876464844, 48.0, 49.333335876464844, 48.0, 48.0, 48.0, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 49.333335876464844, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 46.66666793823242, 46.66666793823242, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 44.0, 44.0, 44.0, 44.0, 40.0, 40.0, 41.333335876464844, 41.333335876464844, 41.333335876464844, 41.333335876464844, 41.333335876464844, 41.333335876464844, 41.333335876464844, 41.333335876464844, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242, 42.66666793823242]
best prec: 50.66666793823242
=> loading checkpoint './save_final/cifarfs/subspace/5_way_5_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.6284
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 1.010 (1.010)	Data 0.082 (0.082)	Loss 3.9800 (3.9800)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.160 (0.160)	Loss 4.9430 (4.9430)	Prec@1 25.333 (25.333)
 * Prec@1 25.333
25.33333396911621
Epoch: [1][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 3.9262 (3.9262)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 3.7587 (3.7587)	Prec@1 30.667 (30.667)
 * Prec@1 30.667
30.666667938232422
Epoch: [2][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 1.9497 (1.9497)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 3.2294 (3.2294)	Prec@1 32.000 (32.000)
 * Prec@1 32.000
32.0
Epoch: [3][0/1]	Time 0.119 (0.119)	Data 0.113 (0.113)	Loss 1.0669 (1.0669)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 3.0052 (3.0052)	Prec@1 38.667 (38.667)
 * Prec@1 38.667
38.66666793823242
Epoch: [4][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.5651 (0.5651)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 2.9708 (2.9708)	Prec@1 34.667 (34.667)
 * Prec@1 34.667
38.66666793823242
Epoch: [5][0/1]	Time 0.131 (0.131)	Data 0.121 (0.121)	Loss 1.0677 (1.0677)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 2.5822 (2.5822)	Prec@1 41.333 (41.333)
 * Prec@1 41.333
41.333335876464844
Epoch: [6][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.7080 (0.7080)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 2.6117 (2.6117)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
41.333335876464844
Epoch: [7][0/1]	Time 0.135 (0.135)	Data 0.126 (0.126)	Loss 0.2625 (0.2625)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 2.6449 (2.6449)	Prec@1 40.000 (40.000)
 * Prec@1 40.000
41.333335876464844
Epoch: [8][0/1]	Time 0.136 (0.136)	Data 0.130 (0.130)	Loss 0.2828 (0.2828)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 2.4221 (2.4221)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
44.0
Epoch: [9][0/1]	Time 0.118 (0.118)	Data 0.113 (0.113)	Loss 0.2739 (0.2739)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 2.1859 (2.1859)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
48.0
Epoch: [10][0/1]	Time 0.124 (0.124)	Data 0.115 (0.115)	Loss 0.1959 (0.1959)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 2.2204 (2.2204)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
48.0
Epoch: [11][0/1]	Time 0.135 (0.135)	Data 0.129 (0.129)	Loss 0.1617 (0.1617)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 2.2073 (2.2073)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
48.0
Epoch: [12][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0940 (0.0940)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 2.1682 (2.1682)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
48.0
Epoch: [13][0/1]	Time 0.117 (0.117)	Data 0.108 (0.108)	Loss 0.6600 (0.6600)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 2.2138 (2.2138)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
48.0
Epoch: [14][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.0833 (0.0833)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 2.2574 (2.2574)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
48.0
Epoch: [15][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 0.7704 (0.7704)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.7429 (1.7429)	Prec@1 50.667 (50.667)
 * Prec@1 50.667
50.66666793823242
Epoch: [16][0/1]	Time 0.126 (0.126)	Data 0.117 (0.117)	Loss 0.2518 (0.2518)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.7482 (1.7482)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [17][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.2573 (0.2573)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.7586 (1.7586)	Prec@1 53.333 (53.333)
 * Prec@1 53.333
53.333335876464844
Epoch: [18][0/1]	Time 0.128 (0.128)	Data 0.120 (0.120)	Loss 0.2023 (0.2023)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 1.7028 (1.7028)	Prec@1 56.000 (56.000)
 * Prec@1 56.000
56.0
Epoch: [19][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 0.1303 (0.1303)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.7358 (1.7358)	Prec@1 56.000 (56.000)
 * Prec@1 56.000
56.0
Epoch: [20][0/1]	Time 0.119 (0.119)	Data 0.111 (0.111)	Loss 0.0679 (0.0679)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.165 (0.165)	Loss 1.7797 (1.7797)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
57.333335876464844
Epoch: [21][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.2857 (0.2857)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.8082 (1.8082)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
57.333335876464844
Epoch: [22][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0142 (0.0142)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.8119 (1.8119)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
57.333335876464844
Epoch: [23][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0231 (0.0231)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.7933 (1.7933)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
57.333335876464844
Epoch: [24][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.4307 (0.4307)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.6264 (1.6264)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
60.0
Epoch: [25][0/1]	Time 0.124 (0.124)	Data 0.118 (0.118)	Loss 0.9549 (0.9549)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.5189 (1.5189)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
61.333335876464844
Epoch: [26][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.4024 (0.4024)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.5047 (1.5047)	Prec@1 60.000 (60.000)
 * Prec@1 60.000
61.333335876464844
Epoch: [27][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.2457 (0.2457)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.3399 (1.3399)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
69.33333587646484
Epoch: [28][0/1]	Time 0.117 (0.117)	Data 0.108 (0.108)	Loss 0.0683 (0.0683)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.3239 (1.3239)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
69.33333587646484
Epoch: [29][0/1]	Time 0.137 (0.137)	Data 0.128 (0.128)	Loss 0.0266 (0.0266)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 1.3206 (1.3206)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
69.33333587646484
Epoch: [30][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 0.0811 (0.0811)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.3161 (1.3161)	Prec@1 70.667 (70.667)
 * Prec@1 70.667
70.66667175292969
Epoch: [31][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 0.0791 (0.0791)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.3292 (1.3292)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [32][0/1]	Time 0.128 (0.128)	Data 0.120 (0.120)	Loss 0.0493 (0.0493)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.3371 (1.3371)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [33][0/1]	Time 0.127 (0.127)	Data 0.121 (0.121)	Loss 0.0363 (0.0363)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.3182 (1.3182)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [34][0/1]	Time 0.123 (0.123)	Data 0.118 (0.118)	Loss 0.0378 (0.0378)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.3234 (1.3234)	Prec@1 70.667 (70.667)
 * Prec@1 70.667
70.66667175292969
Epoch: [35][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0573 (0.0573)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.3496 (1.3496)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [36][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0291 (0.0291)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.3421 (1.3421)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [37][0/1]	Time 0.117 (0.117)	Data 0.112 (0.112)	Loss 0.0258 (0.0258)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.3324 (1.3324)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [38][0/1]	Time 0.120 (0.120)	Data 0.114 (0.114)	Loss 0.0524 (0.0524)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.3034 (1.3034)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [39][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.1062 (0.1062)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.3603 (1.3603)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [40][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.1327 (0.1327)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.3760 (1.3760)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [41][0/1]	Time 0.117 (0.117)	Data 0.112 (0.112)	Loss 0.0429 (0.0429)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.3857 (1.3857)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [42][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.1345 (0.1345)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.4002 (1.4002)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [43][0/1]	Time 0.138 (0.138)	Data 0.130 (0.130)	Loss 0.0955 (0.0955)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4331 (1.4331)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
70.66667175292969
Epoch: [44][0/1]	Time 0.116 (0.116)	Data 0.111 (0.111)	Loss 0.0524 (0.0524)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4292 (1.4292)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [45][0/1]	Time 0.112 (0.112)	Data 0.107 (0.107)	Loss 0.5734 (0.5734)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.8051 (1.8051)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
70.66667175292969
Epoch: [46][0/1]	Time 0.122 (0.122)	Data 0.113 (0.113)	Loss 0.0472 (0.0472)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.8084 (1.8084)	Prec@1 56.000 (56.000)
 * Prec@1 56.000
70.66667175292969
Epoch: [47][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.0702 (0.0702)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.7763 (1.7763)	Prec@1 57.333 (57.333)
 * Prec@1 57.333
70.66667175292969
Epoch: [48][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0362 (0.0362)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.118 (0.118)	Loss 1.6978 (1.6978)	Prec@1 61.333 (61.333)
 * Prec@1 61.333
70.66667175292969
Epoch: [49][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.0319 (0.0319)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.145 (0.145)	Loss 1.6636 (1.6636)	Prec@1 62.667 (62.667)
 * Prec@1 62.667
70.66667175292969
Epoch: [50][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.2682 (0.2682)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4664 (1.4664)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [51][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0486 (0.0486)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4508 (1.4508)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [52][0/1]	Time 0.125 (0.125)	Data 0.118 (0.118)	Loss 0.1131 (0.1131)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.3656 (1.3656)	Prec@1 70.667 (70.667)
 * Prec@1 70.667
70.66667175292969
Epoch: [53][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0622 (0.0622)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.3839 (1.3839)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [54][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.0681 (0.0681)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.3737 (1.3737)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [55][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0893 (0.0893)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 1.3907 (1.3907)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [56][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss 0.0156 (0.0156)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 1.3900 (1.3900)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [57][0/1]	Time 0.131 (0.131)	Data 0.125 (0.125)	Loss 0.0265 (0.0265)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.3917 (1.3917)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [58][0/1]	Time 0.119 (0.119)	Data 0.114 (0.114)	Loss 0.0563 (0.0563)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.3891 (1.3891)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [59][0/1]	Time 0.116 (0.116)	Data 0.107 (0.107)	Loss 0.0200 (0.0200)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.3908 (1.3908)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [60][0/1]	Time 0.138 (0.138)	Data 0.132 (0.132)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.3962 (1.3962)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [61][0/1]	Time 0.110 (0.110)	Data 0.104 (0.104)	Loss 0.0161 (0.0161)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.3960 (1.3960)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [62][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 0.0140 (0.0140)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.3970 (1.3970)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [63][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0297 (0.0297)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.4009 (1.4009)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [64][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0489 (0.0489)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.157 (0.157)	Loss 1.3917 (1.3917)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [65][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0123 (0.0123)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.3969 (1.3969)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [66][0/1]	Time 0.131 (0.131)	Data 0.126 (0.126)	Loss 0.0467 (0.0467)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.4195 (1.4195)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [67][0/1]	Time 0.122 (0.122)	Data 0.115 (0.115)	Loss 0.0310 (0.0310)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.4068 (1.4068)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
70.66667175292969
Epoch: [68][0/1]	Time 0.130 (0.130)	Data 0.123 (0.123)	Loss 0.2729 (0.2729)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4334 (1.4334)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [69][0/1]	Time 0.127 (0.127)	Data 0.119 (0.119)	Loss 0.0997 (0.0997)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.4462 (1.4462)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [70][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.0463 (0.0463)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4402 (1.4402)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
70.66667175292969
Epoch: [71][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0564 (0.0564)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4716 (1.4716)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
70.66667175292969
Epoch: [72][0/1]	Time 0.125 (0.125)	Data 0.116 (0.116)	Loss 0.0788 (0.0788)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.4682 (1.4682)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
70.66667175292969
Epoch: [73][0/1]	Time 0.125 (0.125)	Data 0.118 (0.118)	Loss 0.0194 (0.0194)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.4469 (1.4469)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [74][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.0222 (0.0222)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.4356 (1.4356)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [75][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.0750 (0.0750)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 1.3971 (1.3971)	Prec@1 70.667 (70.667)
 * Prec@1 70.667
70.66667175292969
Epoch: [76][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1094 (0.1094)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4028 (1.4028)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
70.66667175292969
Epoch: [77][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.1418 (0.1418)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 1.3774 (1.3774)	Prec@1 72.000 (72.000)
 * Prec@1 72.000
72.0
Epoch: [78][0/1]	Time 0.150 (0.150)	Data 0.144 (0.144)	Loss 0.0571 (0.0571)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.3923 (1.3923)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [79][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.0237 (0.0237)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.3903 (1.3903)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [80][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0590 (0.0590)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.4141 (1.4141)	Prec@1 70.667 (70.667)
 * Prec@1 70.667
72.0
Epoch: [81][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0030 (0.0030)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4131 (1.4131)	Prec@1 70.667 (70.667)
 * Prec@1 70.667
72.0
Epoch: [82][0/1]	Time 0.117 (0.117)	Data 0.107 (0.107)	Loss 0.0174 (0.0174)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4149 (1.4149)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [83][0/1]	Time 0.108 (0.108)	Data 0.103 (0.103)	Loss 0.0308 (0.0308)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4216 (1.4216)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [84][0/1]	Time 0.110 (0.110)	Data 0.105 (0.105)	Loss 0.0771 (0.0771)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.4082 (1.4082)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [85][0/1]	Time 0.135 (0.135)	Data 0.126 (0.126)	Loss 0.0285 (0.0285)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.117 (0.117)	Loss 1.4107 (1.4107)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [86][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.1111 (0.1111)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4721 (1.4721)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
72.0
Epoch: [87][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.0334 (0.0334)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4640 (1.4640)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
72.0
Epoch: [88][0/1]	Time 0.123 (0.123)	Data 0.114 (0.114)	Loss 0.0210 (0.0210)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4799 (1.4799)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
72.0
Epoch: [89][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0466 (0.0466)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4995 (1.4995)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
72.0
Epoch: [90][0/1]	Time 0.117 (0.117)	Data 0.111 (0.111)	Loss 0.0143 (0.0143)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5012 (1.5012)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
72.0
Epoch: [91][0/1]	Time 0.116 (0.116)	Data 0.108 (0.108)	Loss 0.0591 (0.0591)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4890 (1.4890)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
72.0
Epoch: [92][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0297 (0.0297)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.154 (0.154)	Loss 1.4829 (1.4829)	Prec@1 65.333 (65.333)
 * Prec@1 65.333
72.0
Epoch: [93][0/1]	Time 0.113 (0.113)	Data 0.108 (0.108)	Loss 0.1893 (0.1893)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.4018 (1.4018)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
72.0
Epoch: [94][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0187 (0.0187)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.4032 (1.4032)	Prec@1 66.667 (66.667)
 * Prec@1 66.667
72.0
Epoch: [95][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.1774 (0.1774)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5066 (1.5066)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
72.0
Epoch: [96][0/1]	Time 0.136 (0.136)	Data 0.127 (0.127)	Loss 0.0165 (0.0165)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.4980 (1.4980)	Prec@1 64.000 (64.000)
 * Prec@1 64.000
72.0
Epoch: [97][0/1]	Time 0.109 (0.109)	Data 0.104 (0.104)	Loss 0.0931 (0.0931)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.5452 (1.5452)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [98][0/1]	Time 0.136 (0.136)	Data 0.130 (0.130)	Loss 0.0191 (0.0191)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.5414 (1.5414)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [99][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 0.0122 (0.0122)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.5382 (1.5382)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [100][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0298 (0.0298)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.5384 (1.5384)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [101][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.0390 (0.0390)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5364 (1.5364)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [102][0/1]	Time 0.118 (0.118)	Data 0.113 (0.113)	Loss 0.2055 (0.2055)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.5326 (1.5326)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [103][0/1]	Time 0.115 (0.115)	Data 0.110 (0.110)	Loss 0.0384 (0.0384)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.5304 (1.5304)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [104][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0626 (0.0626)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.5248 (1.5248)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [105][0/1]	Time 0.117 (0.117)	Data 0.108 (0.108)	Loss 0.1132 (0.1132)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.150 (0.150)	Loss 1.5222 (1.5222)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [106][0/1]	Time 0.122 (0.122)	Data 0.116 (0.116)	Loss 0.0188 (0.0188)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.119 (0.119)	Loss 1.5220 (1.5220)	Prec@1 69.333 (69.333)
 * Prec@1 69.333
72.0
Epoch: [107][0/1]	Time 0.162 (0.162)	Data 0.154 (0.154)	Loss 0.5651 (0.5651)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.5205 (1.5205)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [108][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0199 (0.0199)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 1.5205 (1.5205)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [109][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.0627 (0.0627)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.5193 (1.5193)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [110][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.3124 (0.3124)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.5218 (1.5218)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [111][0/1]	Time 0.126 (0.126)	Data 0.120 (0.120)	Loss 0.0156 (0.0156)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.120 (0.120)	Loss 1.5207 (1.5207)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [112][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0319 (0.0319)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5184 (1.5184)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [113][0/1]	Time 0.119 (0.119)	Data 0.111 (0.111)	Loss 0.0500 (0.0500)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.116 (0.116)	Loss 1.5199 (1.5199)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [114][0/1]	Time 0.126 (0.126)	Data 0.119 (0.119)	Loss 0.0073 (0.0073)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5200 (1.5200)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [115][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.1141 (0.1141)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.5138 (1.5138)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [116][0/1]	Time 0.113 (0.113)	Data 0.107 (0.107)	Loss 0.0087 (0.0087)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.5135 (1.5135)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [117][0/1]	Time 0.115 (0.115)	Data 0.110 (0.110)	Loss 0.1130 (0.1130)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.158 (0.158)	Loss 1.5017 (1.5017)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [118][0/1]	Time 0.116 (0.116)	Data 0.110 (0.110)	Loss 0.0486 (0.0486)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4991 (1.4991)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [119][0/1]	Time 0.119 (0.119)	Data 0.114 (0.114)	Loss 0.0166 (0.0166)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4987 (1.4987)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [120][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0501 (0.0501)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.4976 (1.4976)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [121][0/1]	Time 0.121 (0.121)	Data 0.115 (0.115)	Loss 0.2665 (0.2665)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.5086 (1.5086)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [122][0/1]	Time 0.115 (0.115)	Data 0.109 (0.109)	Loss 0.0565 (0.0565)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.5069 (1.5069)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [123][0/1]	Time 0.118 (0.118)	Data 0.109 (0.109)	Loss 0.0100 (0.0100)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.5068 (1.5068)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [124][0/1]	Time 0.130 (0.130)	Data 0.121 (0.121)	Loss 0.0506 (0.0506)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.118 (0.118)	Loss 1.5037 (1.5037)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [125][0/1]	Time 0.133 (0.133)	Data 0.127 (0.127)	Loss 0.0666 (0.0666)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4983 (1.4983)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [126][0/1]	Time 0.113 (0.113)	Data 0.108 (0.108)	Loss 0.0248 (0.0248)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.4992 (1.4992)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [127][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.0730 (0.0730)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.4996 (1.4996)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [128][0/1]	Time 0.123 (0.123)	Data 0.117 (0.117)	Loss 0.0269 (0.0269)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.4997 (1.4997)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [129][0/1]	Time 0.124 (0.124)	Data 0.115 (0.115)	Loss 0.0345 (0.0345)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.4969 (1.4969)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [130][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0609 (0.0609)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.4927 (1.4927)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [131][0/1]	Time 0.111 (0.111)	Data 0.105 (0.105)	Loss 0.0115 (0.0115)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4919 (1.4919)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [132][0/1]	Time 0.125 (0.125)	Data 0.116 (0.116)	Loss 0.0232 (0.0232)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.4907 (1.4907)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [133][0/1]	Time 0.132 (0.132)	Data 0.126 (0.126)	Loss 0.0521 (0.0521)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4931 (1.4931)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [134][0/1]	Time 0.118 (0.118)	Data 0.112 (0.112)	Loss 0.0269 (0.0269)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4935 (1.4935)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [135][0/1]	Time 0.125 (0.125)	Data 0.119 (0.119)	Loss 0.0187 (0.0187)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.4935 (1.4935)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [136][0/1]	Time 0.162 (0.162)	Data 0.155 (0.155)	Loss 0.0151 (0.0151)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4933 (1.4933)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [137][0/1]	Time 0.129 (0.129)	Data 0.123 (0.123)	Loss 0.0085 (0.0085)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.4931 (1.4931)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [138][0/1]	Time 0.117 (0.117)	Data 0.108 (0.108)	Loss 0.0819 (0.0819)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4958 (1.4958)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [139][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.2967 (0.2967)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.4965 (1.4965)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [140][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 0.0028 (0.0028)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.4964 (1.4964)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [141][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.0085 (0.0085)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4960 (1.4960)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [142][0/1]	Time 0.110 (0.110)	Data 0.104 (0.104)	Loss 0.0085 (0.0085)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.4956 (1.4956)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [143][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.0210 (0.0210)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.4960 (1.4960)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [144][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.0410 (0.0410)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.4920 (1.4920)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [145][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.0785 (0.0785)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.4889 (1.4889)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [146][0/1]	Time 0.143 (0.143)	Data 0.137 (0.137)	Loss 0.0467 (0.0467)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.4896 (1.4896)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [147][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.0269 (0.0269)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.4890 (1.4890)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [148][0/1]	Time 0.114 (0.114)	Data 0.108 (0.108)	Loss 0.0308 (0.0308)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.4898 (1.4898)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
Epoch: [149][0/1]	Time 0.109 (0.109)	Data 0.103 (0.103)	Loss 0.0467 (0.0467)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.4892 (1.4892)	Prec@1 68.000 (68.000)
 * Prec@1 68.000
72.0
total time: 53.940067529678345
train loss:  [3.979982852935791, 3.9262452125549316, 1.9496972560882568, 1.0668967962265015, 0.5650917887687683, 1.0676616430282593, 0.7079607844352722, 0.26246577501296997, 0.28282731771469116, 0.2739083468914032, 0.1958645135164261, 0.1616993248462677, 0.09396390616893768, 0.6599793434143066, 0.08330197632312775, 0.7704218029975891, 0.251765638589859, 0.25734177231788635, 0.2023434191942215, 0.13034644722938538, 0.06793179363012314, 0.28568345308303833, 0.014237498864531517, 0.023093556985259056, 0.4307211935520172, 0.9549480676651001, 0.40241941809654236, 0.24568328261375427, 0.06825681030750275, 0.02660684660077095, 0.081085205078125, 0.07912325859069824, 0.04930849000811577, 0.03630561754107475, 0.037790585309267044, 0.057315826416015625, 0.02907080575823784, 0.025798320770263672, 0.05244021490216255, 0.10619454085826874, 0.13269944489002228, 0.04293789714574814, 0.13448867201805115, 0.09545545279979706, 0.0523921474814415, 0.57342529296875, 0.047177840024232864, 0.07017707824707031, 0.03618926927447319, 0.03189287334680557, 0.26820701360702515, 0.04855666309595108, 0.11314363777637482, 0.062216948717832565, 0.06814269721508026, 0.08928094059228897, 0.015606498345732689, 0.026537800207734108, 0.05633578449487686, 0.01998271979391575, 0.017746543511748314, 0.01612377166748047, 0.014003085903823376, 0.02971353568136692, 0.048855971544981, 0.012345695868134499, 0.04673142358660698, 0.030992794781923294, 0.27289876341819763, 0.09967084228992462, 0.0462794303894043, 0.0563960075378418, 0.07878856360912323, 0.019400786608457565, 0.022189998999238014, 0.07497751712799072, 0.10942868888378143, 0.14177346229553223, 0.057056427001953125, 0.023671627044677734, 0.05898027494549751, 0.00299072265625, 0.017432594671845436, 0.030808258801698685, 0.07714829593896866, 0.028540706261992455, 0.11112260818481445, 0.033373020589351654, 0.020986175164580345, 0.04657282680273056, 0.014279079623520374, 0.059061385691165924, 0.029698658734560013, 0.18933066725730896, 0.018687058240175247, 0.17744484543800354, 0.016454219818115234, 0.09312000125646591, 0.01909027062356472, 0.012226248160004616, 0.02980217896401882, 0.03901233524084091, 0.20549540221691132, 0.03836078569293022, 0.0626031905412674, 0.11319772899150848, 0.018793772906064987, 0.5650684833526611, 0.019894981756806374, 0.0627264529466629, 0.3124498724937439, 0.01558837853372097, 0.03193836286664009, 0.04999718815088272, 0.007282733917236328, 0.11410851776599884, 0.008717060089111328, 0.1130046397447586, 0.048560332506895065, 0.016568755730986595, 0.05011286586523056, 0.266463041305542, 0.05646882206201553, 0.009966278448700905, 0.050566721707582474, 0.06663675606250763, 0.024785708636045456, 0.07297482341527939, 0.02691679075360298, 0.03448634222149849, 0.060873936861753464, 0.011536503210663795, 0.023155594244599342, 0.05210380628705025, 0.026912594214081764, 0.01870284043252468, 0.015109395608305931, 0.008485603146255016, 0.08194756507873535, 0.2967156767845154, 0.002755498979240656, 0.00848379172384739, 0.008471012115478516, 0.02095642127096653, 0.04098472744226456, 0.07846470177173615, 0.046739768236875534, 0.02693328820168972, 0.030790995806455612, 0.0467066764831543]
train acc:  [20.0, 60.0, 60.0, 80.0, 80.0, 60.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 80.0, 60.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
test loss:  [4.942961692810059, 3.7586677074432373, 3.229447841644287, 3.00518536567688, 2.9708316326141357, 2.582242727279663, 2.6117491722106934, 2.644937753677368, 2.4220893383026123, 2.1858789920806885, 2.2204041481018066, 2.207329750061035, 2.168165922164917, 2.213782548904419, 2.2573843002319336, 1.7428758144378662, 1.7482472658157349, 1.758634090423584, 1.7028292417526245, 1.7357900142669678, 1.7797322273254395, 1.8081943988800049, 1.8118951320648193, 1.7932604551315308, 1.6264487504959106, 1.5188848972320557, 1.5047110319137573, 1.339853286743164, 1.3238626718521118, 1.3206270933151245, 1.3161424398422241, 1.3292491436004639, 1.3371484279632568, 1.318215012550354, 1.3233660459518433, 1.3495906591415405, 1.3420672416687012, 1.3323663473129272, 1.3034385442733765, 1.360258936882019, 1.3760255575180054, 1.3856823444366455, 1.4001805782318115, 1.4330545663833618, 1.4292495250701904, 1.8051369190216064, 1.808406949043274, 1.7763018608093262, 1.6978473663330078, 1.663575291633606, 1.4663926362991333, 1.4508057832717896, 1.365605115890503, 1.383851408958435, 1.3736908435821533, 1.3907432556152344, 1.3899998664855957, 1.3916586637496948, 1.3890968561172485, 1.390757441520691, 1.39622962474823, 1.3959500789642334, 1.3969910144805908, 1.4008737802505493, 1.3917027711868286, 1.3969011306762695, 1.419481873512268, 1.4068108797073364, 1.4334008693695068, 1.4462436437606812, 1.440195918083191, 1.4716451168060303, 1.468190312385559, 1.446882963180542, 1.435600757598877, 1.3970612287521362, 1.4027873277664185, 1.3773630857467651, 1.3922654390335083, 1.390300989151001, 1.4141087532043457, 1.4131251573562622, 1.4148602485656738, 1.4216216802597046, 1.4082010984420776, 1.410738468170166, 1.4720749855041504, 1.4640499353408813, 1.4798955917358398, 1.499453067779541, 1.5012184381484985, 1.4889647960662842, 1.4828928709030151, 1.4017775058746338, 1.4031790494918823, 1.5065579414367676, 1.4979708194732666, 1.545181393623352, 1.541443109512329, 1.5381861925125122, 1.5383630990982056, 1.5363705158233643, 1.5326048135757446, 1.5304371118545532, 1.5247973203659058, 1.5222219228744507, 1.5220305919647217, 1.52046537399292, 1.5205090045928955, 1.5193147659301758, 1.521849513053894, 1.5207479000091553, 1.5183719396591187, 1.5199167728424072, 1.5200064182281494, 1.5138424634933472, 1.513457179069519, 1.5017493963241577, 1.4991425275802612, 1.498659372329712, 1.4976001977920532, 1.5086464881896973, 1.5069000720977783, 1.5067545175552368, 1.5036777257919312, 1.4983288049697876, 1.4992399215698242, 1.4995959997177124, 1.4997037649154663, 1.496915340423584, 1.4926550388336182, 1.4919004440307617, 1.490699291229248, 1.4930578470230103, 1.4935482740402222, 1.4934874773025513, 1.4933416843414307, 1.4930527210235596, 1.4957690238952637, 1.496543049812317, 1.496408462524414, 1.4960463047027588, 1.495599389076233, 1.4960342645645142, 1.4919966459274292, 1.4888607263565063, 1.4896382093429565, 1.488968849182129, 1.4898042678833008, 1.4891711473464966]
test acc:  [25.33333396911621, 30.666667938232422, 32.0, 38.66666793823242, 34.66666793823242, 41.333335876464844, 40.0, 40.0, 44.0, 48.0, 48.0, 45.333335876464844, 45.333335876464844, 48.0, 45.333335876464844, 50.66666793823242, 53.333335876464844, 53.333335876464844, 56.0, 56.0, 57.333335876464844, 57.333335876464844, 57.333335876464844, 57.333335876464844, 60.0, 61.333335876464844, 60.0, 69.33333587646484, 68.0, 69.33333587646484, 70.66667175292969, 68.0, 66.66667175292969, 66.66667175292969, 70.66667175292969, 68.0, 69.33333587646484, 69.33333587646484, 69.33333587646484, 66.66667175292969, 65.33333587646484, 66.66667175292969, 65.33333587646484, 64.0, 65.33333587646484, 57.333335876464844, 56.0, 57.333335876464844, 61.333335876464844, 62.66666793823242, 65.33333587646484, 65.33333587646484, 70.66667175292969, 68.0, 69.33333587646484, 66.66667175292969, 66.66667175292969, 68.0, 68.0, 68.0, 68.0, 68.0, 66.66667175292969, 66.66667175292969, 66.66667175292969, 66.66667175292969, 65.33333587646484, 65.33333587646484, 68.0, 66.66667175292969, 66.66667175292969, 64.0, 68.0, 69.33333587646484, 69.33333587646484, 70.66667175292969, 69.33333587646484, 72.0, 69.33333587646484, 69.33333587646484, 70.66667175292969, 70.66667175292969, 69.33333587646484, 68.0, 68.0, 68.0, 65.33333587646484, 66.66667175292969, 65.33333587646484, 65.33333587646484, 65.33333587646484, 65.33333587646484, 65.33333587646484, 66.66667175292969, 66.66667175292969, 64.0, 64.0, 68.0, 69.33333587646484, 69.33333587646484, 69.33333587646484, 69.33333587646484, 69.33333587646484, 69.33333587646484, 69.33333587646484, 69.33333587646484, 69.33333587646484, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0]
best prec: 72.0
=> loading checkpoint './save_final/cifarfs/subspace/5_way_1_shot/checkpoint.pth.tar'
from  19951
best_prec: 0.4524
=> loaded checkpoint 'False' (epoch 19951)
./convnet_5way_1shot.mat
Files already downloaded and verified
Files already downloaded and verified
Train: (19951, 150)
Epoch: [0][0/1]	Time 1.009 (1.009)	Data 0.081 (0.081)	Loss 3.9800 (3.9800)	Prec@1 20.000 (20.000)
Test: [0/1]	Time 0.152 (0.152)	Loss 1.6774 (1.6774)	Prec@1 32.000 (32.000)
 * Prec@1 32.000
32.0
Epoch: [1][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss 1.2099 (1.2099)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.6085 (1.6085)	Prec@1 37.333 (37.333)
 * Prec@1 37.333
37.333335876464844
Epoch: [2][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 1.5393 (1.5393)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.4720 (1.4720)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
45.333335876464844
Epoch: [3][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 1.2706 (1.2706)	Prec@1 40.000 (40.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4344 (1.4344)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
46.66666793823242
Epoch: [4][0/1]	Time 0.140 (0.140)	Data 0.133 (0.133)	Loss 1.0377 (1.0377)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.3772 (1.3772)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
46.66666793823242
Epoch: [5][0/1]	Time 0.125 (0.125)	Data 0.118 (0.118)	Loss 0.9601 (0.9601)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.3587 (1.3587)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
46.66666793823242
Epoch: [6][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 0.8597 (0.8597)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.3221 (1.3221)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
48.0
Epoch: [7][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.9520 (0.9520)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.3073 (1.3073)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [8][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.8836 (0.8836)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.2527 (1.2527)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [9][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.6788 (0.6788)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.2329 (1.2329)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [10][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 0.8706 (0.8706)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.2027 (1.2027)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [11][0/1]	Time 0.118 (0.118)	Data 0.108 (0.108)	Loss 0.7395 (0.7395)	Prec@1 60.000 (60.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.1847 (1.1847)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [12][0/1]	Time 0.122 (0.122)	Data 0.112 (0.112)	Loss 0.6277 (0.6277)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.1806 (1.1806)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [13][0/1]	Time 0.134 (0.134)	Data 0.127 (0.127)	Loss 0.6227 (0.6227)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.1803 (1.1803)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [14][0/1]	Time 0.137 (0.137)	Data 0.130 (0.130)	Loss 0.5702 (0.5702)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.1735 (1.1735)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [15][0/1]	Time 0.129 (0.129)	Data 0.119 (0.119)	Loss 0.6454 (0.6454)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.1712 (1.1712)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [16][0/1]	Time 0.117 (0.117)	Data 0.107 (0.107)	Loss 0.6759 (0.6759)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.1760 (1.1760)	Prec@1 49.333 (49.333)
 * Prec@1 49.333
49.333335876464844
Epoch: [17][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.4263 (0.4263)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.1873 (1.1873)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [18][0/1]	Time 0.132 (0.132)	Data 0.122 (0.122)	Loss 0.3581 (0.3581)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.2012 (1.2012)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
49.333335876464844
Epoch: [19][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss 0.4708 (0.4708)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.2103 (1.2103)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [20][0/1]	Time 0.110 (0.110)	Data 0.103 (0.103)	Loss 0.5513 (0.5513)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.149 (0.149)	Loss 1.1931 (1.1931)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [21][0/1]	Time 0.125 (0.125)	Data 0.118 (0.118)	Loss 0.3975 (0.3975)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.2117 (1.2117)	Prec@1 42.667 (42.667)
 * Prec@1 42.667
49.333335876464844
Epoch: [22][0/1]	Time 0.111 (0.111)	Data 0.104 (0.104)	Loss 0.4068 (0.4068)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.2062 (1.2062)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [23][0/1]	Time 0.116 (0.116)	Data 0.108 (0.108)	Loss 0.3119 (0.3119)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.2004 (1.2004)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [24][0/1]	Time 0.124 (0.124)	Data 0.112 (0.112)	Loss 0.4601 (0.4601)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.154 (0.154)	Loss 1.2033 (1.2033)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [25][0/1]	Time 0.138 (0.138)	Data 0.130 (0.130)	Loss 0.4472 (0.4472)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.2124 (1.2124)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [26][0/1]	Time 0.112 (0.112)	Data 0.104 (0.104)	Loss 0.2980 (0.2980)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.151 (0.151)	Loss 1.2083 (1.2083)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [27][0/1]	Time 0.121 (0.121)	Data 0.111 (0.111)	Loss 0.2246 (0.2246)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.2251 (1.2251)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [28][0/1]	Time 0.110 (0.110)	Data 0.103 (0.103)	Loss 0.3379 (0.3379)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.2179 (1.2179)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [29][0/1]	Time 0.117 (0.117)	Data 0.106 (0.106)	Loss 0.3814 (0.3814)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 1.2348 (1.2348)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [30][0/1]	Time 0.126 (0.126)	Data 0.116 (0.116)	Loss 0.1749 (0.1749)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.145 (0.145)	Loss 1.2445 (1.2445)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [31][0/1]	Time 0.122 (0.122)	Data 0.114 (0.114)	Loss 0.3916 (0.3916)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.144 (0.144)	Loss 1.2469 (1.2469)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [32][0/1]	Time 0.121 (0.121)	Data 0.113 (0.113)	Loss 0.2440 (0.2440)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.2557 (1.2557)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [33][0/1]	Time 0.110 (0.110)	Data 0.103 (0.103)	Loss 0.2245 (0.2245)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.143 (0.143)	Loss 1.2561 (1.2561)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [34][0/1]	Time 0.128 (0.128)	Data 0.121 (0.121)	Loss 0.2064 (0.2064)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.141 (0.141)	Loss 1.2581 (1.2581)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [35][0/1]	Time 0.119 (0.119)	Data 0.111 (0.111)	Loss 0.4665 (0.4665)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.2528 (1.2528)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [36][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.3965 (0.3965)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.2553 (1.2553)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [37][0/1]	Time 0.136 (0.136)	Data 0.128 (0.128)	Loss 0.1959 (0.1959)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.111 (0.111)	Loss 1.2655 (1.2655)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [38][0/1]	Time 0.132 (0.132)	Data 0.123 (0.123)	Loss 0.2275 (0.2275)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 1.2628 (1.2628)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [39][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.2019 (0.2019)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.2759 (1.2759)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [40][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss 0.3700 (0.3700)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.2757 (1.2757)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [41][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 0.3474 (0.3474)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.2850 (1.2850)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [42][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 0.2823 (0.2823)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.2913 (1.2913)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [43][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.2448 (0.2448)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.145 (0.145)	Loss 1.2955 (1.2955)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [44][0/1]	Time 0.116 (0.116)	Data 0.108 (0.108)	Loss 0.1861 (0.1861)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 1.2920 (1.2920)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [45][0/1]	Time 0.118 (0.118)	Data 0.108 (0.108)	Loss 0.1695 (0.1695)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.3055 (1.3055)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [46][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.2433 (0.2433)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.154 (0.154)	Loss 1.3058 (1.3058)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [47][0/1]	Time 0.120 (0.120)	Data 0.108 (0.108)	Loss 0.1708 (0.1708)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.114 (0.114)	Loss 1.3163 (1.3163)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [48][0/1]	Time 0.124 (0.124)	Data 0.116 (0.116)	Loss 0.2273 (0.2273)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.111 (0.111)	Loss 1.3222 (1.3222)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [49][0/1]	Time 0.128 (0.128)	Data 0.117 (0.117)	Loss 0.2250 (0.2250)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.148 (0.148)	Loss 1.3218 (1.3218)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [50][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.1525 (0.1525)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.3340 (1.3340)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [51][0/1]	Time 0.108 (0.108)	Data 0.100 (0.100)	Loss 0.1320 (0.1320)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.121 (0.121)	Loss 1.3401 (1.3401)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [52][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1317 (0.1317)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.154 (0.154)	Loss 1.3489 (1.3489)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [53][0/1]	Time 0.133 (0.133)	Data 0.124 (0.124)	Loss 0.3326 (0.3326)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.3380 (1.3380)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [54][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.1240 (0.1240)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.3481 (1.3481)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [55][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.1175 (0.1175)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.3619 (1.3619)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [56][0/1]	Time 0.125 (0.125)	Data 0.117 (0.117)	Loss 0.2280 (0.2280)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.3790 (1.3790)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [57][0/1]	Time 0.125 (0.125)	Data 0.116 (0.116)	Loss 0.0637 (0.0637)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.3806 (1.3806)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [58][0/1]	Time 0.135 (0.135)	Data 0.127 (0.127)	Loss 0.1636 (0.1636)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.3835 (1.3835)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [59][0/1]	Time 0.117 (0.117)	Data 0.109 (0.109)	Loss 0.2267 (0.2267)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.3855 (1.3855)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [60][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.1157 (0.1157)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.3873 (1.3873)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [61][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.3023 (0.3023)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.3916 (1.3916)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [62][0/1]	Time 0.138 (0.138)	Data 0.128 (0.128)	Loss 0.0887 (0.0887)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.3930 (1.3930)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [63][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1866 (0.1866)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.3910 (1.3910)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [64][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1786 (0.1786)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.3962 (1.3962)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [65][0/1]	Time 0.116 (0.116)	Data 0.108 (0.108)	Loss 0.0935 (0.0935)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.4070 (1.4070)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [66][0/1]	Time 0.118 (0.118)	Data 0.106 (0.106)	Loss 0.2933 (0.2933)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.4092 (1.4092)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [67][0/1]	Time 0.128 (0.128)	Data 0.118 (0.118)	Loss 0.0827 (0.0827)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.139 (0.139)	Loss 1.4137 (1.4137)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [68][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1416 (0.1416)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.4227 (1.4227)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [69][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.2588 (0.2588)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.4281 (1.4281)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [70][0/1]	Time 0.112 (0.112)	Data 0.105 (0.105)	Loss 0.1008 (0.1008)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.4366 (1.4366)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [71][0/1]	Time 0.113 (0.113)	Data 0.105 (0.105)	Loss 0.1609 (0.1609)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 1.4344 (1.4344)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [72][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.1514 (0.1514)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4490 (1.4490)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [73][0/1]	Time 0.126 (0.126)	Data 0.118 (0.118)	Loss 0.0456 (0.0456)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.4511 (1.4511)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [74][0/1]	Time 0.125 (0.125)	Data 0.115 (0.115)	Loss 0.2058 (0.2058)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.4493 (1.4493)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [75][0/1]	Time 0.132 (0.132)	Data 0.125 (0.125)	Loss 0.2791 (0.2791)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.118 (0.118)	Loss 1.4378 (1.4378)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [76][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.1811 (0.1811)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.4395 (1.4395)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [77][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1177 (0.1177)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.4451 (1.4451)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [78][0/1]	Time 0.121 (0.121)	Data 0.109 (0.109)	Loss 0.2124 (0.2124)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.4480 (1.4480)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [79][0/1]	Time 0.123 (0.123)	Data 0.111 (0.111)	Loss 0.0667 (0.0667)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.4562 (1.4562)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [80][0/1]	Time 0.122 (0.122)	Data 0.112 (0.112)	Loss 0.1213 (0.1213)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.4641 (1.4641)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [81][0/1]	Time 0.115 (0.115)	Data 0.106 (0.106)	Loss 0.1378 (0.1378)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.4771 (1.4771)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [82][0/1]	Time 0.171 (0.171)	Data 0.160 (0.160)	Loss 0.1180 (0.1180)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.4817 (1.4817)	Prec@1 48.000 (48.000)
 * Prec@1 48.000
49.333335876464844
Epoch: [83][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.1290 (0.1290)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.4858 (1.4858)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [84][0/1]	Time 0.120 (0.120)	Data 0.111 (0.111)	Loss 0.0808 (0.0808)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.4938 (1.4938)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [85][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0452 (0.0452)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.4980 (1.4980)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [86][0/1]	Time 0.116 (0.116)	Data 0.106 (0.106)	Loss 0.0703 (0.0703)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.5010 (1.5010)	Prec@1 45.333 (45.333)
 * Prec@1 45.333
49.333335876464844
Epoch: [87][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.0987 (0.0987)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.132 (0.132)	Loss 1.5097 (1.5097)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [88][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0772 (0.0772)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5192 (1.5192)	Prec@1 44.000 (44.000)
 * Prec@1 44.000
49.333335876464844
Epoch: [89][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.1408 (0.1408)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5069 (1.5069)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [90][0/1]	Time 0.134 (0.134)	Data 0.127 (0.127)	Loss 0.0832 (0.0832)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5113 (1.5113)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [91][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.2139 (0.2139)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.5154 (1.5154)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [92][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1508 (0.1508)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5192 (1.5192)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [93][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.0552 (0.0552)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.5229 (1.5229)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [94][0/1]	Time 0.139 (0.139)	Data 0.129 (0.129)	Loss 0.1515 (0.1515)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5190 (1.5190)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [95][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0799 (0.0799)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5276 (1.5276)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [96][0/1]	Time 0.119 (0.119)	Data 0.111 (0.111)	Loss 0.0751 (0.0751)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.124 (0.124)	Loss 1.5278 (1.5278)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [97][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.0697 (0.0697)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.5354 (1.5354)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [98][0/1]	Time 0.147 (0.147)	Data 0.136 (0.136)	Loss 0.0476 (0.0476)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.146 (0.146)	Loss 1.5421 (1.5421)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [99][0/1]	Time 0.126 (0.126)	Data 0.118 (0.118)	Loss 0.1034 (0.1034)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.5535 (1.5535)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [100][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.1427 (0.1427)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.5546 (1.5546)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [101][0/1]	Time 0.134 (0.134)	Data 0.126 (0.126)	Loss 0.0906 (0.0906)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.5555 (1.5555)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [102][0/1]	Time 0.116 (0.116)	Data 0.109 (0.109)	Loss 0.2208 (0.2208)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5550 (1.5550)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [103][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.0529 (0.0529)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.5552 (1.5552)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [104][0/1]	Time 0.119 (0.119)	Data 0.110 (0.110)	Loss 0.1051 (0.1051)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5557 (1.5557)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [105][0/1]	Time 0.141 (0.141)	Data 0.133 (0.133)	Loss 0.1951 (0.1951)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5548 (1.5548)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [106][0/1]	Time 0.121 (0.121)	Data 0.114 (0.114)	Loss 0.1491 (0.1491)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5551 (1.5551)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [107][0/1]	Time 0.120 (0.120)	Data 0.113 (0.113)	Loss 0.0369 (0.0369)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5554 (1.5554)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [108][0/1]	Time 0.113 (0.113)	Data 0.105 (0.105)	Loss 0.0732 (0.0732)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5557 (1.5557)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [109][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.0888 (0.0888)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.5559 (1.5559)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [110][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.0432 (0.0432)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.133 (0.133)	Loss 1.5562 (1.5562)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [111][0/1]	Time 0.136 (0.136)	Data 0.128 (0.128)	Loss 0.0371 (0.0371)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.147 (0.147)	Loss 1.5565 (1.5565)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [112][0/1]	Time 0.123 (0.123)	Data 0.116 (0.116)	Loss 0.1302 (0.1302)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5585 (1.5585)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [113][0/1]	Time 0.130 (0.130)	Data 0.123 (0.123)	Loss 0.1046 (0.1046)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.153 (0.153)	Loss 1.5579 (1.5579)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [114][0/1]	Time 0.120 (0.120)	Data 0.112 (0.112)	Loss 0.1438 (0.1438)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.5610 (1.5610)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [115][0/1]	Time 0.124 (0.124)	Data 0.117 (0.117)	Loss 0.1221 (0.1221)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5614 (1.5614)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [116][0/1]	Time 0.108 (0.108)	Data 0.101 (0.101)	Loss 0.0936 (0.0936)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5621 (1.5621)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [117][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.0493 (0.0493)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5626 (1.5626)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [118][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0872 (0.0872)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.5636 (1.5636)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [119][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.2222 (0.2222)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.129 (0.129)	Loss 1.5639 (1.5639)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [120][0/1]	Time 0.134 (0.134)	Data 0.124 (0.124)	Loss 0.0796 (0.0796)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.122 (0.122)	Loss 1.5644 (1.5644)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [121][0/1]	Time 0.109 (0.109)	Data 0.102 (0.102)	Loss 0.1662 (0.1662)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.137 (0.137)	Loss 1.5639 (1.5639)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [122][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.1699 (0.1699)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.140 (0.140)	Loss 1.5646 (1.5646)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [123][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0626 (0.0626)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5651 (1.5651)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [124][0/1]	Time 0.125 (0.125)	Data 0.118 (0.118)	Loss 0.0840 (0.0840)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.5654 (1.5654)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [125][0/1]	Time 0.123 (0.123)	Data 0.113 (0.113)	Loss 0.0698 (0.0698)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.138 (0.138)	Loss 1.5665 (1.5665)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [126][0/1]	Time 0.118 (0.118)	Data 0.111 (0.111)	Loss 0.2442 (0.2442)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5676 (1.5676)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [127][0/1]	Time 0.127 (0.127)	Data 0.120 (0.120)	Loss 0.2181 (0.2181)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.135 (0.135)	Loss 1.5666 (1.5666)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [128][0/1]	Time 0.130 (0.130)	Data 0.119 (0.119)	Loss 0.1629 (0.1629)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5669 (1.5669)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [129][0/1]	Time 0.122 (0.122)	Data 0.113 (0.113)	Loss 0.0788 (0.0788)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5671 (1.5671)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [130][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.3102 (0.3102)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5665 (1.5665)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [131][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0477 (0.0477)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5675 (1.5675)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [132][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.1944 (0.1944)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.113 (0.113)	Loss 1.5689 (1.5689)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [133][0/1]	Time 0.114 (0.114)	Data 0.106 (0.106)	Loss 0.2449 (0.2449)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.134 (0.134)	Loss 1.5680 (1.5680)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [134][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.1874 (0.1874)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5700 (1.5700)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [135][0/1]	Time 0.122 (0.122)	Data 0.112 (0.112)	Loss 0.0745 (0.0745)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.125 (0.125)	Loss 1.5699 (1.5699)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [136][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.1254 (0.1254)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5716 (1.5716)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [137][0/1]	Time 0.120 (0.120)	Data 0.110 (0.110)	Loss 0.0355 (0.0355)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5721 (1.5721)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [138][0/1]	Time 0.126 (0.126)	Data 0.115 (0.115)	Loss 0.0485 (0.0485)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.127 (0.127)	Loss 1.5726 (1.5726)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [139][0/1]	Time 0.114 (0.114)	Data 0.107 (0.107)	Loss 0.4896 (0.4896)	Prec@1 80.000 (80.000)
Test: [0/1]	Time 0.130 (0.130)	Loss 1.5724 (1.5724)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [140][0/1]	Time 0.125 (0.125)	Data 0.115 (0.115)	Loss 0.0309 (0.0309)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.157 (0.157)	Loss 1.5727 (1.5727)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [141][0/1]	Time 0.119 (0.119)	Data 0.112 (0.112)	Loss 0.1036 (0.1036)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.115 (0.115)	Loss 1.5733 (1.5733)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [142][0/1]	Time 0.117 (0.117)	Data 0.110 (0.110)	Loss 0.0786 (0.0786)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.5739 (1.5739)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [143][0/1]	Time 0.115 (0.115)	Data 0.107 (0.107)	Loss 0.0578 (0.0578)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.142 (0.142)	Loss 1.5747 (1.5747)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [144][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.1498 (0.1498)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.128 (0.128)	Loss 1.5763 (1.5763)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [145][0/1]	Time 0.120 (0.120)	Data 0.111 (0.111)	Loss 0.0929 (0.0929)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.136 (0.136)	Loss 1.5769 (1.5769)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [146][0/1]	Time 0.113 (0.113)	Data 0.106 (0.106)	Loss 0.0783 (0.0783)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.123 (0.123)	Loss 1.5782 (1.5782)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [147][0/1]	Time 0.115 (0.115)	Data 0.108 (0.108)	Loss 0.3327 (0.3327)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.131 (0.131)	Loss 1.5774 (1.5774)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [148][0/1]	Time 0.130 (0.130)	Data 0.124 (0.124)	Loss 0.1765 (0.1765)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.112 (0.112)	Loss 1.5768 (1.5768)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
Epoch: [149][0/1]	Time 0.121 (0.121)	Data 0.111 (0.111)	Loss 0.0550 (0.0550)	Prec@1 100.000 (100.000)
Test: [0/1]	Time 0.126 (0.126)	Loss 1.5773 (1.5773)	Prec@1 46.667 (46.667)
 * Prec@1 46.667
49.333335876464844
total time: 53.39931797981262
train loss:  [3.979975938796997, 1.2098628282546997, 1.5393478870391846, 1.2706336975097656, 1.0377333164215088, 0.9600993990898132, 0.8597244024276733, 0.9520414471626282, 0.8835973739624023, 0.6787570118904114, 0.8706452250480652, 0.7395153641700745, 0.6276885271072388, 0.6227485537528992, 0.57020103931427, 0.6453763246536255, 0.6758923530578613, 0.4263013005256653, 0.3581056594848633, 0.47079119086265564, 0.5512663722038269, 0.39750295877456665, 0.406769335269928, 0.3118649423122406, 0.46011048555374146, 0.44720807671546936, 0.2980267405509949, 0.22459697723388672, 0.3379113972187042, 0.38144537806510925, 0.17486277222633362, 0.3916327953338623, 0.2440090924501419, 0.22451218962669373, 0.20638592541217804, 0.4664662778377533, 0.39646202325820923, 0.19592790305614471, 0.22749710083007812, 0.201939657330513, 0.3700006902217865, 0.34737205505371094, 0.28233760595321655, 0.24479655921459198, 0.18613243103027344, 0.16947583854198456, 0.24332746863365173, 0.17081665992736816, 0.22727656364440918, 0.22497272491455078, 0.15251398086547852, 0.132012277841568, 0.1316739022731781, 0.3325998783111572, 0.12403754889965057, 0.1175290122628212, 0.22803711891174316, 0.06369433552026749, 0.16364403069019318, 0.22666366398334503, 0.11574558913707733, 0.30234479904174805, 0.08865022659301758, 0.18662996590137482, 0.17859001457691193, 0.09353180229663849, 0.29330071806907654, 0.08268161118030548, 0.1416480541229248, 0.258842408657074, 0.10077200084924698, 0.16085562109947205, 0.15140271186828613, 0.04555373266339302, 0.20584335923194885, 0.27912086248397827, 0.18108868598937988, 0.11773500591516495, 0.21239082515239716, 0.06665744632482529, 0.12129120528697968, 0.13780131936073303, 0.11795015633106232, 0.12898878753185272, 0.0808199867606163, 0.045159101486206055, 0.07030558586120605, 0.09873108565807343, 0.07721762359142303, 0.1407756358385086, 0.08321042358875275, 0.2139090597629547, 0.15082669258117676, 0.05516252666711807, 0.15151457488536835, 0.0799410343170166, 0.07514586299657822, 0.06974444538354874, 0.04759030416607857, 0.10341677814722061, 0.14270921051502228, 0.09055991470813751, 0.22078242897987366, 0.05288510397076607, 0.10512657463550568, 0.19509272277355194, 0.14907622337341309, 0.03694157674908638, 0.07322797924280167, 0.08884763717651367, 0.04318070411682129, 0.03708763048052788, 0.13015742599964142, 0.10460992157459259, 0.1438373625278473, 0.12205228954553604, 0.09358706325292587, 0.049303531646728516, 0.08716078102588654, 0.2221774160861969, 0.07960071414709091, 0.16620774567127228, 0.16987605392932892, 0.06259431689977646, 0.08399343490600586, 0.06980724632740021, 0.24422231316566467, 0.21809224784374237, 0.16289296746253967, 0.07882008701562881, 0.3101671636104584, 0.047725580632686615, 0.19440646469593048, 0.24488338828086853, 0.18735608458518982, 0.07446841895580292, 0.12540030479431152, 0.03554673120379448, 0.048522233963012695, 0.48957961797714233, 0.030884599313139915, 0.1035684123635292, 0.07856898009777069, 0.057804059237241745, 0.1498083621263504, 0.09286689758300781, 0.07829771190881729, 0.33266133069992065, 0.1765352189540863, 0.05497903749346733]
train acc:  [20.0, 40.0, 40.0, 40.0, 60.0, 80.0, 80.0, 60.0, 80.0, 100.0, 80.0, 60.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
test loss:  [1.6773675680160522, 1.6084628105163574, 1.4719665050506592, 1.4344171285629272, 1.377230167388916, 1.358742117881775, 1.3221110105514526, 1.3073089122772217, 1.2527055740356445, 1.2328846454620361, 1.2027438879013062, 1.1846872568130493, 1.1805800199508667, 1.1802990436553955, 1.1735104322433472, 1.1711574792861938, 1.175994873046875, 1.1872589588165283, 1.201159954071045, 1.2103351354599, 1.193097710609436, 1.2117453813552856, 1.2062222957611084, 1.2003756761550903, 1.2033352851867676, 1.2124111652374268, 1.2083113193511963, 1.22512948513031, 1.217934012413025, 1.234763741493225, 1.244491696357727, 1.2468767166137695, 1.2556655406951904, 1.2561267614364624, 1.2581305503845215, 1.2527917623519897, 1.2552969455718994, 1.2655160427093506, 1.2627911567687988, 1.275887131690979, 1.2756754159927368, 1.2849658727645874, 1.2913389205932617, 1.2955142259597778, 1.2920457124710083, 1.305467963218689, 1.3058440685272217, 1.3162838220596313, 1.3222005367279053, 1.3217692375183105, 1.334043025970459, 1.3401374816894531, 1.3489073514938354, 1.3379652500152588, 1.3480839729309082, 1.3618993759155273, 1.3789716958999634, 1.3806127309799194, 1.3835111856460571, 1.3854697942733765, 1.3872904777526855, 1.3916401863098145, 1.3929675817489624, 1.3910067081451416, 1.396207332611084, 1.4069633483886719, 1.409151554107666, 1.4136898517608643, 1.422737956047058, 1.4281071424484253, 1.4366497993469238, 1.4344277381896973, 1.4489762783050537, 1.4510910511016846, 1.4492863416671753, 1.43784761428833, 1.4394783973693848, 1.4451091289520264, 1.4480397701263428, 1.4561779499053955, 1.464140772819519, 1.4771164655685425, 1.4816910028457642, 1.4857652187347412, 1.493799090385437, 1.4980320930480957, 1.5010485649108887, 1.5097423791885376, 1.5191900730133057, 1.5068825483322144, 1.5112937688827515, 1.5153758525848389, 1.5192068815231323, 1.5228886604309082, 1.519049048423767, 1.5275566577911377, 1.5278326272964478, 1.5354405641555786, 1.5421286821365356, 1.5534565448760986, 1.5545674562454224, 1.5555046796798706, 1.5550413131713867, 1.5551791191101074, 1.5556585788726807, 1.554835319519043, 1.5551235675811768, 1.5553802251815796, 1.5556634664535522, 1.5559002161026, 1.5561509132385254, 1.5564714670181274, 1.5585484504699707, 1.5578804016113281, 1.5609585046768188, 1.5614311695098877, 1.562114953994751, 1.5626435279846191, 1.563560128211975, 1.5638740062713623, 1.564447283744812, 1.5639348030090332, 1.5645889043807983, 1.5650962591171265, 1.56536865234375, 1.566464900970459, 1.5675537586212158, 1.5665689706802368, 1.5668753385543823, 1.5671085119247437, 1.5664761066436768, 1.5674593448638916, 1.568932294845581, 1.5679939985275269, 1.569953441619873, 1.5699282884597778, 1.571640968322754, 1.572074055671692, 1.5726332664489746, 1.5723968744277954, 1.5726827383041382, 1.5733178853988647, 1.573923945426941, 1.5746879577636719, 1.5762627124786377, 1.5769236087799072, 1.5781978368759155, 1.5774136781692505, 1.5767743587493896, 1.577289342880249]
test acc:  [32.0, 37.333335876464844, 45.333335876464844, 46.66666793823242, 45.333335876464844, 46.66666793823242, 48.0, 49.333335876464844, 45.333335876464844, 44.0, 45.333335876464844, 48.0, 48.0, 48.0, 48.0, 46.66666793823242, 49.333335876464844, 48.0, 42.66666793823242, 44.0, 48.0, 42.66666793823242, 44.0, 44.0, 44.0, 46.66666793823242, 45.333335876464844, 44.0, 45.333335876464844, 44.0, 44.0, 45.333335876464844, 46.66666793823242, 45.333335876464844, 45.333335876464844, 46.66666793823242, 48.0, 48.0, 46.66666793823242, 46.66666793823242, 46.66666793823242, 45.333335876464844, 45.333335876464844, 45.333335876464844, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 48.0, 48.0, 48.0, 45.333335876464844, 45.333335876464844, 46.66666793823242, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 45.333335876464844, 46.66666793823242, 45.333335876464844, 45.333335876464844, 44.0, 44.0, 45.333335876464844, 45.333335876464844, 46.66666793823242, 45.333335876464844, 45.333335876464844, 45.333335876464844, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 46.66666793823242, 48.0, 45.333335876464844, 44.0, 44.0, 45.333335876464844, 44.0, 44.0, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242, 46.66666793823242]
best prec: 49.333335876464844
